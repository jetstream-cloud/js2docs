{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Jetstream2 is a user-friendly cloud computing environment for researchers and educators running on OpenStack and featuring Exosphere as the primary user interface. It is built on the successes of Jetstream1 and continues the main features of that system while extending to a broader range of hardware and services, including GPUs, large memory nodes, virtual clustering, programmable cyberinfrastructure with OpenStack Heat and Terraform, and many other features. It is designed to provide both infrastructure for gateways and other \u201calways on\u201d services as well as giving researchers access to interactive computing and data analysis resources on demand. For a more in-depth description please see the System Overview . Jetstream2 Status \u00b6 Overall JS2 system status Please visit https://jetstream.status.io/ for detailed system status information, planned maintenance announcements, and to subscribe to our downtime/outages mailing list. Also see, Jetstream2 system status and information for additional news information. New users may wish to review: \u00b6 System Overview Acceptable Usage Policies Allocations Overview Exosphere Overview Troubleshooting and FAQ may have helpful suggestions for common problems. Jetstream1 users may wish to review: \u00b6 Migration","title":"Jetstream2 Home"},{"location":"#jetstream2-status","text":"Overall JS2 system status Please visit https://jetstream.status.io/ for detailed system status information, planned maintenance announcements, and to subscribe to our downtime/outages mailing list. Also see, Jetstream2 system status and information for additional news information.","title":"Jetstream2 Status"},{"location":"#new-users-may-wish-to-review","text":"System Overview Acceptable Usage Policies Allocations Overview Exosphere Overview Troubleshooting and FAQ may have helpful suggestions for common problems.","title":"New users may wish to review:"},{"location":"#jetstream1-users-may-wish-to-review","text":"Migration","title":"Jetstream1 users may wish to review:"},{"location":"alloc/education/","text":"Jetstream2 Education Allocations \u00b6 Jetstream2 education allocations are meant to be used for teaching courses, workshops, or tutorials. They are not intended for research. You\u2019ll need a copy of your CV, the course syllabus, and a resource justification in PDF format. The latter is basically figuring the size of the VM your students will need, the number of weeks they\u2019ll need it, and the number of students. An example of the resource selection is below. You\u2019ll first need to create an XSEDE Portal Account if you do not already have one: To create an XSEDE portal account if you do not have one: \u00b6 Go to https://portal.xsede.org/ Click \u201cCreate Account\u201d on the left side of your screen. Fill out the form and click Submit. Upon receipt of the email notification click the link in the email to verify your account and set your username and password. If the link doesn\u2019t work, go to https://portal.xsede.org/ , click \u201cSign In\u201d and then select \u201cVerify Account\u201d under the \u201cOther Sign In Options\u201d. Following account verification, if not already logged in, go to https://portal.xsede.org/ , click \u201cSign In\u201d and sign in with the username and password set in the verification step. You will be asked to read and accept the User Responsibilities form. This form outlines acceptable use to protect shared resources and intellectual property. Apply for an XSEDE Education Allocation \u00b6 Read the Education Allocations Overview . There are sample allocation requests in the overview that you may find helpful. Go to the XSEDE Resource Allocation System page. On the Available Opportunities page, click \u201cStart a New Submission\u201d under \u201cEducational\u201d. If you are not familiar with the process, select \u201cBegin Guided Submission\u201d for step-by-step instructions. Before submitting an allocation request have the following information available: * XSEDE usersnams for PI (required), Co-PIs (optional), and Allocation Managers (optional) * Additional XSEDE user names to add so they may use your allocation time and resources (optional) * Title * Abstract (typically a paragraph or two for an Educational request will suffice) * Keywords * Field of science (secondary areas of science may be also be added) Select your resources - you can have any combination of Jetstream2 CPU, Jetstream2 Large Memory, and Jetstream2 GPU - but you must justify Large Memory or GPU requests specifically Select the appropriate resources(s) from the list. Enter the number of SUs you\u2019ll need. The Virtual Machine Sizes and Configurations page can help you figure VM sizes needed. See the section below for a walkthrough of creating your resource justification. Fill in number of Virtual Machines needed (this is an estimate for planning purposes \u2013 there\u2019s no wrong answer \u2013 try to best guess at how many instances you\u2019ll run at one time) Fill in number of public IP addresses needed (same as above) If you need additional storage beyond the VM\u2019s root disks, select \u201cJetstream2 Storage\u201d. All allocations by default will receive 1TB of storage so if that covers your needs, you do NOT need to select Jetstream2 Storage Upload your supporting documents - PDF format required PI CV (2 page limit) CoPI CV required for every CoPI added to request (2 page limit) \u2013 optional Syllabus Resource Justification (see below) Supporting Grants \u2013 optional Publications of previous/supporting work \u2013 optional 4. Submit allocation request. At this point, all entered information is validated, errors or omissions are flagged. Allow 3-5 business days for your application to go through the approval process. Detailed information about the allocation request process, with screenshots, is available in the XRAS Submit Allocation Request Step-by-Step Guide . Writing your resource justification \u00b6 A resource justification is required for an Educational allocation. You\u2019ll need to explain how the resources will be used and for how long. A sample calculation for your SU request might be: 16 week course 20 students m1.medium instance size (8 vCPU) Planning for 24 hour/day usage - 8 vCPU (SUs) x 24 hours/day x 7 days x 16 weeks = 21,504 SUs/student x 20 students = 430,080 SUs It would be standard practice to round that to 450,000 SUs for development time for the instructors plus instructor VMs during the duration of the course. You\u2019ll also need to briefly explain why you chose the VM sizes that you did \u2013 what you expect each student to be running on those VMs. The Virtual Machine Sizes and Configurations page can help you figure VM sizes needed. A paragraph or two should suffice to cover this. If you have questions about the resource justification please do contact Jetstream Help via help@xsede.org and we can assist you.","title":"Education Allocations"},{"location":"alloc/education/#jetstream2-education-allocations","text":"Jetstream2 education allocations are meant to be used for teaching courses, workshops, or tutorials. They are not intended for research. You\u2019ll need a copy of your CV, the course syllabus, and a resource justification in PDF format. The latter is basically figuring the size of the VM your students will need, the number of weeks they\u2019ll need it, and the number of students. An example of the resource selection is below. You\u2019ll first need to create an XSEDE Portal Account if you do not already have one:","title":"Jetstream2 Education Allocations"},{"location":"alloc/education/#to-create-an-xsede-portal-account-if-you-do-not-have-one","text":"Go to https://portal.xsede.org/ Click \u201cCreate Account\u201d on the left side of your screen. Fill out the form and click Submit. Upon receipt of the email notification click the link in the email to verify your account and set your username and password. If the link doesn\u2019t work, go to https://portal.xsede.org/ , click \u201cSign In\u201d and then select \u201cVerify Account\u201d under the \u201cOther Sign In Options\u201d. Following account verification, if not already logged in, go to https://portal.xsede.org/ , click \u201cSign In\u201d and sign in with the username and password set in the verification step. You will be asked to read and accept the User Responsibilities form. This form outlines acceptable use to protect shared resources and intellectual property.","title":"To create an XSEDE portal account if you do not have one:"},{"location":"alloc/education/#apply-for-an-xsede-education-allocation","text":"Read the Education Allocations Overview . There are sample allocation requests in the overview that you may find helpful. Go to the XSEDE Resource Allocation System page. On the Available Opportunities page, click \u201cStart a New Submission\u201d under \u201cEducational\u201d. If you are not familiar with the process, select \u201cBegin Guided Submission\u201d for step-by-step instructions. Before submitting an allocation request have the following information available: * XSEDE usersnams for PI (required), Co-PIs (optional), and Allocation Managers (optional) * Additional XSEDE user names to add so they may use your allocation time and resources (optional) * Title * Abstract (typically a paragraph or two for an Educational request will suffice) * Keywords * Field of science (secondary areas of science may be also be added) Select your resources - you can have any combination of Jetstream2 CPU, Jetstream2 Large Memory, and Jetstream2 GPU - but you must justify Large Memory or GPU requests specifically Select the appropriate resources(s) from the list. Enter the number of SUs you\u2019ll need. The Virtual Machine Sizes and Configurations page can help you figure VM sizes needed. See the section below for a walkthrough of creating your resource justification. Fill in number of Virtual Machines needed (this is an estimate for planning purposes \u2013 there\u2019s no wrong answer \u2013 try to best guess at how many instances you\u2019ll run at one time) Fill in number of public IP addresses needed (same as above) If you need additional storage beyond the VM\u2019s root disks, select \u201cJetstream2 Storage\u201d. All allocations by default will receive 1TB of storage so if that covers your needs, you do NOT need to select Jetstream2 Storage Upload your supporting documents - PDF format required PI CV (2 page limit) CoPI CV required for every CoPI added to request (2 page limit) \u2013 optional Syllabus Resource Justification (see below) Supporting Grants \u2013 optional Publications of previous/supporting work \u2013 optional 4. Submit allocation request. At this point, all entered information is validated, errors or omissions are flagged. Allow 3-5 business days for your application to go through the approval process. Detailed information about the allocation request process, with screenshots, is available in the XRAS Submit Allocation Request Step-by-Step Guide .","title":"Apply for an XSEDE Education Allocation"},{"location":"alloc/education/#writing-your-resource-justification","text":"A resource justification is required for an Educational allocation. You\u2019ll need to explain how the resources will be used and for how long. A sample calculation for your SU request might be: 16 week course 20 students m1.medium instance size (8 vCPU) Planning for 24 hour/day usage - 8 vCPU (SUs) x 24 hours/day x 7 days x 16 weeks = 21,504 SUs/student x 20 students = 430,080 SUs It would be standard practice to round that to 450,000 SUs for development time for the instructors plus instructor VMs during the duration of the course. You\u2019ll also need to briefly explain why you chose the VM sizes that you did \u2013 what you expect each student to be running on those VMs. The Virtual Machine Sizes and Configurations page can help you figure VM sizes needed. A paragraph or two should suffice to cover this. If you have questions about the resource justification please do contact Jetstream Help via help@xsede.org and we can assist you.","title":"Writing your resource justification"},{"location":"alloc/faq/","text":"Jetstream2 Allocations FAQ \u00b6 1. Is there an overview of the types of allocations available as well as any restrictions those allocations have? The Getting Started guide describes the process of getting onto XSEDE, applying for allocations and using XSEDE resources. To review the types of allocations XSEDE and the process to get an allocation, here are some links you might find useful: Allocations overview Types of allocations, eligibility, details Allocation policies Submit and manage allocation requests Proposal deadlines 2. Is there a example or demonstration of how to get an allocation that I could follow? The Research Allocation page has information and links for writing a successful Jetstream2 research allocation request. There is a Cornell Virtual Workshop (CVW) on getting a Research Allocation for Jetstream here: https://cvw.cac.cornell.edu/JetstreamReq/. It is not updated for Jetstream2 though the principles are the same. 3. How do I let other XSEDE accounts use my allocation? You can add users to (or remove them from) your XSEDE allocation via the XSEDE User Portal. Users must have already created their XSEDE accounts before they can be added to an allocation. To add users to, or remove them from, an active Extreme Science and Engineering Discovery Environment ( XSEDE ) allocation, the principal investigator, co-principal investigator, or allocation manager can follow the instructions here: https://portal.xsede.org/allocations/managing Please note that it can take up to four hours for users added to an allocation to become active. 4. How often can I get a startup allocation? Applications for startup allocations will only be accepted once. If you have modest needs that are equal or less than startup values, you may renew your startup allocation. If you need a larger allocation, it is best to apply for a research allocation . Maximum Startup/Campus Champion Allocation values for each resource are: * Jetstream2 CPU - 200,000 SUs * Jetstream2 Large Memory - 400,000 SUs * Jetstream2 GPU - 600,000 SUs * Jetstream2 Storage - 1TB default* Storage limits may be larger than 1TB per allocation for a startup if well-justified. 5. Can I renew a startup allocation? If your SU needs are equal to or less than the maximum startup values (see item 4 just above) you may renew your startup allocation. If you need a signficantly larger amound of SUs for any of the resources, you will need to pursue a research allocation 6. I\u2019m running out of Service Units (SUs) or storage. How do I request more? If you already have an XSEDE allocation and need to request additional service units (SUs), the PI, co-PI, or delegate may submit a request via the XSEDE User Portal. For instructions on how to submit the request, see Requesting additional SUs, other Jetstream resources, or storage for Jetstream \u2013 Supplemental Allocations . 7. I am at or exceeding the quota limits for my allocation. How do I request additional resources such as CPUs and memory? You may contact help@xsede.org or help@jetstream-cloud.org with those requests. It\u2019s important to note that Jetstream Trial Allocation quotas are fixed and will NOT be increased under any circumstances. For other allocation types, justification will be required and will be granted at the discretion of the Jetstream2 staff based on the justification and available resources. Please note that large memory and GPU resources are limited so requests for those will require strong justification for success or partial success. We strive to make resources available to all researchers that require them, so striking a balance between the needs of one versus many is often necessary. 8. Can you extend my allocation for me or give me access to my allocation for just a few days/weeks/months more? If your allocation is expired or out of SUs, you may request an extension, renewal, or supplement. Please see one of the following links: Requesting additional SUs, other Jetstream resources, or storage for Jetstream \u2013 Supplemental Allocations Jetstream2 Allocation Extensions and Renewals Jetstream2 staff are unable to take these actions on your behalf.","title":"Frequently Asked Questions"},{"location":"alloc/faq/#jetstream2-allocations-faq","text":"1. Is there an overview of the types of allocations available as well as any restrictions those allocations have? The Getting Started guide describes the process of getting onto XSEDE, applying for allocations and using XSEDE resources. To review the types of allocations XSEDE and the process to get an allocation, here are some links you might find useful: Allocations overview Types of allocations, eligibility, details Allocation policies Submit and manage allocation requests Proposal deadlines 2. Is there a example or demonstration of how to get an allocation that I could follow? The Research Allocation page has information and links for writing a successful Jetstream2 research allocation request. There is a Cornell Virtual Workshop (CVW) on getting a Research Allocation for Jetstream here: https://cvw.cac.cornell.edu/JetstreamReq/. It is not updated for Jetstream2 though the principles are the same. 3. How do I let other XSEDE accounts use my allocation? You can add users to (or remove them from) your XSEDE allocation via the XSEDE User Portal. Users must have already created their XSEDE accounts before they can be added to an allocation. To add users to, or remove them from, an active Extreme Science and Engineering Discovery Environment ( XSEDE ) allocation, the principal investigator, co-principal investigator, or allocation manager can follow the instructions here: https://portal.xsede.org/allocations/managing Please note that it can take up to four hours for users added to an allocation to become active. 4. How often can I get a startup allocation? Applications for startup allocations will only be accepted once. If you have modest needs that are equal or less than startup values, you may renew your startup allocation. If you need a larger allocation, it is best to apply for a research allocation . Maximum Startup/Campus Champion Allocation values for each resource are: * Jetstream2 CPU - 200,000 SUs * Jetstream2 Large Memory - 400,000 SUs * Jetstream2 GPU - 600,000 SUs * Jetstream2 Storage - 1TB default* Storage limits may be larger than 1TB per allocation for a startup if well-justified. 5. Can I renew a startup allocation? If your SU needs are equal to or less than the maximum startup values (see item 4 just above) you may renew your startup allocation. If you need a signficantly larger amound of SUs for any of the resources, you will need to pursue a research allocation 6. I\u2019m running out of Service Units (SUs) or storage. How do I request more? If you already have an XSEDE allocation and need to request additional service units (SUs), the PI, co-PI, or delegate may submit a request via the XSEDE User Portal. For instructions on how to submit the request, see Requesting additional SUs, other Jetstream resources, or storage for Jetstream \u2013 Supplemental Allocations . 7. I am at or exceeding the quota limits for my allocation. How do I request additional resources such as CPUs and memory? You may contact help@xsede.org or help@jetstream-cloud.org with those requests. It\u2019s important to note that Jetstream Trial Allocation quotas are fixed and will NOT be increased under any circumstances. For other allocation types, justification will be required and will be granted at the discretion of the Jetstream2 staff based on the justification and available resources. Please note that large memory and GPU resources are limited so requests for those will require strong justification for success or partial success. We strive to make resources available to all researchers that require them, so striking a balance between the needs of one versus many is often necessary. 8. Can you extend my allocation for me or give me access to my allocation for just a few days/weeks/months more? If your allocation is expired or out of SUs, you may request an extension, renewal, or supplement. Please see one of the following links: Requesting additional SUs, other Jetstream resources, or storage for Jetstream \u2013 Supplemental Allocations Jetstream2 Allocation Extensions and Renewals Jetstream2 staff are unable to take these actions on your behalf.","title":"Jetstream2 Allocations FAQ"},{"location":"alloc/overview/","text":"Jetstream2 Allocations Overview \u00b6 Access to Jetstream2 is available solely through Xtreme Science and Engineering Discovery Environment (XSEDE) allocations. You must be on a valid allocation or the PI of a valid allocation to have access to Jetstream2. Jetstream allocations are awarded exclusively through XSEDE. XSEDE provides XSEDE User Portal (XUP) accounts free of charge. XSEDE allocations require that the Principal Investigator (PI) be a US-based researcher. A guide with more detail on Jetstream allocations is available online at the XSEDE Resource Information page. Allocation types: \u00b6 Trial allocation - Limited in hours and cores , non-renewable, meant to explore using Jetstream2 Startup allocation - For smaller scale research or exploratory usage before applying for a larger research allocation. Not intended for any courses or workshops. Education allocation - For workshops, tutorials, or courses. Startup limits may not apply. Research allocation - Larger scale allocations intended for research. Limits are generally only in what the PI can justify requesting. Maximum Trial Allocation values for each resource are: \u00b6 Jetstream2 CPU - 2,000 SUs, 2 cores Maximum Startup/Campus Champion Allocation values for each resource are: \u00b6 Jetstream2 CPU - 200,000 SUs Jetstream2 Large Memory - 400,000 SUs Jetstream2 GPU - 600,000 SUs Jetstream2 Storage - 1TB default * * Storage limits may be larger than 1TB per allocation for a startup if well-justified. You\u2019ll need to have an XSEDE Portal account to request access to the Jetstream2 Trial Allocation, be added to a PI\u2019s allocation, or apply for an allocation. To create an XSEDE portal account if you do not have one: \u00b6 Go to https://portal.xsede.org/ Click \u201cCreate Account\u201d on the left side of your screen. Fill out the form and click Submit. Upon receipt of the email notification click the link in the email to verify your account and set your username and password. If the link doesn\u2019t work, go to https://portal.xsede.org/ , click \u201cSign In\u201d and then select \u201cVerify Account\u201d under the \u201cOther Sign In Options\u201d. Following account verification, if not already logged in, go to https://portal.xsede.org/ , click \u201cSign In\u201d and sign in with the username and password set in the verification step. You will be asked to read and accept the User Responsibilities form. This form outlines acceptable use to protect shared resources and intellectual property.","title":"Allocations Overview"},{"location":"alloc/overview/#jetstream2-allocations-overview","text":"Access to Jetstream2 is available solely through Xtreme Science and Engineering Discovery Environment (XSEDE) allocations. You must be on a valid allocation or the PI of a valid allocation to have access to Jetstream2. Jetstream allocations are awarded exclusively through XSEDE. XSEDE provides XSEDE User Portal (XUP) accounts free of charge. XSEDE allocations require that the Principal Investigator (PI) be a US-based researcher. A guide with more detail on Jetstream allocations is available online at the XSEDE Resource Information page.","title":"Jetstream2 Allocations Overview"},{"location":"alloc/overview/#allocation-types","text":"Trial allocation - Limited in hours and cores , non-renewable, meant to explore using Jetstream2 Startup allocation - For smaller scale research or exploratory usage before applying for a larger research allocation. Not intended for any courses or workshops. Education allocation - For workshops, tutorials, or courses. Startup limits may not apply. Research allocation - Larger scale allocations intended for research. Limits are generally only in what the PI can justify requesting.","title":"Allocation types:"},{"location":"alloc/overview/#maximum-trial-allocation-values-for-each-resource-are","text":"Jetstream2 CPU - 2,000 SUs, 2 cores","title":"Maximum Trial Allocation values for each resource are:"},{"location":"alloc/overview/#maximum-startupcampus-champion-allocation-values-for-each-resource-are","text":"Jetstream2 CPU - 200,000 SUs Jetstream2 Large Memory - 400,000 SUs Jetstream2 GPU - 600,000 SUs Jetstream2 Storage - 1TB default * * Storage limits may be larger than 1TB per allocation for a startup if well-justified. You\u2019ll need to have an XSEDE Portal account to request access to the Jetstream2 Trial Allocation, be added to a PI\u2019s allocation, or apply for an allocation.","title":"Maximum Startup/Campus Champion Allocation values for each resource are:"},{"location":"alloc/overview/#to-create-an-xsede-portal-account-if-you-do-not-have-one","text":"Go to https://portal.xsede.org/ Click \u201cCreate Account\u201d on the left side of your screen. Fill out the form and click Submit. Upon receipt of the email notification click the link in the email to verify your account and set your username and password. If the link doesn\u2019t work, go to https://portal.xsede.org/ , click \u201cSign In\u201d and then select \u201cVerify Account\u201d under the \u201cOther Sign In Options\u201d. Following account verification, if not already logged in, go to https://portal.xsede.org/ , click \u201cSign In\u201d and sign in with the username and password set in the verification step. You will be asked to read and accept the User Responsibilities form. This form outlines acceptable use to protect shared resources and intellectual property.","title":"To create an XSEDE portal account if you do not have one:"},{"location":"alloc/renew-extend/","text":"Jetstream2 Allocation Extensions and Renewals \u00b6 If your allocation is expired or close to expired and has adequate SUs remaining for the resource(s), you may request an extension if you need 3-6 months to complete your work or to prepare for a research allocation. If you are planning to continue your research for another year (or more), you should pursue a renewal of your allocation. Detailed information about the extension request process is available from the XSEDE Manage Allocations page. Please note: Extensions are not a substitution for the renewal process. A renewal is ALWAYS preferred over an extension. Please see below for information on renewing a startup allocation. If you have a startup allocation and you are planning to stay under the startup limits (as noted on the Jetstream Resources page), you may continue to renew your startup and not have to enter the research allocations cycle. Research allocations may be extended once with adequate justification . Research allocations are renewable on a quarterly basis. More information may be found on the XSEDE Research Allocations page Instructions for requesting an extension \u00b6 Login to portal.xsede.org Go to the Submit / Review Requests page (You can also mouse over the \u201cAllocations tab\u201d and then select \u201cSubmit/Review Request\u201d) Find your allocation. Under the Action menu for your allocation, select \u201cExtension\u201d Note: You will not see the extension option until your allocation is within 90 days of expiring. Choose \u201cStart Extension\u201d Select the duration from the list. Enter all comments and justifications for the extension. After you have added that, you may submit the request. Instructions for requesting a renewal \u00b6 Login to portal.xsede.org You will need the same materials as a startup submission plus a progress report in PDF format with any pertinent information about the status of the project, including publications (preferably in bioliography form with DOIs or URLs), user and job counts for science gateways, and any other information you\u2019d like to provide to XSEDE and the NSF. Go to the Submit / Review Requests page (You can also mouse over the \u201cAllocations tab\u201d and then select \u201cSubmit/Review Request\u201d) Look on the LEFT side of the page and scroll down to find your allocation. Note: You will not see the renewal option until your allocation is within 30 days of expiring. Choose \u201cStart Renewal for [Your allocation number] \u201c Select the duration from the list. Enter all comments and justifications for the extension. After you have added that, you may submit the request. Requests are typically reviewed within 1 to 2 business days. You\u2019ll receive notification from XSEDE Allocations on the status of your request once it has been reviewed by all service providers on the request.","title":"Extensions & Renewals"},{"location":"alloc/renew-extend/#jetstream2-allocation-extensions-and-renewals","text":"If your allocation is expired or close to expired and has adequate SUs remaining for the resource(s), you may request an extension if you need 3-6 months to complete your work or to prepare for a research allocation. If you are planning to continue your research for another year (or more), you should pursue a renewal of your allocation. Detailed information about the extension request process is available from the XSEDE Manage Allocations page. Please note: Extensions are not a substitution for the renewal process. A renewal is ALWAYS preferred over an extension. Please see below for information on renewing a startup allocation. If you have a startup allocation and you are planning to stay under the startup limits (as noted on the Jetstream Resources page), you may continue to renew your startup and not have to enter the research allocations cycle. Research allocations may be extended once with adequate justification . Research allocations are renewable on a quarterly basis. More information may be found on the XSEDE Research Allocations page","title":"Jetstream2 Allocation Extensions and Renewals"},{"location":"alloc/renew-extend/#instructions-for-requesting-an-extension","text":"Login to portal.xsede.org Go to the Submit / Review Requests page (You can also mouse over the \u201cAllocations tab\u201d and then select \u201cSubmit/Review Request\u201d) Find your allocation. Under the Action menu for your allocation, select \u201cExtension\u201d Note: You will not see the extension option until your allocation is within 90 days of expiring. Choose \u201cStart Extension\u201d Select the duration from the list. Enter all comments and justifications for the extension. After you have added that, you may submit the request.","title":"Instructions for requesting an extension"},{"location":"alloc/renew-extend/#instructions-for-requesting-a-renewal","text":"Login to portal.xsede.org You will need the same materials as a startup submission plus a progress report in PDF format with any pertinent information about the status of the project, including publications (preferably in bioliography form with DOIs or URLs), user and job counts for science gateways, and any other information you\u2019d like to provide to XSEDE and the NSF. Go to the Submit / Review Requests page (You can also mouse over the \u201cAllocations tab\u201d and then select \u201cSubmit/Review Request\u201d) Look on the LEFT side of the page and scroll down to find your allocation. Note: You will not see the renewal option until your allocation is within 30 days of expiring. Choose \u201cStart Renewal for [Your allocation number] \u201c Select the duration from the list. Enter all comments and justifications for the extension. After you have added that, you may submit the request. Requests are typically reviewed within 1 to 2 business days. You\u2019ll receive notification from XSEDE Allocations on the status of your request once it has been reviewed by all service providers on the request.","title":"Instructions for requesting a renewal"},{"location":"alloc/research/","text":"Jetstream2 Research Allocations \u00b6 Jetstream2 ressearch allocations are meant to be used for building on research exploration that you likely began initially with a startup allocation. Research allocations are intended only for research or research infrastructure purposes. They are not intended for teaching purposes. Please apply for an education allocation using the instructions here for any course, tutorial, or workshop needs. XSEDE strongly encourages researchers to request a Startup Allocation prior to requesting a Research Allocation, in order to obtain benchmark results before you prepare your Research request. While having a Startup project is not absolutely required, the lack of benchmark results on the resources requested will greatly increase the level of scrutiny from the panel, especially for requests on highly oversubscribed resources. For projects that have progressed beyond the Startup phase, either in purpose or scale of computational activities, a Research request is appropriate. Research requests are accepted and reviewed quarterly by the XSEDE Resource Allocations Committee (XRAC). XSEDE maintains a page with all of the information you will need to submit a research allocation: https://portal.xsede.org/allocations/research We highly encourage all potential applicants to view: Webinars on writing a successful research allocation request and also the webinar for code and scaling documentation. Example Research Requests an applicant can review. Required Components section of the XSEDE documentation. If your research allocation request does not have all of the required components, it may be rejected without review. Science Gateway research requests may have some different criteria. We recommend reviewing this section of the document if you will be submitting a science gateway oriented research request. Please do not forget to review the formatting guidelines - failure to adhere to the guidelines may also result in a submission being rejected without review","title":"Research Allocations"},{"location":"alloc/research/#jetstream2-research-allocations","text":"Jetstream2 ressearch allocations are meant to be used for building on research exploration that you likely began initially with a startup allocation. Research allocations are intended only for research or research infrastructure purposes. They are not intended for teaching purposes. Please apply for an education allocation using the instructions here for any course, tutorial, or workshop needs. XSEDE strongly encourages researchers to request a Startup Allocation prior to requesting a Research Allocation, in order to obtain benchmark results before you prepare your Research request. While having a Startup project is not absolutely required, the lack of benchmark results on the resources requested will greatly increase the level of scrutiny from the panel, especially for requests on highly oversubscribed resources. For projects that have progressed beyond the Startup phase, either in purpose or scale of computational activities, a Research request is appropriate. Research requests are accepted and reviewed quarterly by the XSEDE Resource Allocations Committee (XRAC). XSEDE maintains a page with all of the information you will need to submit a research allocation: https://portal.xsede.org/allocations/research We highly encourage all potential applicants to view: Webinars on writing a successful research allocation request and also the webinar for code and scaling documentation. Example Research Requests an applicant can review. Required Components section of the XSEDE documentation. If your research allocation request does not have all of the required components, it may be rejected without review. Science Gateway research requests may have some different criteria. We recommend reviewing this section of the document if you will be submitting a science gateway oriented research request. Please do not forget to review the formatting guidelines - failure to adhere to the guidelines may also result in a submission being rejected without review","title":"Jetstream2 Research Allocations"},{"location":"alloc/startup/","text":"Jetstream2 Startup Allocations \u00b6 Jetstream2 startup allocations are meant to be used for exploring the Jetstream2 system for research or research infrastructure purposes. They are not intended for teaching purposes. Please apply for an education allocation using the instructions here for any course, tutorial, or workshop needs. You\u2019ll need a copy of your CV, abstract, and description of your intended research in PDF format. You\u2019ll first need to create an XSEDE Portal Account if you do not already have one: To create an XSEDE portal account if you do not have one: \u00b6 Go to https://portal.xsede.org/ Click \u201cCreate Account\u201d on the left side of your screen. Fill out the form and click Submit. Upon receipt of the email notification click the link in the email to verify your account and set your username and password. If the link doesn\u2019t work, go to https://portal.xsede.org/ , click \u201cSign In\u201d and then select \u201cVerify Account\u201d under the \u201cOther Sign In Options\u201d. Following account verification, if not already logged in, go to https://portal.xsede.org/ , click \u201cSign In\u201d and sign in with the username and password set in the verification step. You will be asked to read and accept the User Responsibilities form. This form outlines acceptable use to protect shared resources and intellectual property. Apply for an XSEDE Startup Allocation \u00b6 Read the Startup Allocations Overview . There are sample allocation requests in the overview that you may find helpful. Go to the XSEDE Resource Allocation System page. On the Available Opportunities page, click \u201cStart a New Submission\u201d under \u201cStartup\u201d. If you are not familiar with the process, select \u201cBegin Guided Submission\u201d for step-by-step instructions. Before submitting an allocation request have the following information available: * XSEDE usersnams for PI (required), Co-PIs (optional), and Allocation Managers (optional) * Additional XSEDE user names to add so they may use your allocation time and resources (optional) * Title * Abstract (typically a paragraph or two for an Educational request will suffice) * Keywords * Field of science (secondary areas of science may be also be added) Select your resources - you can have any combination of Jetstream2 CPU, Jetstream2 Large Memory, and Jetstream2 GPU - but you must justify Large Memory or GPU requests specifically Select the appropriate resources(s) from the list. Enter the number of SUs you\u2019ll need. The Virtual Machine Sizes and Configurations page can help you figure VM sizes needed. Fill in number of Virtual Machines needed (this is an estimate for planning purposes \u2013 there\u2019s no wrong answer \u2013 try to best guess at how many instances you\u2019ll run at one time) Fill in number of public IP addresses needed (same as above) If you need additional storage beyond the VM\u2019s root disks, select \u201cJetstream2 Storage\u201d. All allocations by default will receive 1TB of storage so if that covers your needs, you do NOT need to select Jetstream2 Storage Upload your supporting documents - PDF format required PI CV (2 page limit) CoPI CV required for every CoPI added to request (2 page limit) \u2013 optional Syllabus Resource Justification (see below) Supporting Grants \u2013 optional Publications of previous/supporting work \u2013 optional 4. Submit allocation request. At this point, all entered information is validated, errors or omissions are flagged. Allow 1-2 business days for your application to go through the approval process. Detailed information about the allocation request process, with screenshots, is available in the XRAS Submit Allocation Request Step-by-Step Guide .","title":"Startup Allocations"},{"location":"alloc/startup/#jetstream2-startup-allocations","text":"Jetstream2 startup allocations are meant to be used for exploring the Jetstream2 system for research or research infrastructure purposes. They are not intended for teaching purposes. Please apply for an education allocation using the instructions here for any course, tutorial, or workshop needs. You\u2019ll need a copy of your CV, abstract, and description of your intended research in PDF format. You\u2019ll first need to create an XSEDE Portal Account if you do not already have one:","title":"Jetstream2 Startup Allocations"},{"location":"alloc/startup/#to-create-an-xsede-portal-account-if-you-do-not-have-one","text":"Go to https://portal.xsede.org/ Click \u201cCreate Account\u201d on the left side of your screen. Fill out the form and click Submit. Upon receipt of the email notification click the link in the email to verify your account and set your username and password. If the link doesn\u2019t work, go to https://portal.xsede.org/ , click \u201cSign In\u201d and then select \u201cVerify Account\u201d under the \u201cOther Sign In Options\u201d. Following account verification, if not already logged in, go to https://portal.xsede.org/ , click \u201cSign In\u201d and sign in with the username and password set in the verification step. You will be asked to read and accept the User Responsibilities form. This form outlines acceptable use to protect shared resources and intellectual property.","title":"To create an XSEDE portal account if you do not have one:"},{"location":"alloc/startup/#apply-for-an-xsede-startup-allocation","text":"Read the Startup Allocations Overview . There are sample allocation requests in the overview that you may find helpful. Go to the XSEDE Resource Allocation System page. On the Available Opportunities page, click \u201cStart a New Submission\u201d under \u201cStartup\u201d. If you are not familiar with the process, select \u201cBegin Guided Submission\u201d for step-by-step instructions. Before submitting an allocation request have the following information available: * XSEDE usersnams for PI (required), Co-PIs (optional), and Allocation Managers (optional) * Additional XSEDE user names to add so they may use your allocation time and resources (optional) * Title * Abstract (typically a paragraph or two for an Educational request will suffice) * Keywords * Field of science (secondary areas of science may be also be added) Select your resources - you can have any combination of Jetstream2 CPU, Jetstream2 Large Memory, and Jetstream2 GPU - but you must justify Large Memory or GPU requests specifically Select the appropriate resources(s) from the list. Enter the number of SUs you\u2019ll need. The Virtual Machine Sizes and Configurations page can help you figure VM sizes needed. Fill in number of Virtual Machines needed (this is an estimate for planning purposes \u2013 there\u2019s no wrong answer \u2013 try to best guess at how many instances you\u2019ll run at one time) Fill in number of public IP addresses needed (same as above) If you need additional storage beyond the VM\u2019s root disks, select \u201cJetstream2 Storage\u201d. All allocations by default will receive 1TB of storage so if that covers your needs, you do NOT need to select Jetstream2 Storage Upload your supporting documents - PDF format required PI CV (2 page limit) CoPI CV required for every CoPI added to request (2 page limit) \u2013 optional Syllabus Resource Justification (see below) Supporting Grants \u2013 optional Publications of previous/supporting work \u2013 optional 4. Submit allocation request. At this point, all entered information is validated, errors or omissions are flagged. Allow 1-2 business days for your application to go through the approval process. Detailed information about the allocation request process, with screenshots, is available in the XRAS Submit Allocation Request Step-by-Step Guide .","title":"Apply for an XSEDE Startup Allocation"},{"location":"alloc/supplement/","text":"Jetstream2 Allocation Supplements \u00b6 If your allocation has been depleted or you need to add SUs or storage for a Jetstream2, resource to your existing allocation, you may request a supplement. Detailed information about the supplement request process is available from the XSEDE Manage Allocations page. Instructions for requesting a supplements \u00b6 Login to portal.xsede.org Go to the Submit / Review Requests page (You can also mouse over the \u201cAllocations tab\u201d and then select \u201cSubmit/Review Request\u201d) Find your allocation. Under the Action menu for your allocation, select \u201cSupplement\u201d Note: If your allocation is going to expire in 30 days or less, you will not see the Supplement option. You should see the Extension option, though. Choose a 3 month extension and once it\u2019s approved, the supplement button should appear. Choose \u201cStart Supplement\u201d Select the appropriate Jetstream2 resources(s) from the list. Once you have selected the resources you need, you\u2019ll need to fill in the values of your request. The SU request has the same options as when you got your allocation. You\u2019ll fill in the SUs needed, the number VMs and IPs you expect to use (for forecasting purposes), and any comments. For storage requests, you\u2019ll fill in the value in gigabytes - e.g. 1000gb = 1TB. For the number of SUs you\u2019ll need. The Virtual Machine Sizes and Configurations page can help you estimage VM sizes needed and SUs that will consume. For more information about the resources, please see the Jetstream2 Resources page You will have to include a PDF \u201cProgress Report\u201d \u2013 basically a paragraph or two on the need for the storage or SU request. We do like to see any publications that have been published or submitted during your allocation period as part of this report. Any other milestones or achievements would also be welcomed in the progress report along with user or job counts if it\u2019s a science gateway resource. After you have added that, you may submit the request. Requests are typically reviewed within 1 to 2 business days. You\u2019ll receive notification from XSEDE Allocations on the status of your request once it has been reviewed by all service providers on the request.","title":"Supplements (Storage/SUs)"},{"location":"alloc/supplement/#jetstream2-allocation-supplements","text":"If your allocation has been depleted or you need to add SUs or storage for a Jetstream2, resource to your existing allocation, you may request a supplement. Detailed information about the supplement request process is available from the XSEDE Manage Allocations page.","title":"Jetstream2 Allocation Supplements"},{"location":"alloc/supplement/#instructions-for-requesting-a-supplements","text":"Login to portal.xsede.org Go to the Submit / Review Requests page (You can also mouse over the \u201cAllocations tab\u201d and then select \u201cSubmit/Review Request\u201d) Find your allocation. Under the Action menu for your allocation, select \u201cSupplement\u201d Note: If your allocation is going to expire in 30 days or less, you will not see the Supplement option. You should see the Extension option, though. Choose a 3 month extension and once it\u2019s approved, the supplement button should appear. Choose \u201cStart Supplement\u201d Select the appropriate Jetstream2 resources(s) from the list. Once you have selected the resources you need, you\u2019ll need to fill in the values of your request. The SU request has the same options as when you got your allocation. You\u2019ll fill in the SUs needed, the number VMs and IPs you expect to use (for forecasting purposes), and any comments. For storage requests, you\u2019ll fill in the value in gigabytes - e.g. 1000gb = 1TB. For the number of SUs you\u2019ll need. The Virtual Machine Sizes and Configurations page can help you estimage VM sizes needed and SUs that will consume. For more information about the resources, please see the Jetstream2 Resources page You will have to include a PDF \u201cProgress Report\u201d \u2013 basically a paragraph or two on the need for the storage or SU request. We do like to see any publications that have been published or submitted during your allocation period as part of this report. Any other milestones or achievements would also be welcomed in the progress report along with user or job counts if it\u2019s a science gateway resource. After you have added that, you may submit the request. Requests are typically reviewed within 1 to 2 business days. You\u2019ll receive notification from XSEDE Allocations on the status of your request once it has been reviewed by all service providers on the request.","title":"Instructions for requesting a supplements"},{"location":"alloc/trial/","text":"Jetstream2 Trial Allocations (JTA) \u00b6 JTA Coming Soon Trial allocations will be available Q2 2022. This page will be updated with more information as we near that time. Jetstream2 is an NSF/XSEDE resource designed to promote and provide configurable cyberinfrastructure in the form of cloud computing virtual computers (VMs) to both novice and experienced users. XSEDE offers Trial Access allocations (JTA) to Jetstream2 for evaluation purposes. JTA provide expedited but limited access for potential users to Jetstream2 (JS2), such that, within one business day, approved users will be able to access JS2 and evaluate deploying a VM prior to requesting a larger STARTUP, EDUCATION, or RESEARCH allocation. JTA are limited to: 1000 Service Units on the main JS2 cloud at Indiana University (IU) ( access to Regional clouds at TACC, ASU, UH, or Cornell is NOT currently provided) 1 m3.tiny (single core) or 1 m3.small (2-core) Virtual Machine (VM) instance per cloud at a time ( see https://docs.jetstream-cloud.org/general/vmsizes/#jetstream2-cpu ) 1 VM backup snapshot per instance 1 small 10 GB disk external storage volume. This is enough capacity to give new users an experience with virtual computing and try some \u201ccloud-native\u201d work, but is not a substantial usage of the system. While Virtual GPUs are available on JS2, they are outside the scope of the JTA and NOT currently provided. NOTE : The limits applied to JTA users are intrinsic to the allocation and cannot be increased. For more information, please see the Jetstream documentation at: http://docs.jetstream-cloud.org JTA Coming Soon Trial allocations will be available Q2 2022. This page will be updated with more information as we near that time.","title":"Trial Allocation"},{"location":"alloc/trial/#jetstream2-trial-allocations-jta","text":"JTA Coming Soon Trial allocations will be available Q2 2022. This page will be updated with more information as we near that time. Jetstream2 is an NSF/XSEDE resource designed to promote and provide configurable cyberinfrastructure in the form of cloud computing virtual computers (VMs) to both novice and experienced users. XSEDE offers Trial Access allocations (JTA) to Jetstream2 for evaluation purposes. JTA provide expedited but limited access for potential users to Jetstream2 (JS2), such that, within one business day, approved users will be able to access JS2 and evaluate deploying a VM prior to requesting a larger STARTUP, EDUCATION, or RESEARCH allocation. JTA are limited to: 1000 Service Units on the main JS2 cloud at Indiana University (IU) ( access to Regional clouds at TACC, ASU, UH, or Cornell is NOT currently provided) 1 m3.tiny (single core) or 1 m3.small (2-core) Virtual Machine (VM) instance per cloud at a time ( see https://docs.jetstream-cloud.org/general/vmsizes/#jetstream2-cpu ) 1 VM backup snapshot per instance 1 small 10 GB disk external storage volume. This is enough capacity to give new users an experience with virtual computing and try some \u201ccloud-native\u201d work, but is not a substantial usage of the system. While Virtual GPUs are available on JS2, they are outside the scope of the JTA and NOT currently provided. NOTE : The limits applied to JTA users are intrinsic to the allocation and cannot be increased. For more information, please see the Jetstream documentation at: http://docs.jetstream-cloud.org JTA Coming Soon Trial allocations will be available Q2 2022. This page will be updated with more information as we near that time.","title":"Jetstream2 Trial Allocations (JTA)"},{"location":"faq/alloc/","text":"Jetstream2 Allocations FAQ \u00b6 1. Is there an overview of the types of allocations available as well as any restrictions those allocations have? The Getting Started guide describes the process of getting onto XSEDE, applying for allocations and using XSEDE resources. To review the types of allocations XSEDE and the process to get an allocation, here are some links you might find useful: Allocations overview Types of allocations, eligibility, details Allocation policies Submit and manage allocation requests Proposal deadlines 2. Is there a example or demonstration of how to get an allocation that I could follow? The Research Allocation page has information and links for writing a successful Jetstream2 research allocation request. There is a Cornell Virtual Workshop (CVW) on getting a Research Allocation for Jetstream here: https://cvw.cac.cornell.edu/JetstreamReq/. It is not updated for Jetstream2 though the principles are the same. 3. How do I let other XSEDE accounts use my allocation? You can add users to (or remove them from) your XSEDE allocation via the XSEDE User Portal. Users must have already created their XSEDE accounts before they can be added to an allocation. To add users to, or remove them from, an active Extreme Science and Engineering Discovery Environment ( XSEDE ) allocation, the principal investigator, co-principal investigator, or allocation manager can follow the instructions here: https://portal.xsede.org/allocations/managing Please note that it can take up to four hours for users added to an allocation to become active. 4. How often can I get a startup allocation? Applications for startup allocations will only be accepted once. If you have modest needs that are equal or less than startup values, you may renew your startup allocation. If you need a larger allocation, it is best to apply for a research allocation . Maximum Startup/Campus Champion Allocation values for each resource are: * Jetstream2 CPU - 200,000 SUs * Jetstream2 Large Memory - 400,000 SUs * Jetstream2 GPU - 600,000 SUs * Jetstream2 Storage - 1TB default* Storage limits may be larger than 1TB per allocation for a startup if well-justified. 5. Can I renew a startup allocation? If your SU needs are equal to or less than the maximum startup values (see item 4 just above) you may renew your startup allocation. If you need a signficantly larger amound of SUs for any of the resources, you will need to pursue a research allocation 6. I\u2019m running out of Service Units (SUs) or storage. How do I request more? If you already have an XSEDE allocation and need to request additional service units (SUs), the PI, co-PI, or delegate may submit a request via the XSEDE User Portal. For instructions on how to submit the request, see Requesting additional SUs, other Jetstream resources, or storage for Jetstream \u2013 Supplemental Allocations . 7. I am at or exceeding the quota limits for my allocation. How do I request additional resources such as CPUs and memory? You may contact help@xsede.org or help@jetstream-cloud.org with those requests. It\u2019s important to note that Jetstream Trial Allocation quotas are fixed and will NOT be increased under any circumstances. For other allocation types, justification will be required and will be granted at the discretion of the Jetstream2 staff based on the justification and available resources. Please note that large memory and GPU resources are limited so requests for those will require strong justification for success or partial success. We strive to make resources available to all researchers that require them, so striking a balance between the needs of one versus many is often necessary. 8. Can you extend my allocation for me or give me access to my allocation for just a few days/weeks/months more? If your allocation is expired or out of SUs, you may request an extension, renewal, or supplement. Please see one of the following links: Requesting additional SUs, other Jetstream resources, or storage for Jetstream \u2013 Supplemental Allocations Jetstream2 Allocation Extensions and Renewals Jetstream2 staff are unable to take these actions on your behalf.","title":"Allocations"},{"location":"faq/alloc/#jetstream2-allocations-faq","text":"1. Is there an overview of the types of allocations available as well as any restrictions those allocations have? The Getting Started guide describes the process of getting onto XSEDE, applying for allocations and using XSEDE resources. To review the types of allocations XSEDE and the process to get an allocation, here are some links you might find useful: Allocations overview Types of allocations, eligibility, details Allocation policies Submit and manage allocation requests Proposal deadlines 2. Is there a example or demonstration of how to get an allocation that I could follow? The Research Allocation page has information and links for writing a successful Jetstream2 research allocation request. There is a Cornell Virtual Workshop (CVW) on getting a Research Allocation for Jetstream here: https://cvw.cac.cornell.edu/JetstreamReq/. It is not updated for Jetstream2 though the principles are the same. 3. How do I let other XSEDE accounts use my allocation? You can add users to (or remove them from) your XSEDE allocation via the XSEDE User Portal. Users must have already created their XSEDE accounts before they can be added to an allocation. To add users to, or remove them from, an active Extreme Science and Engineering Discovery Environment ( XSEDE ) allocation, the principal investigator, co-principal investigator, or allocation manager can follow the instructions here: https://portal.xsede.org/allocations/managing Please note that it can take up to four hours for users added to an allocation to become active. 4. How often can I get a startup allocation? Applications for startup allocations will only be accepted once. If you have modest needs that are equal or less than startup values, you may renew your startup allocation. If you need a larger allocation, it is best to apply for a research allocation . Maximum Startup/Campus Champion Allocation values for each resource are: * Jetstream2 CPU - 200,000 SUs * Jetstream2 Large Memory - 400,000 SUs * Jetstream2 GPU - 600,000 SUs * Jetstream2 Storage - 1TB default* Storage limits may be larger than 1TB per allocation for a startup if well-justified. 5. Can I renew a startup allocation? If your SU needs are equal to or less than the maximum startup values (see item 4 just above) you may renew your startup allocation. If you need a signficantly larger amound of SUs for any of the resources, you will need to pursue a research allocation 6. I\u2019m running out of Service Units (SUs) or storage. How do I request more? If you already have an XSEDE allocation and need to request additional service units (SUs), the PI, co-PI, or delegate may submit a request via the XSEDE User Portal. For instructions on how to submit the request, see Requesting additional SUs, other Jetstream resources, or storage for Jetstream \u2013 Supplemental Allocations . 7. I am at or exceeding the quota limits for my allocation. How do I request additional resources such as CPUs and memory? You may contact help@xsede.org or help@jetstream-cloud.org with those requests. It\u2019s important to note that Jetstream Trial Allocation quotas are fixed and will NOT be increased under any circumstances. For other allocation types, justification will be required and will be granted at the discretion of the Jetstream2 staff based on the justification and available resources. Please note that large memory and GPU resources are limited so requests for those will require strong justification for success or partial success. We strive to make resources available to all researchers that require them, so striking a balance between the needs of one versus many is often necessary. 8. Can you extend my allocation for me or give me access to my allocation for just a few days/weeks/months more? If your allocation is expired or out of SUs, you may request an extension, renewal, or supplement. Please see one of the following links: Requesting additional SUs, other Jetstream resources, or storage for Jetstream \u2013 Supplemental Allocations Jetstream2 Allocation Extensions and Renewals Jetstream2 staff are unable to take these actions on your behalf.","title":"Jetstream2 Allocations FAQ"},{"location":"faq/gateways/","text":"Science Gateways and related services FAQ \u00b6 How can I federate logins on my gateway or JupyterHub service? \u00b6 The recommended and supported means for doing so on Jetstream2 is to use the Custos software provided by the Cyberinfrastructure Integration Research Center Information on federating your gateway or Jupyter service is outlined in on Federating a gateway or JupyterHub on Jetstream2","title":"Gateways"},{"location":"faq/gateways/#science-gateways-and-related-services-faq","text":"","title":"Science Gateways and related services FAQ"},{"location":"faq/gateways/#how-can-i-federate-logins-on-my-gateway-or-jupyterhub-service","text":"The recommended and supported means for doing so on Jetstream2 is to use the Custos software provided by the Cyberinfrastructure Integration Research Center Information on federating your gateway or Jupyter service is outlined in on Federating a gateway or JupyterHub on Jetstream2","title":"How can I federate logins on my gateway or JupyterHub service?"},{"location":"faq/general-faq/","text":"Jetstream2 FAQ Home \u00b6 For specific FAQs, see the following pages: Early Operations FAQ Allocations FAQ Troubleshooting Security FAQ Software FAQ Gateways FAQ General FAQs \u00b6 I need a root disk larger than the maximum size for Jetstream2 instances. Can you create a custom flavor for me? \u00b6 In OpenStack, flavors define the compute, memory, and storage capacity of instances. We may also refer to it in support tickets or documentation as the VM\u2019s size. We won\u2019t create custom flavors, but there are ways to get larger root disks. You can review the flavors and see if moving up from one of the smaller VMs to a slightly larger one would yield a larger root disk. The other option is to use a custom sized root disk using what Openstack calls \u201cboot from volume\u201d, or a \u201cvolume-backed\u201d instance. What this means is that instead of an ephemeral boot disk for the instance, a volume is used to be the root disk. There are several upsides to this: You can have a root disk large enough to fit your needs The disk becomes reusable as a volume if necessary Shelving the instance is extremely fast compared to shelving a typical instance The downside is that using boot from volume will count against your Jetstream2-Storage allocation whereas root disks do not. Instructions for using boot from volume are here: Exosphere: Choose a Root Disk Size Cacao (link coming) Horizon (link coming) CLI (link coming) How do I get access to one of the Jetstream2 regional clouds? \u00b6 The regional clouds of Jetstream2 (Arizona State University, Cornell University, University of Hawai\u2019i, and Texas Advanced Computing Center) are available via invitation only. They have limited resources and the local PIs have discretion as to which projects may run there. All allocations gain access to the primary Jetstream2 cloud at Indiana University. If you have an allocation and wish to have it added to a regional cloud, you can request access using this form As the regional clouds have autonomy over their access and because resources are limited, it\u2019s important to note that not all requests may be accommodated. Will there be Microsoft Windows support on Jetstream2 ? \u00b6 Microsoft Windows is not officially supported on Jetstream2. We will be making a limited number of images available for experimental use. It is not known at this time whether GPUs will work on Microsoft-based instances. We will test this as time permits. More information may be found on the Microsoft Windows on Jetstream2 page. How do I share a volume between virtual machines? \u00b6 You can\u2019t easily share volumes in OpenStack without deploying a Shared File System service. However, the native Openstack Manila filesystems-as-a-service option is available. Instructions for using manila on Jetstream2 are here - Manila - Filesystems-as-a-service - on Jetstream2 Can I set the password for a user on my virtual machine? \u00b6 We generally don\u2019t recommend using password authentication on Jetstream2, recommending that you use SSH keys for access. That said, if you need to set a password for console access or for some other reason, you can do it like this: sudo passwd *username*","title":"General FAQs"},{"location":"faq/general-faq/#jetstream2-faq-home","text":"For specific FAQs, see the following pages: Early Operations FAQ Allocations FAQ Troubleshooting Security FAQ Software FAQ Gateways FAQ","title":"Jetstream2 FAQ Home"},{"location":"faq/general-faq/#general-faqs","text":"","title":"General FAQs"},{"location":"faq/general-faq/#i-need-a-root-disk-larger-than-the-maximum-size-for-jetstream2-instances-can-you-create-a-custom-flavor-for-me","text":"In OpenStack, flavors define the compute, memory, and storage capacity of instances. We may also refer to it in support tickets or documentation as the VM\u2019s size. We won\u2019t create custom flavors, but there are ways to get larger root disks. You can review the flavors and see if moving up from one of the smaller VMs to a slightly larger one would yield a larger root disk. The other option is to use a custom sized root disk using what Openstack calls \u201cboot from volume\u201d, or a \u201cvolume-backed\u201d instance. What this means is that instead of an ephemeral boot disk for the instance, a volume is used to be the root disk. There are several upsides to this: You can have a root disk large enough to fit your needs The disk becomes reusable as a volume if necessary Shelving the instance is extremely fast compared to shelving a typical instance The downside is that using boot from volume will count against your Jetstream2-Storage allocation whereas root disks do not. Instructions for using boot from volume are here: Exosphere: Choose a Root Disk Size Cacao (link coming) Horizon (link coming) CLI (link coming)","title":"I need a root disk larger than the maximum size for Jetstream2 instances. Can you create a custom flavor for me?"},{"location":"faq/general-faq/#how-do-i-get-access-to-one-of-the-jetstream2-regional-clouds","text":"The regional clouds of Jetstream2 (Arizona State University, Cornell University, University of Hawai\u2019i, and Texas Advanced Computing Center) are available via invitation only. They have limited resources and the local PIs have discretion as to which projects may run there. All allocations gain access to the primary Jetstream2 cloud at Indiana University. If you have an allocation and wish to have it added to a regional cloud, you can request access using this form As the regional clouds have autonomy over their access and because resources are limited, it\u2019s important to note that not all requests may be accommodated.","title":"How do I get access to one of the Jetstream2 regional clouds?"},{"location":"faq/general-faq/#will-there-be-microsoft-windows-support-on-jetstream2","text":"Microsoft Windows is not officially supported on Jetstream2. We will be making a limited number of images available for experimental use. It is not known at this time whether GPUs will work on Microsoft-based instances. We will test this as time permits. More information may be found on the Microsoft Windows on Jetstream2 page.","title":"Will there be Microsoft Windows support on Jetstream2 ?"},{"location":"faq/general-faq/#how-do-i-share-a-volume-between-virtual-machines","text":"You can\u2019t easily share volumes in OpenStack without deploying a Shared File System service. However, the native Openstack Manila filesystems-as-a-service option is available. Instructions for using manila on Jetstream2 are here - Manila - Filesystems-as-a-service - on Jetstream2","title":"How do I share a volume between virtual machines?"},{"location":"faq/general-faq/#can-i-set-the-password-for-a-user-on-my-virtual-machine","text":"We generally don\u2019t recommend using password authentication on Jetstream2, recommending that you use SSH keys for access. That said, if you need to set a password for console access or for some other reason, you can do it like this: sudo passwd *username*","title":"Can I set the password for a user on my virtual machine?"},{"location":"faq/js2-earlyops-faq/","text":"Jetstream2 Early Operations FAQ \u00b6 Early operations has started on Jetstream2! Official start date is February 9, 2022. We will continue early operations until acceptance is complete by the National Science Foundation. We will post more complete timelines as we will post them here and all official announcements will be made via XSEDE user news . 1. When will Jetstream2 be available for XSEDE research allocations? Jetstream2 is presently available via the XSEDE Resource Allocation System for research allocation submissions. Please see this page for more information: XSEDE Research Allocations 2. When will Jetstream2 be available for XSEDE startup and education allocations? Jetstream2 is presently available via the XSEDE Resource Allocation System for startup, education, and Champion allocation submissions. Please see these pages for more information: Startup Allocations Education Allocations Jetstream2 Allocatable Resources The system has been running with researchers, educators, and students since February 9, 2022. We feel the system is stable and ready for general usage. Please watch this space and XSEDE user news and mailing lists for announcements about Jetstream2 operations, access, and migration. To ensure that you are receiving all updates to your inbox, login on the page above to manage your XSEDE User News subscriptions. All Jetstream users should be added when they are placed on an allocation, but you may wish to verify so that you don\u2019t miss any important operations updates. For information about migrating from Jetstream1 to Jetstream2, you may want to also monitor Migrating from Jetstream1 to Jetstream2 3. When will Jetstream2 be available for Jetstream2 Trial allocations? Jetstream2 Trial Allocations will be made available after Jetstream2 goes into production. They will likely not be available in early user operations. Until Jetstream2 completes National Science Foundation (NSF) acceptance testing, timelines remain uncertain. Once acceptance testing has been completed, documented, presented and approved by the NSF, we will be able to publish a more definitive timeline. Please watch this space and XSEDE user news and mailing lists for announcements about Jetstream2 operations, access, and migration. To ensure that you are receiving all updates to your inbox, login on the page above to manage your XSEDE User News subscriptions. All Jetstream users should be added when they are placed on an allocation, but you may wish to verify so that you don\u2019t miss any important operations updates.","title":"FAQ"},{"location":"faq/js2-earlyops-faq/#jetstream2-early-operations-faq","text":"Early operations has started on Jetstream2! Official start date is February 9, 2022. We will continue early operations until acceptance is complete by the National Science Foundation. We will post more complete timelines as we will post them here and all official announcements will be made via XSEDE user news . 1. When will Jetstream2 be available for XSEDE research allocations? Jetstream2 is presently available via the XSEDE Resource Allocation System for research allocation submissions. Please see this page for more information: XSEDE Research Allocations 2. When will Jetstream2 be available for XSEDE startup and education allocations? Jetstream2 is presently available via the XSEDE Resource Allocation System for startup, education, and Champion allocation submissions. Please see these pages for more information: Startup Allocations Education Allocations Jetstream2 Allocatable Resources The system has been running with researchers, educators, and students since February 9, 2022. We feel the system is stable and ready for general usage. Please watch this space and XSEDE user news and mailing lists for announcements about Jetstream2 operations, access, and migration. To ensure that you are receiving all updates to your inbox, login on the page above to manage your XSEDE User News subscriptions. All Jetstream users should be added when they are placed on an allocation, but you may wish to verify so that you don\u2019t miss any important operations updates. For information about migrating from Jetstream1 to Jetstream2, you may want to also monitor Migrating from Jetstream1 to Jetstream2 3. When will Jetstream2 be available for Jetstream2 Trial allocations? Jetstream2 Trial Allocations will be made available after Jetstream2 goes into production. They will likely not be available in early user operations. Until Jetstream2 completes National Science Foundation (NSF) acceptance testing, timelines remain uncertain. Once acceptance testing has been completed, documented, presented and approved by the NSF, we will be able to publish a more definitive timeline. Please watch this space and XSEDE user news and mailing lists for announcements about Jetstream2 operations, access, and migration. To ensure that you are receiving all updates to your inbox, login on the page above to manage your XSEDE User News subscriptions. All Jetstream users should be added when they are placed on an allocation, but you may wish to verify so that you don\u2019t miss any important operations updates.","title":"Jetstream2 Early Operations FAQ"},{"location":"faq/security/","text":"Security FAQ \u00b6 Do I need to patch/update my VMs? \u00b6 The featured images provided by the Jetstream team have unattended security updates enabled. Instances will not reboot, but they will apply any update marked as a security update. It\u2019s still a good idea to update your VM periodically. CentOS 7: sudo yum update Rocky 8: sudo dnf update Ubuntu: sudo apt-get update ; sudo apt-get upgrade If the kernel or glibc/libc packages are being updated, rebooting is necessary to implement those updates Always run updates before taking snapshots so your new instances will have the latest patches in place. Do I need to secure services running on my VMs? \u00b6 If you have a public IP (also called a floating IP in OpenStack terminology), your instance may be exposed to the internet. While you can use security groups or host-based firewalls like iptables or UFW to limit access, it\u2019s best to take a posture of defense in depth and always secure any listening services. We strongly recommend limiting access to any service ports via security group and/or firewall making sure any non-public services like databases have secure passwords for all accounts. If your service only needs to be accessible to the VM it\u2019s running on, check the documentation to see if it can be bound only to the localhost port. (Not all services have this ability.) Do I need to run a firewall on my VM? \u00b6 Jetstream2 staff encourages a defense-in-depth approach to security. This potentially involves several methods of restricting access and securing instances. Firewalls are not enabled by default on Jetstream2 instances. Depending on the user interface you launched your instance from, you may have different security groups established for your instance. (See What is the default security profile for Jetstream2 VMs? on this page for more information on that.) We encourage keeping your instances patched, rebooting as needed for any kernel or glibc patches, limiting access to all services as much as possible, utilizing security groups if your interface allows it, and running your own host-based firewall if you\u2019re comfortable administering it. Please refer to Jetstream2 Virtual Machine Firewalls for more information. What is the default security profile for Jetstream2 VMs? \u00b6 That depends on the interface. The CLI and Horizon by default allow egress only. You have to apply the appropriate security groups for ingress. Please refer to the CLI or Horizon for more information on managing security groups in those interfaces. For Exosphere, the default security group allows all egress and inbound access. For CACAO, the default security group will be announced when it is available for general usage. In Exosphere, there is a way to get a passphrase for any instance. Can I prevent other users on my allocation from accessing my instance(s)? \u00b6 The way Jetstream2 is currently architected, all users on an allocation have access to all resources on the allocation. By default, Exosphere hides some resources created by other users, but this is only a convenience and it cannot assure separation of access. It is possible to make it less straightforward for another user on the same allocation to access your running instance. You can do this by changing the password for the default exouser account. Changing the password does not prevent any access, but makes it more difficult. Note that currently, changing the exouser account password will break Web Shell, Web Desktop, and some other Exosphere-powered instance interactions. This may change in the future.) We note how to change a user password here: https://docs.jetstream-cloud.org/faq/general-faq/#can-i-set-the-password-for-a-user-on-my-virtual-machine you can do: sudo passwd exouser and that solves the issue of them being able to access your instance using the credential listed on the Exosphere page. While you can use the console option still, we HIGHLY suggest utilizing ssh keys for your instances to ensure you have access. That\u2019s covered here: https://docs.jetstream-cloud.org/ui/exo/create_instance/ under the advanced options. You can also manually add your key to an already running instance.","title":"Security"},{"location":"faq/security/#security-faq","text":"","title":"Security FAQ"},{"location":"faq/security/#do-i-need-to-patchupdate-my-vms","text":"The featured images provided by the Jetstream team have unattended security updates enabled. Instances will not reboot, but they will apply any update marked as a security update. It\u2019s still a good idea to update your VM periodically. CentOS 7: sudo yum update Rocky 8: sudo dnf update Ubuntu: sudo apt-get update ; sudo apt-get upgrade If the kernel or glibc/libc packages are being updated, rebooting is necessary to implement those updates Always run updates before taking snapshots so your new instances will have the latest patches in place.","title":"Do I need to patch/update my VMs?"},{"location":"faq/security/#do-i-need-to-secure-services-running-on-my-vms","text":"If you have a public IP (also called a floating IP in OpenStack terminology), your instance may be exposed to the internet. While you can use security groups or host-based firewalls like iptables or UFW to limit access, it\u2019s best to take a posture of defense in depth and always secure any listening services. We strongly recommend limiting access to any service ports via security group and/or firewall making sure any non-public services like databases have secure passwords for all accounts. If your service only needs to be accessible to the VM it\u2019s running on, check the documentation to see if it can be bound only to the localhost port. (Not all services have this ability.)","title":"Do I need to secure services running on my VMs?"},{"location":"faq/security/#do-i-need-to-run-a-firewall-on-my-vm","text":"Jetstream2 staff encourages a defense-in-depth approach to security. This potentially involves several methods of restricting access and securing instances. Firewalls are not enabled by default on Jetstream2 instances. Depending on the user interface you launched your instance from, you may have different security groups established for your instance. (See What is the default security profile for Jetstream2 VMs? on this page for more information on that.) We encourage keeping your instances patched, rebooting as needed for any kernel or glibc patches, limiting access to all services as much as possible, utilizing security groups if your interface allows it, and running your own host-based firewall if you\u2019re comfortable administering it. Please refer to Jetstream2 Virtual Machine Firewalls for more information.","title":"Do I need to run a firewall on my VM?"},{"location":"faq/security/#what-is-the-default-security-profile-for-jetstream2-vms","text":"That depends on the interface. The CLI and Horizon by default allow egress only. You have to apply the appropriate security groups for ingress. Please refer to the CLI or Horizon for more information on managing security groups in those interfaces. For Exosphere, the default security group allows all egress and inbound access. For CACAO, the default security group will be announced when it is available for general usage.","title":"What is the default security profile for Jetstream2 VMs? "},{"location":"faq/security/#in-exosphere-there-is-a-way-to-get-a-passphrase-for-any-instance-can-i-prevent-other-users-on-my-allocation-from-accessing-my-instances","text":"The way Jetstream2 is currently architected, all users on an allocation have access to all resources on the allocation. By default, Exosphere hides some resources created by other users, but this is only a convenience and it cannot assure separation of access. It is possible to make it less straightforward for another user on the same allocation to access your running instance. You can do this by changing the password for the default exouser account. Changing the password does not prevent any access, but makes it more difficult. Note that currently, changing the exouser account password will break Web Shell, Web Desktop, and some other Exosphere-powered instance interactions. This may change in the future.) We note how to change a user password here: https://docs.jetstream-cloud.org/faq/general-faq/#can-i-set-the-password-for-a-user-on-my-virtual-machine you can do: sudo passwd exouser and that solves the issue of them being able to access your instance using the credential listed on the Exosphere page. While you can use the console option still, we HIGHLY suggest utilizing ssh keys for your instances to ensure you have access. That\u2019s covered here: https://docs.jetstream-cloud.org/ui/exo/create_instance/ under the advanced options. You can also manually add your key to an already running instance.","title":"In Exosphere, there is a way to get a passphrase for any instance. Can I prevent other users on my allocation from accessing my instance(s)?"},{"location":"faq/software/","text":"Software FAQ \u00b6 Jetstream2 provides some software packages for all researchers. The sheer number of available packages across all of the scientific disciplines makes it impossible to meet the desires of everyone in installing and maintaining software for research. How can I see what software is maintained by the Jetstream2 staff? \u00b6 The Jetstream2 software collection is documented here : Software Collection How do I access the software in this collection? \u00b6 This software store is mounted automatically when the instance is launched. Instructions for using it are in the software collection information page Can I request to have other software packages added to the software collection? \u00b6 Since everyone on Jetstream2 allocations potentially has root access on their VMs, you may install any software you need on a per VM basis. You can then snapshot and save your customized images. If you feel a package would be broadly useful to the Jetstream User community, you may request software using the contact form and staff will review the package. Please do keep in mind that we cannot add and maintain all requested software packages. We will review any requests and track those requests.","title":"Software"},{"location":"faq/software/#software-faq","text":"Jetstream2 provides some software packages for all researchers. The sheer number of available packages across all of the scientific disciplines makes it impossible to meet the desires of everyone in installing and maintaining software for research.","title":"Software FAQ"},{"location":"faq/software/#how-can-i-see-what-software-is-maintained-by-the-jetstream2-staff","text":"The Jetstream2 software collection is documented here : Software Collection","title":"How can I see what software is maintained by the Jetstream2 staff?"},{"location":"faq/software/#how-do-i-access-the-software-in-this-collection","text":"This software store is mounted automatically when the instance is launched. Instructions for using it are in the software collection information page","title":"How do I access the software in this collection?"},{"location":"faq/software/#can-i-request-to-have-other-software-packages-added-to-the-software-collection","text":"Since everyone on Jetstream2 allocations potentially has root access on their VMs, you may install any software you need on a per VM basis. You can then snapshot and save your customized images. If you feel a package would be broadly useful to the Jetstream User community, you may request software using the contact form and staff will review the package. Please do keep in mind that we cannot add and maintain all requested software packages. We will review any requests and track those requests.","title":"Can I request to have other software packages added to the software collection?"},{"location":"faq/trouble/","text":"Troubleshooting \u00b6 Jetstream2 requires that you be on a valid XSEDE allocation . If you are having issues accessing the various Jetstream2 interfaces, please first verify that you\u2019re on a valid allocation via the XSEDE User Portal Troubleshooting Jetstream2 Interfaces: Exosphere Cacao Horizon CLI General Troubleshooting: In Ubuntu 20 web desktop, I can\u2019t load items from the JS2 Software Collection. \u00b6 The Jetstream2 Software Collection requires Lmod modules to work. By default, the Gnome terminal in the the Ubuntu web desktop does not act like a login shell \u2013 meaning it doesn\u2019t source the normal Bash login/environment files setting up your path and other environment variables. We\u2019re looking for a longer term solution for this, but in the meantime, you can fix this in the terminal preferences. With Terminal as the active application: Go to Edit -> Profile Preferences. Select the Title and Command tab. Check the \u201cRun command as login shell\u201d checkbox You\u2019ll need to start a new terminal window, but that new session should allow you to do commands like module avail There is a known issue with suspending GPU instances \u00b6 We will update this Status IO Incident with details/ There is an issue/bug with suspending GPU instances with the version of libvirt Jetstream2 is using for virtualization. DO NOT SUSPEND GPU instances. We will have to upgrade the compute nodes to resolve it. This is on the near-term timeline but we do not have a precise date at this time. In the meantime, please only use stop or shelve with GPU instances.","title":"Troubleshooting"},{"location":"faq/trouble/#troubleshooting","text":"Jetstream2 requires that you be on a valid XSEDE allocation . If you are having issues accessing the various Jetstream2 interfaces, please first verify that you\u2019re on a valid allocation via the XSEDE User Portal Troubleshooting Jetstream2 Interfaces: Exosphere Cacao Horizon CLI General Troubleshooting:","title":"Troubleshooting"},{"location":"faq/trouble/#in-ubuntu-20-web-desktop-i-cant-load-items-from-the-js2-software-collection","text":"The Jetstream2 Software Collection requires Lmod modules to work. By default, the Gnome terminal in the the Ubuntu web desktop does not act like a login shell \u2013 meaning it doesn\u2019t source the normal Bash login/environment files setting up your path and other environment variables. We\u2019re looking for a longer term solution for this, but in the meantime, you can fix this in the terminal preferences. With Terminal as the active application: Go to Edit -> Profile Preferences. Select the Title and Command tab. Check the \u201cRun command as login shell\u201d checkbox You\u2019ll need to start a new terminal window, but that new session should allow you to do commands like module avail","title":"In Ubuntu 20 web desktop, I can't load items from the JS2 Software Collection."},{"location":"faq/trouble/#there-is-a-known-issue-with-suspending-gpu-instances","text":"We will update this Status IO Incident with details/ There is an issue/bug with suspending GPU instances with the version of libvirt Jetstream2 is using for virtualization. DO NOT SUSPEND GPU instances. We will have to upgrade the compute nodes to resolve it. This is on the near-term timeline but we do not have a precise date at this time. In the meantime, please only use stop or shelve with GPU instances.","title":"There is a known issue with suspending GPU instances"},{"location":"general/adduser/","text":"Adding Users to your VM \u00b6 These steps will let the user that you create ssh to a running instance using a password you set. The user can reset the password once they login and/or add their ssh keys. Step-by-step guide \u00b6 All steps to be run as root or using sudo. should be replaced with an actual username Sudo or otherwise become root: sudo su - Create the user: adduser <username> Assign a temporary password: passwd <username> As an optional step, you can add the user to any other groups as needed. We\u2019ll use the docker group as an example. Add the user to group \u201cdocker\u201d : usermod -a -G docker <username>","title":"Add User to a VM"},{"location":"general/adduser/#adding-users-to-your-vm","text":"These steps will let the user that you create ssh to a running instance using a password you set. The user can reset the password once they login and/or add their ssh keys.","title":"Adding Users to your VM"},{"location":"general/adduser/#step-by-step-guide","text":"All steps to be run as root or using sudo. should be replaced with an actual username Sudo or otherwise become root: sudo su - Create the user: adduser <username> Assign a temporary password: passwd <username> As an optional step, you can add the user to any other groups as needed. We\u2019ll use the docker group as an example. Add the user to group \u201cdocker\u201d : usermod -a -G docker <username>","title":"Step-by-step guide"},{"location":"general/bestpractice/","text":"Best Practices for Jetstream2 \u00b6 Many of the best practices for using Jetstream2 are laid out in the Security FAQ . These include such things as: Updating your virtual machines Running host-based firewalls Securing services Using the latest Linux releases available on Jetstream2 \u00b6 Using the latest version of Ubuntu or RPM-based (e.g. CentOS, Rocky, Alma) Linux distribution is highly encouraged. While we do always maintain one revision back from the current release, we highly encourage people to use the most recent operating system that will meet their needs. As such, we will also provide new Ubuntu and Rocky releases as they become available, are tested thoroughly, and are found to work with the Jetstream2 cloud. We will retire the oldest version as we bring the newest version into service. Storage Best Practices \u00b6 These were taken from Ceph\u2019s Application Best Practices for Distributed File Systems document The brief summary is: Keep directories as small as possible - breaking up data into chunks in multiple directories when possible. Don\u2019t do expensive operations like stat or other file metadata operations on large directories if you don\u2019t need to Use symlinks instead of hard links 1. Running ls -l or other directory operations When you run \u201cls -l\u201d, the ls program is first doing a directory listing, and then calling stat on every file in the directory. This is usually far in excess of what an application really needs, and it can be slow for large directories. If you don\u2019t really need all this metadata for each file, then use a plain ls. If another client is currently extending files in the listed directory, then an ls -l may take an exceptionally long time to complete, as the lister must wait for the writer to flush data in order to do a valid read of the every file\u2019s size. So unless you really need to know the exact size of every file in the directory, just don\u2019t do it! This would also apply to any application code that was directly issuing stat system calls on files being appended from another node. 2. Very large directories Do you really need that 10,000,000 file directory? While directory fragmentation enables CephFS to handle it, it is always going to be less efficient than splitting your files into more modest-sized directories. Even standard userspace tools can become quite slow when operating on very large directories. For example, the default behaviour of ls is to give an alphabetically ordered result, but readdir system calls do not give an ordered result (this is true in general, not just with CephFS). So when you ls on a million file directory, it is loading a list of a million names into memory, sorting the list, then writing it out to the display. We\u2019ve found on Jetstream2 that more than 10,000 items in a single directory impacts performance and that in excess of 100,000 items in a single directory might cause unexpected behavior that could directly impact your VM operations as well as the operations of the Ceph store, potentially impacting ALL users. 3. Hard links Hard links have an intrinsic cost in terms of the internal housekeeping that a file system has to do to keep two references to the same data. In CephFS there is a particular performance cost, because with normal files the inode is embedded in the directory (i.e. there is no extra fetch of the inode after looking up the path).","title":"Best Practices"},{"location":"general/bestpractice/#best-practices-for-jetstream2","text":"Many of the best practices for using Jetstream2 are laid out in the Security FAQ . These include such things as: Updating your virtual machines Running host-based firewalls Securing services","title":"Best Practices for Jetstream2"},{"location":"general/bestpractice/#using-the-latest-linux-releases-available-on-jetstream2","text":"Using the latest version of Ubuntu or RPM-based (e.g. CentOS, Rocky, Alma) Linux distribution is highly encouraged. While we do always maintain one revision back from the current release, we highly encourage people to use the most recent operating system that will meet their needs. As such, we will also provide new Ubuntu and Rocky releases as they become available, are tested thoroughly, and are found to work with the Jetstream2 cloud. We will retire the oldest version as we bring the newest version into service.","title":"Using the latest Linux releases available on Jetstream2"},{"location":"general/bestpractice/#storage-best-practices","text":"These were taken from Ceph\u2019s Application Best Practices for Distributed File Systems document The brief summary is: Keep directories as small as possible - breaking up data into chunks in multiple directories when possible. Don\u2019t do expensive operations like stat or other file metadata operations on large directories if you don\u2019t need to Use symlinks instead of hard links 1. Running ls -l or other directory operations When you run \u201cls -l\u201d, the ls program is first doing a directory listing, and then calling stat on every file in the directory. This is usually far in excess of what an application really needs, and it can be slow for large directories. If you don\u2019t really need all this metadata for each file, then use a plain ls. If another client is currently extending files in the listed directory, then an ls -l may take an exceptionally long time to complete, as the lister must wait for the writer to flush data in order to do a valid read of the every file\u2019s size. So unless you really need to know the exact size of every file in the directory, just don\u2019t do it! This would also apply to any application code that was directly issuing stat system calls on files being appended from another node. 2. Very large directories Do you really need that 10,000,000 file directory? While directory fragmentation enables CephFS to handle it, it is always going to be less efficient than splitting your files into more modest-sized directories. Even standard userspace tools can become quite slow when operating on very large directories. For example, the default behaviour of ls is to give an alphabetically ordered result, but readdir system calls do not give an ordered result (this is true in general, not just with CephFS). So when you ls on a million file directory, it is loading a list of a million names into memory, sorting the list, then writing it out to the display. We\u2019ve found on Jetstream2 that more than 10,000 items in a single directory impacts performance and that in excess of 100,000 items in a single directory might cause unexpected behavior that could directly impact your VM operations as well as the operations of the Ceph store, potentially impacting ALL users. 3. Hard links Hard links have an intrinsic cost in terms of the internal housekeeping that a file system has to do to keep two references to the same data. In CephFS there is a particular performance cost, because with normal files the inode is embedded in the directory (i.e. there is no extra fetch of the inode after looking up the path).","title":"Storage Best Practices"},{"location":"general/docker/","text":"Containers on Jetstream2 \u00b6 Containers are isolated environments in which to run your applications. They created by objects called images. An image is defined as a read only template with instructions for creating a container. These instructions define everything a container needs; Software, dependencies, system libraries, environment variables, configuration files, etc. Jetstream2 supports Docker and Apptainer/Singularity. Other container software may be included in the future. Jetstream Featured Images all include Docker as part of the build. Additionally, the NVIDIA Docker2 container environment is also built in so that all Featured images may be used for GPU usage or using NVIDIA containers for code development. Apptainer (previously known as Singularity) is installed as part of the Jetstream Software Collection . You can access Apptainer from any Jetstream Featured Image by doing: module load apptainer Some basic Docker commands: \u00b6 docker -version - will give the version of Docker is installed. docker pull <image_name> - will download the image from dockerhub. docker run <image_name> - will run the image pulled from dockerhub to create a container. If you don\u2019t have a local copy of the image, the run command will pull and then run the image to create a container. docker ps - Process status of containers. If no container is running, you get a blank line. docker ps -a - process status of all containers. docker exec -it <container_id> bash - allows you to run a command in the docker container. The -it flag provides an interactive tty (shell) within the container. docker stop <container_id> - shuts down a container. docker build <path_to_docker_file> - Builds an image from the specified docker file. docker push <docker_hub_username/image_name> - pushes an image to the docker hub repository. For more information, please see Docker: Orientation and Setup","title":"Docker"},{"location":"general/docker/#containers-on-jetstream2","text":"Containers are isolated environments in which to run your applications. They created by objects called images. An image is defined as a read only template with instructions for creating a container. These instructions define everything a container needs; Software, dependencies, system libraries, environment variables, configuration files, etc. Jetstream2 supports Docker and Apptainer/Singularity. Other container software may be included in the future. Jetstream Featured Images all include Docker as part of the build. Additionally, the NVIDIA Docker2 container environment is also built in so that all Featured images may be used for GPU usage or using NVIDIA containers for code development. Apptainer (previously known as Singularity) is installed as part of the Jetstream Software Collection . You can access Apptainer from any Jetstream Featured Image by doing: module load apptainer","title":"Containers on Jetstream2"},{"location":"general/docker/#some-basic-docker-commands","text":"docker -version - will give the version of Docker is installed. docker pull <image_name> - will download the image from dockerhub. docker run <image_name> - will run the image pulled from dockerhub to create a container. If you don\u2019t have a local copy of the image, the run command will pull and then run the image to create a container. docker ps - Process status of containers. If no container is running, you get a blank line. docker ps -a - process status of all containers. docker exec -it <container_id> bash - allows you to run a command in the docker container. The -it flag provides an interactive tty (shell) within the container. docker stop <container_id> - shuts down a container. docker build <path_to_docker_file> - Builds an image from the specified docker file. docker push <docker_hub_username/image_name> - pushes an image to the docker hub repository. For more information, please see Docker: Orientation and Setup","title":"Some basic Docker commands:"},{"location":"general/export/","text":"Jetstream2 Export Control Guidance \u00b6 Countries with heightened restrictions \u00b6 Some countries have some heightened control over certain types of research and technology. Please consult with your institution\u2019s export control office to ensure that you are in full compliance with US regulations and institutional policies when working with countries under heightened controls and/or when working with research or information that may be limited by export control law. The US Department of Commerce, Bureau of Industrial Security (BIS) publishes a list of EAR Country Group designations. Country Group D (countries of national security concern to the United States) face these heightened control restrictions and are included in this document of country groups. (Please note that Country Group D begins on page 6 of this document.) Sanctioned countries (not eligible to use Jetstream2) \u00b6 In accordance to federal guidelines, Jetstream2 will not be accessible to IP ranges from the following countries: Cuba Iran North Korea Syria Crimea Region of the Ukraine The Bureau of Industry and Security (BIS) implements U.S. Government certain sanctions against these countries pursuant to the Export Administration Regulations (EAR), either unilaterally or to implement United Nations Security Council Resolutions. The US Department of Commerce, Bureau of Industrial Security (BIS) publishes a list of EAR Country Group designations. Country Groups E1 and E2 identify embargoed countries subject to comprehensive restrictions and are included in this document of country groups. (Please note that Country Group E begins on page 8 of this document.)","title":"Export Control Guidance"},{"location":"general/export/#jetstream2-export-control-guidance","text":"","title":"Jetstream2 Export Control Guidance"},{"location":"general/export/#countries-with-heightened-restrictions","text":"Some countries have some heightened control over certain types of research and technology. Please consult with your institution\u2019s export control office to ensure that you are in full compliance with US regulations and institutional policies when working with countries under heightened controls and/or when working with research or information that may be limited by export control law. The US Department of Commerce, Bureau of Industrial Security (BIS) publishes a list of EAR Country Group designations. Country Group D (countries of national security concern to the United States) face these heightened control restrictions and are included in this document of country groups. (Please note that Country Group D begins on page 6 of this document.)","title":"Countries with heightened restrictions"},{"location":"general/export/#sanctioned-countries-not-eligible-to-use-jetstream2","text":"In accordance to federal guidelines, Jetstream2 will not be accessible to IP ranges from the following countries: Cuba Iran North Korea Syria Crimea Region of the Ukraine The Bureau of Industry and Security (BIS) implements U.S. Government certain sanctions against these countries pursuant to the Export Administration Regulations (EAR), either unilaterally or to implement United Nations Security Council Resolutions. The US Department of Commerce, Bureau of Industrial Security (BIS) publishes a list of EAR Country Group designations. Country Groups E1 and E2 identify embargoed countries subject to comprehensive restrictions and are included in this document of country groups. (Please note that Country Group E begins on page 8 of this document.)","title":"Sanctioned countries (not eligible to use Jetstream2)"},{"location":"general/featured/","text":"Featured Images \u00b6 Jetstream2 will have a limited set of featured images. These are images that the Jestream2 team curates and maintains. A key difference between Jetstream1 and Jetstream2 is that there will not be application specific featured images. We will maintain a software collection that will be made available on every virtual machine at boot. Software will be loaded via Lmod Modules . At this time, the featured images will be: Ubuntu 20.04 Ubuntu 18.04 Rocky 8 CentOS 7 All featured images are named Featured-yyyyyyyy (e.g. Featured-Ubuntu20) on Jetstream2. These featured images will evolve over time. As distributions leave support (e.g Ubuntu 18 will end support in April 2023), we will replace them with newer, supported versions. NVIDIA drivers will be present on all featured images so any of the featured images will work with Jetstream2 GPUs. Our goal is to maintain a minimum of featured images but keep them updated via automated pipeline on a weekly basis.","title":"Featured Images"},{"location":"general/featured/#featured-images","text":"Jetstream2 will have a limited set of featured images. These are images that the Jestream2 team curates and maintains. A key difference between Jetstream1 and Jetstream2 is that there will not be application specific featured images. We will maintain a software collection that will be made available on every virtual machine at boot. Software will be loaded via Lmod Modules . At this time, the featured images will be: Ubuntu 20.04 Ubuntu 18.04 Rocky 8 CentOS 7 All featured images are named Featured-yyyyyyyy (e.g. Featured-Ubuntu20) on Jetstream2. These featured images will evolve over time. As distributions leave support (e.g Ubuntu 18 will end support in April 2023), we will replace them with newer, supported versions. NVIDIA drivers will be present on all featured images so any of the featured images will work with Jetstream2 GPUs. Our goal is to maintain a minimum of featured images but keep them updated via automated pipeline on a weekly basis.","title":"Featured Images"},{"location":"general/federating/","text":"Federating a gateway or JupyterHub on Jetstream2 \u00b6 One of Jetstream2\u2019s key features is to enable the federating of gateways and JupyterHub services. To accomplish this, we have partnered with the Cyberinfrastructure Integration Research Center to provide their Custos software to researchers and educators on Jetstream2. Custos open source software is operated as a service by the IU PTI Cyberinfrastructure Integration Research Center . Custos and its constituent components (CILogon, Keycloak, and HashiCorp Vault) are all multi-tenanted services, where a tenant is a science gateway, JupyterHub, or similar service. To request Custos integration with your service, please follow these steps: Tenant Request Phase: A gateway or JupyterHub administrator requests a Custos tenant - https://portal.usecustos.org/ IU Custos administrators review the request and interact with the requesting gateway admins. If the tenant request is approved, a tenant is automatically created within each of the dependent CILogon, Keycloak, Vault, user profile, and group services that Custos manages. Gateway admin can now login to Custos Admin Portal and obtain their OAuth2 credentials. Gateway Integration Phase: Gateway developers call Custos services as appropriate using the Custos API methods. Custos provides REST interfaces and Software Development Kits (SDKs). This step is already done for JupyterHub, Galaxy, and Airavata Django Portal-based gateways. PyPI packages are available from https://pypi.org/project/custos-jupyterhub-authenticator/ Python SDK documentation is available from https://cwiki.apache.org/confluence/display/CUSTOS/Use+Custos+Python+SDK REST API documentation is available from https://cwiki.apache.org/confluence/display/CUSTOS/Use+Custos+REST+Endpoints Gateway configures the Custos tenant credentials in its configuration file. See https://cwiki.apache.org/confluence/display/CUSTOS/Custos+Configuration for additional details. Gateway administrators can optionally make additional configuration choices, such as filtering users based on Identity Providers (institutions) to further limit access. The Gateway or Hub admin makes these changes in the tenant profile within the Custos Admin Portal. These configuration choices are described in the Custos Wiki; see https://cwiki.apache.org/confluence/display/CUSTOS/Custos+User+Filtering . After Step 3, the gateway or JupyterHub is ready to be used.","title":"Federating Gateways on Jetstream2"},{"location":"general/federating/#federating-a-gateway-or-jupyterhub-on-jetstream2","text":"One of Jetstream2\u2019s key features is to enable the federating of gateways and JupyterHub services. To accomplish this, we have partnered with the Cyberinfrastructure Integration Research Center to provide their Custos software to researchers and educators on Jetstream2. Custos open source software is operated as a service by the IU PTI Cyberinfrastructure Integration Research Center . Custos and its constituent components (CILogon, Keycloak, and HashiCorp Vault) are all multi-tenanted services, where a tenant is a science gateway, JupyterHub, or similar service. To request Custos integration with your service, please follow these steps: Tenant Request Phase: A gateway or JupyterHub administrator requests a Custos tenant - https://portal.usecustos.org/ IU Custos administrators review the request and interact with the requesting gateway admins. If the tenant request is approved, a tenant is automatically created within each of the dependent CILogon, Keycloak, Vault, user profile, and group services that Custos manages. Gateway admin can now login to Custos Admin Portal and obtain their OAuth2 credentials. Gateway Integration Phase: Gateway developers call Custos services as appropriate using the Custos API methods. Custos provides REST interfaces and Software Development Kits (SDKs). This step is already done for JupyterHub, Galaxy, and Airavata Django Portal-based gateways. PyPI packages are available from https://pypi.org/project/custos-jupyterhub-authenticator/ Python SDK documentation is available from https://cwiki.apache.org/confluence/display/CUSTOS/Use+Custos+Python+SDK REST API documentation is available from https://cwiki.apache.org/confluence/display/CUSTOS/Use+Custos+REST+Endpoints Gateway configures the Custos tenant credentials in its configuration file. See https://cwiki.apache.org/confluence/display/CUSTOS/Custos+Configuration for additional details. Gateway administrators can optionally make additional configuration choices, such as filtering users based on Identity Providers (institutions) to further limit access. The Gateway or Hub admin makes these changes in the tenant profile within the Custos Admin Portal. These configuration choices are described in the Custos Wiki; see https://cwiki.apache.org/confluence/display/CUSTOS/Custos+User+Filtering . After Step 3, the gateway or JupyterHub is ready to be used.","title":"Federating a gateway or JupyterHub on Jetstream2"},{"location":"general/filetransfer/","text":"Transferring Files \u00b6 There are a number of ways to transfer files to and from your VM instances. Some common ways are listed below. Web shell/Web Desktop The web shell and web desktop allow file transfers to be easily done with a built in facility. Exosphere : Transferring files with web shell and web desktop Cacao : Coming soon! SCP/SFTP Here are some recommended clients for transferring files with SCP or SFTP. These clients, while suggested, are not endorsed by Jetstream in any way. We have limited means to help with support issues related to these clients. Cyberduck is a well-known client for both Windows and Mac OSX to move files to and from individual VMs. WinSCP is another well-known Windows specific option. Filezilla is an option supporting several operating systems, including Windows. Globus Globus is a fast, reliable, and secure file transfer service for easily moving data to, from, and between digital resources. Information for transferring files with Globus can be found here.","title":"File Transfer"},{"location":"general/filetransfer/#transferring-files","text":"There are a number of ways to transfer files to and from your VM instances. Some common ways are listed below. Web shell/Web Desktop The web shell and web desktop allow file transfers to be easily done with a built in facility. Exosphere : Transferring files with web shell and web desktop Cacao : Coming soon! SCP/SFTP Here are some recommended clients for transferring files with SCP or SFTP. These clients, while suggested, are not endorsed by Jetstream in any way. We have limited means to help with support issues related to these clients. Cyberduck is a well-known client for both Windows and Mac OSX to move files to and from individual VMs. WinSCP is another well-known Windows specific option. Filezilla is an option supporting several operating systems, including Windows. Globus Globus is a fast, reliable, and secure file transfer service for easily moving data to, from, and between digital resources. Information for transferring files with Globus can be found here.","title":"Transferring Files"},{"location":"general/firewalls/","text":"Firewalls \u00b6 Jetstream2 staff encourages a defense-in-depth approach to security. This potentially involves several methods of restricting access and securing instances. Firewalls are not enabled by default on Jetstream2 instances. Depending on the user interface you launched your instance from, you may have different security groups established for your instance. (See What is the default security profile for Jetstream2 VMs? for more information on that.) We encourage keeping your instances patched, rebooting as needed for any kernel or glibc patches, limiting access to all services as much as possible, utilizing security groups if your interface allows it, and running your own host-based firewall if you\u2019re comfortable administering it. If you are comfortable administering a firewall, we would encourage you to read the following tutorials for their respect Linux variants. Ubuntu\u2019s UFW (Uncomplicated FireWall) is very simple to use, though making sure you leave SSH access open is crucial (and often missed by first time UFW users) so you do not lock yourself out of your virtual machine. Ubuntu 20 and 18 \u00b6 How to Set Up a Firewall with UFW on Ubuntu 20.04 is a good initial tutorial for setting up UFW. Rocky 8 / Alma 8 / CentOS 7 \u00b6 The Redhat variants are a little less user friendly. How to Open or close ports in AlmaLinux 8 or Rocky Firewall is a good reference for getting started with firewalld. How to Set Up a Firewall with FirewallD on CentOS 7 will get you started with firewalld on CentOS 7","title":"Firewalls"},{"location":"general/firewalls/#firewalls","text":"Jetstream2 staff encourages a defense-in-depth approach to security. This potentially involves several methods of restricting access and securing instances. Firewalls are not enabled by default on Jetstream2 instances. Depending on the user interface you launched your instance from, you may have different security groups established for your instance. (See What is the default security profile for Jetstream2 VMs? for more information on that.) We encourage keeping your instances patched, rebooting as needed for any kernel or glibc patches, limiting access to all services as much as possible, utilizing security groups if your interface allows it, and running your own host-based firewall if you\u2019re comfortable administering it. If you are comfortable administering a firewall, we would encourage you to read the following tutorials for their respect Linux variants. Ubuntu\u2019s UFW (Uncomplicated FireWall) is very simple to use, though making sure you leave SSH access open is crucial (and often missed by first time UFW users) so you do not lock yourself out of your virtual machine.","title":"Firewalls"},{"location":"general/firewalls/#ubuntu-20-and-18","text":"How to Set Up a Firewall with UFW on Ubuntu 20.04 is a good initial tutorial for setting up UFW.","title":"Ubuntu 20 and 18"},{"location":"general/firewalls/#rocky-8-alma-8-centos-7","text":"The Redhat variants are a little less user friendly. How to Open or close ports in AlmaLinux 8 or Rocky Firewall is a good reference for getting started with firewalld. How to Set Up a Firewall with FirewallD on CentOS 7 will get you started with firewalld on CentOS 7","title":"Rocky 8 / Alma 8 / CentOS 7"},{"location":"general/galaxy/","text":"Galaxy \u00b6 Galaxy is a data analysis platform for genomic data analysis, although support other domains is growing: machine learning, natural language processing, climate science. Galaxy enables scientists to share, analyze and visualize their own data via a browser. Thounsands of modern and popoular tools are readily available alongside pre-formatted reference data. There are also extensive training resources for a variety of roles, including researchers, tool developers, and educators. Galaxy on Jetstream \u00b6 The primary way Galaxy leverages Jetstream today is totally invisible to the user. You may already be using Jetstream without even knowing it! The Galaxy public server available at https://usegalaxy.org/ is making opportunistic use of Jetstream resources for suitable jobs. When you submit a job from usegalaxy.org , it may be automatically routed to a set of machines running on Jetstream. The decision is based on the selected tool and input properties, and as a user, there is currently no way to request a job execute on Jetstream. It is worth noting that the current implementation does not require you as a user to have a Jetstream allocation but it also does not support custom tools or usage quotas. Creating your own Galaxy Instance \u00b6 But what if I have an allocation and want to run my own Galaxy server or install custom tools? Galaxy is an open source application so it can be installed on a virtual machine running on Jetstream. If you choose this path, we strongly recommend that the installation and server management is done by a system administrator familiar with cloud computing technology, and ideally Galaxy. Running a Galaxy server can quickly become a time-consuming job that requires substantial understanding of the underlying technology. To get started with installing, visit https://getgalaxy.org/ and follow the documentation. For any installation other than a temporary development instance, we recommend creating a Production Environment for Galaxy.","title":"Galaxy"},{"location":"general/galaxy/#galaxy","text":"Galaxy is a data analysis platform for genomic data analysis, although support other domains is growing: machine learning, natural language processing, climate science. Galaxy enables scientists to share, analyze and visualize their own data via a browser. Thounsands of modern and popoular tools are readily available alongside pre-formatted reference data. There are also extensive training resources for a variety of roles, including researchers, tool developers, and educators.","title":"Galaxy"},{"location":"general/galaxy/#galaxy-on-jetstream","text":"The primary way Galaxy leverages Jetstream today is totally invisible to the user. You may already be using Jetstream without even knowing it! The Galaxy public server available at https://usegalaxy.org/ is making opportunistic use of Jetstream resources for suitable jobs. When you submit a job from usegalaxy.org , it may be automatically routed to a set of machines running on Jetstream. The decision is based on the selected tool and input properties, and as a user, there is currently no way to request a job execute on Jetstream. It is worth noting that the current implementation does not require you as a user to have a Jetstream allocation but it also does not support custom tools or usage quotas.","title":"Galaxy on Jetstream"},{"location":"general/galaxy/#creating-your-own-galaxy-instance","text":"But what if I have an allocation and want to run my own Galaxy server or install custom tools? Galaxy is an open source application so it can be installed on a virtual machine running on Jetstream. If you choose this path, we strongly recommend that the installation and server management is done by a system administrator familiar with cloud computing technology, and ideally Galaxy. Running a Galaxy server can quickly become a time-consuming job that requires substantial understanding of the underlying technology. To get started with installing, visit https://getgalaxy.org/ and follow the documentation. For any installation other than a temporary development instance, we recommend creating a Production Environment for Galaxy.","title":"Creating your own Galaxy Instance"},{"location":"general/gateways/","text":"Acceptable Usage Policies for Jetstream2-hosted Gateways \u00b6 Gateways hosted on Jetstream2 agree to abide by the usage policies defined on Policies: Acceptable Use and Research and Export Control Guidance . We also recommend having an Acceptable Usage Policy (AUP) on your gateway. We recommend adding a statement to any acceptable use agreement that targets export control issues, such as the language below: Ensure that data that must be protected by Federal security or privacy laws (e.g., HIPAA, FERPA, ITAR, classified information, export control, etc.) are not stored on this system unless such storage and usage is specifically authorized by the responsible University administrator and complies with any processes for management of access to such information. For export controlled information, including ITAR information, approval of the University Export Compliance Office is required prior to use of the [name of system] systems for storage/processing of export controlled data. The [name of system] system is not intended, by default, to meet the security requirements of these laws or regulations and specific usage related controls or restrictions may be required prior to authorization of the use of the [name of system] system for such purposes. Ensure that the project does not violate any export control end use restrictions contained in Part 744 of the EAR. The TrustedCI project \u2013 https://www.trustedci.org/ - maintains a sample AUP that you may use to model your gateway\u2019s policy on.","title":"AUPs for Jetstream2 Hosted Gateways"},{"location":"general/gateways/#acceptable-usage-policies-for-jetstream2-hosted-gateways","text":"Gateways hosted on Jetstream2 agree to abide by the usage policies defined on Policies: Acceptable Use and Research and Export Control Guidance . We also recommend having an Acceptable Usage Policy (AUP) on your gateway. We recommend adding a statement to any acceptable use agreement that targets export control issues, such as the language below: Ensure that data that must be protected by Federal security or privacy laws (e.g., HIPAA, FERPA, ITAR, classified information, export control, etc.) are not stored on this system unless such storage and usage is specifically authorized by the responsible University administrator and complies with any processes for management of access to such information. For export controlled information, including ITAR information, approval of the University Export Compliance Office is required prior to use of the [name of system] systems for storage/processing of export controlled data. The [name of system] system is not intended, by default, to meet the security requirements of these laws or regulations and specific usage related controls or restrictions may be required prior to authorization of the use of the [name of system] system for such purposes. Ensure that the project does not violate any export control end use restrictions contained in Part 744 of the EAR. The TrustedCI project \u2013 https://www.trustedci.org/ - maintains a sample AUP that you may use to model your gateway\u2019s policy on.","title":"Acceptable Usage Policies for Jetstream2-hosted Gateways"},{"location":"general/installsoftware/","text":"Installing Software on your VM \u00b6 On Jetstream2 virtual machines, you have the ability to install whatever software you need. There are a number of different ways to do this. We will address some of the most common ones. For software available from the distribution (e.g. Ubuntu, Rocky, Alma, CentOS) you can either search via the web or from the command line. Below are some examples. Please keep in mind that there are other methods not covered here like Anaconda/Conda/MiniConda. Covering all possibilities can be difficult. To install software packages from the Linux distribution or to install Python packages systemwide, you\u2019ll need to have priveleged access. Please refer to Being root on your virtual machines if you are not familiar with sudo or su. Using Ubuntu \u00b6 Check to see if a packages is installed: apt list some_package_name Search for a package if it\u2019s not installed: apt search some_package_name Install the package: apt install some_package_name Using Rocky8 or AlmaLinux \u00b6 Check to see if a packages is installed: dnf list packagename Search for a package if it\u2019s not installed: dnf search some_package_name Install the package: dnf install some_package_name Using CentOS 7 \u00b6 Check to see if a packages is installed: yum list packagename Search for a package if it\u2019s not installed: yum search some_package_name Install the package: yum install some_package_name Using pip or pip3 \u00b6 Depending on your python installation, your pip may be called pip3 to denote Python3. You can do pip --version to see what version of Python it\u2019s tied to. Please note that Python 2.x is deprecated and should not be used. To install something for your user only pip install module_name To install something for the entire VM sudo pip install module_name","title":"Installing Software on your VM"},{"location":"general/installsoftware/#installing-software-on-your-vm","text":"On Jetstream2 virtual machines, you have the ability to install whatever software you need. There are a number of different ways to do this. We will address some of the most common ones. For software available from the distribution (e.g. Ubuntu, Rocky, Alma, CentOS) you can either search via the web or from the command line. Below are some examples. Please keep in mind that there are other methods not covered here like Anaconda/Conda/MiniConda. Covering all possibilities can be difficult. To install software packages from the Linux distribution or to install Python packages systemwide, you\u2019ll need to have priveleged access. Please refer to Being root on your virtual machines if you are not familiar with sudo or su.","title":"Installing Software on your VM"},{"location":"general/installsoftware/#using-ubuntu","text":"Check to see if a packages is installed: apt list some_package_name Search for a package if it\u2019s not installed: apt search some_package_name Install the package: apt install some_package_name","title":"Using Ubuntu"},{"location":"general/installsoftware/#using-rocky8-or-almalinux","text":"Check to see if a packages is installed: dnf list packagename Search for a package if it\u2019s not installed: dnf search some_package_name Install the package: dnf install some_package_name","title":"Using Rocky8 or AlmaLinux"},{"location":"general/installsoftware/#using-centos-7","text":"Check to see if a packages is installed: yum list packagename Search for a package if it\u2019s not installed: yum search some_package_name Install the package: yum install some_package_name","title":"Using CentOS 7"},{"location":"general/installsoftware/#using-pip-or-pip3","text":"Depending on your python installation, your pip may be called pip3 to denote Python3. You can do pip --version to see what version of Python it\u2019s tied to. Please note that Python 2.x is deprecated and should not be used. To install something for your user only pip install module_name To install something for the entire VM sudo pip install module_name","title":"Using pip or pip3"},{"location":"general/instancemgt/","text":"Instance Management Actions \u00b6 Please orient yourself to the following instance management actions. These will help you use Jetstream2 effectively and conserve your allocation. Each user interface for Jetstream2 has instance actions in a slightly different place \u2013 see guidance for Exosphere , Cacao , Horizon , and the OpenStack CLI . Basic Actions \u00b6 Shelve and Unshelve \u00b6 When your instance is not performing work or otherwise in active use, please shelve it. Shelving an instance shuts it down and frees up resources on the cloud for other users. It also conserves the SUs (service units) on your allocation. Shelving an instance shuts down its operating system. The instance\u2019s disk contents are preserved, but any running programs will exit, so please save any unfinished work before shelving. Shelving and unshelving each take a few minutes, so shelving doesn\u2019t make sense for very short periods of inactivity. In other words, shelve your instance when you\u2019re done for the day or the week, not merely for your lunch break. A shelved instance will not accept shell, SSH, or any other connections. So, if your instance runs a server that you want to provide others the ability to connect to at any time, you must leave it active. If this describes your instance, consider re-sizing it to the smallest flavor that will work for your server needs. This will conserve SUs on your allocation. Lock and Unlock \u00b6 Locking an instance helps prevent anyone from accidentally deleting or performing other actions on it, from Exosphere and all other Jetstream2 interfaces. If your instance is running an important job or used in \u2018production\u2019 capacity, consider keeping it locked. You must unlock your instance again before performing other actions (such as shelving it). Locking and unlocking are non-disruptive actions \u2013 they do not affect a running instance. Be aware that locking an instance does not prevent: another user on your allocation from unlocking it. modifications to the instance\u2019s filesystem(s) or running software. For example, someone with access to the instance could still log in and delete files. Locking only prevents instance actions at the cloud (OpenStack) level. the instance from shutting off when your allocation expires or is exhausted. Reboot \u00b6 Rebooting an instance is just like restarting a computer. The cloud will first attempt a graceful or \u201csoft\u201d reboot, where all of your programs are allowed to exit. If that fails then OpenStack will perform a \u201chard\u201d reboot, which will lose any work that is not yet written to disk. If you cannot connect to your instance, rebooting is a good troubleshooting step before creating a support ticket. Advanced Actions \u00b6 The following actions are for more sophisticated use cases. If you\u2019re a new cloud user, it\u2019s okay to skip reading about these for now. Resize \u00b6 Resizing allows you to choose a different flavor for your instance. When you resize, your instance will shut down and then restart with the new flavor (so please save any work in progress first). You can now resize using Exosphere, Horizon, or the CLI. Consider resizing if you find yourself in one of these situations: Your instance exhausts its compute resources, e.g. you run out of working memory (RAM) or you want it to process work faster. Your instance\u2019s CPU is sitting idle most of the time, in which case a smaller flavor would burn your allocation more slowly. Exosphere\u2019s instance resource usage graphs are a useful guide here. You launched a GPU flavor, then later find that you no longer need the GPU, but want to keep using the instance. You launched a non-GPU flavor, then later find that you want to use the same instance with a GPU. If your software stack sometimes needs a large flavor to run a compute-intensive job, but you can develop and tune it on a smaller flavor, consider resizing down to a small flavor for development work, and back up when you\u2019re ready to run it at a larger scale. This get you best performance when you need it, while conserving your allocation when you don\u2019t. Moving to a larger flavor is generally not appropriate in these situations: The speed of your workload is limited by a process that is single-threaded (not parallelized). If this process cannot be parallelized then resizing is unlikely to speed it up. The speed of your workload is limited by disk or network transfer speed. Larger instances do not have faster storage or network connectivity. Your instance is running out of storage. Instead, create a volume, attach it, and move your data to the volume. If you\u2019re installing a lot of software that is not easily moved to a volume, resizing may be appropriate \u2013 open a ticket and ask for advice. When resizing, you must select your desired new flavor. After the resize is complete, the instance will be in status \u201cResize verify\u201d. At that time, access the instance (e.g. using Web Shell or SSH) and confirm that it is working, then choose the \u201cConfirm resize\u201d action. If the resize process broke something and you need to return to the previous flavor, choose the \u201cRevert resize\u201d action. Image \u00b6 When you create an image, you capture the entire state of your instance\u2019s root disk. New instances can be launched from that image, which means that you can use an image to \u2018snapshot\u2019 and \u2018clone\u2019 an instance. After you specify the image name, it will take several minutes for the image to finish creating and become active. Consider creating an image in the following situations: You want to create a new instance that is a clone of your existing instance. In this case, create an image of the existing instance and launch your clone(s) from that image. You are about to perform a possibly destructive action on your instance (like installing, upgrading, or removing software), and you need the ability to go back and get to the prior disk state of the instance if something goes wrong. You are building a software stack that other people will consume via their own instance (e.g. you are teaching a class). Providing an image can be an easy way for other people to get a new instance just like yours. Be aware that system images quickly fall behind on operating system updates. As more time passes since an image was created, the more software will need to be updated when a new instance is created for it. This can lead to excessively long instance lanuch times and other problems. For this reason, custom images are not the right tool for sharing software or workflows more than a few months into the future. If this describes your situation, please open a support ticket and ask for advice. Suspend and Resume \u00b6 Suspending an instance is like placing a computer on standby (a.k.a. sleep). When you resume the instance, all running programs will be in the state they were in prior to entering standby. (Still, it is wise to save any work in progress before suspending.) Please consider suspending instead of shelving only if you are running software that was complex or labor-intensive to start (not install), and you only need to leave it suspended for a relatively short time (e.g. a few days). Suspended instances still occupy resources on the cloud, and they continue to consume your allocation at a reduced rate.","title":"Instance Management Actions"},{"location":"general/instancemgt/#instance-management-actions","text":"Please orient yourself to the following instance management actions. These will help you use Jetstream2 effectively and conserve your allocation. Each user interface for Jetstream2 has instance actions in a slightly different place \u2013 see guidance for Exosphere , Cacao , Horizon , and the OpenStack CLI .","title":"Instance Management Actions"},{"location":"general/instancemgt/#basic-actions","text":"","title":"Basic Actions"},{"location":"general/instancemgt/#shelve-and-unshelve","text":"When your instance is not performing work or otherwise in active use, please shelve it. Shelving an instance shuts it down and frees up resources on the cloud for other users. It also conserves the SUs (service units) on your allocation. Shelving an instance shuts down its operating system. The instance\u2019s disk contents are preserved, but any running programs will exit, so please save any unfinished work before shelving. Shelving and unshelving each take a few minutes, so shelving doesn\u2019t make sense for very short periods of inactivity. In other words, shelve your instance when you\u2019re done for the day or the week, not merely for your lunch break. A shelved instance will not accept shell, SSH, or any other connections. So, if your instance runs a server that you want to provide others the ability to connect to at any time, you must leave it active. If this describes your instance, consider re-sizing it to the smallest flavor that will work for your server needs. This will conserve SUs on your allocation.","title":"Shelve and Unshelve"},{"location":"general/instancemgt/#lock-and-unlock","text":"Locking an instance helps prevent anyone from accidentally deleting or performing other actions on it, from Exosphere and all other Jetstream2 interfaces. If your instance is running an important job or used in \u2018production\u2019 capacity, consider keeping it locked. You must unlock your instance again before performing other actions (such as shelving it). Locking and unlocking are non-disruptive actions \u2013 they do not affect a running instance. Be aware that locking an instance does not prevent: another user on your allocation from unlocking it. modifications to the instance\u2019s filesystem(s) or running software. For example, someone with access to the instance could still log in and delete files. Locking only prevents instance actions at the cloud (OpenStack) level. the instance from shutting off when your allocation expires or is exhausted.","title":"Lock and Unlock"},{"location":"general/instancemgt/#reboot","text":"Rebooting an instance is just like restarting a computer. The cloud will first attempt a graceful or \u201csoft\u201d reboot, where all of your programs are allowed to exit. If that fails then OpenStack will perform a \u201chard\u201d reboot, which will lose any work that is not yet written to disk. If you cannot connect to your instance, rebooting is a good troubleshooting step before creating a support ticket.","title":"Reboot"},{"location":"general/instancemgt/#advanced-actions","text":"The following actions are for more sophisticated use cases. If you\u2019re a new cloud user, it\u2019s okay to skip reading about these for now.","title":"Advanced Actions"},{"location":"general/instancemgt/#resize","text":"Resizing allows you to choose a different flavor for your instance. When you resize, your instance will shut down and then restart with the new flavor (so please save any work in progress first). You can now resize using Exosphere, Horizon, or the CLI. Consider resizing if you find yourself in one of these situations: Your instance exhausts its compute resources, e.g. you run out of working memory (RAM) or you want it to process work faster. Your instance\u2019s CPU is sitting idle most of the time, in which case a smaller flavor would burn your allocation more slowly. Exosphere\u2019s instance resource usage graphs are a useful guide here. You launched a GPU flavor, then later find that you no longer need the GPU, but want to keep using the instance. You launched a non-GPU flavor, then later find that you want to use the same instance with a GPU. If your software stack sometimes needs a large flavor to run a compute-intensive job, but you can develop and tune it on a smaller flavor, consider resizing down to a small flavor for development work, and back up when you\u2019re ready to run it at a larger scale. This get you best performance when you need it, while conserving your allocation when you don\u2019t. Moving to a larger flavor is generally not appropriate in these situations: The speed of your workload is limited by a process that is single-threaded (not parallelized). If this process cannot be parallelized then resizing is unlikely to speed it up. The speed of your workload is limited by disk or network transfer speed. Larger instances do not have faster storage or network connectivity. Your instance is running out of storage. Instead, create a volume, attach it, and move your data to the volume. If you\u2019re installing a lot of software that is not easily moved to a volume, resizing may be appropriate \u2013 open a ticket and ask for advice. When resizing, you must select your desired new flavor. After the resize is complete, the instance will be in status \u201cResize verify\u201d. At that time, access the instance (e.g. using Web Shell or SSH) and confirm that it is working, then choose the \u201cConfirm resize\u201d action. If the resize process broke something and you need to return to the previous flavor, choose the \u201cRevert resize\u201d action.","title":"Resize"},{"location":"general/instancemgt/#image","text":"When you create an image, you capture the entire state of your instance\u2019s root disk. New instances can be launched from that image, which means that you can use an image to \u2018snapshot\u2019 and \u2018clone\u2019 an instance. After you specify the image name, it will take several minutes for the image to finish creating and become active. Consider creating an image in the following situations: You want to create a new instance that is a clone of your existing instance. In this case, create an image of the existing instance and launch your clone(s) from that image. You are about to perform a possibly destructive action on your instance (like installing, upgrading, or removing software), and you need the ability to go back and get to the prior disk state of the instance if something goes wrong. You are building a software stack that other people will consume via their own instance (e.g. you are teaching a class). Providing an image can be an easy way for other people to get a new instance just like yours. Be aware that system images quickly fall behind on operating system updates. As more time passes since an image was created, the more software will need to be updated when a new instance is created for it. This can lead to excessively long instance lanuch times and other problems. For this reason, custom images are not the right tool for sharing software or workflows more than a few months into the future. If this describes your situation, please open a support ticket and ask for advice.","title":"Image"},{"location":"general/instancemgt/#suspend-and-resume","text":"Suspending an instance is like placing a computer on standby (a.k.a. sleep). When you resume the instance, all running programs will be in the state they were in prior to entering standby. (Still, it is wise to save any work in progress before suspending.) Please consider suspending instead of shelving only if you are running software that was complex or labor-intensive to start (not install), and you only need to leave it suspended for a relatively short time (e.g. a few days). Suspended instances still occupy resources on the cloud, and they continue to consume your allocation at a reduced rate.","title":"Suspend and Resume"},{"location":"general/jupyter/","text":"Jupyter on Jetstream2 \u00b6 Jupyter in the web desktop \u00b6 The easiest way to use a Jupyter notebook on Jetstream2 is to launch an instance using Exosphere , making sure to launch with the web desktop enabled . Then you can open a web desktop session in Exosphere, open a terminal, and type the commands module load anaconda jupyter notebook This should load the Anaconda package and then initialize a new jupyter notebook and open the default browser with the notebook. Jupyter from the web shell or ssh session \u00b6 We have made a front end script for jupyter that fetches your virtual machine\u2019s IP address and substitutes it into the URL that Jupyter generates for you. Since the VM doesn\u2019t really ever know it\u2019s public IP address, it cannot be set directly in Jupyter. To start Jupyter from the web shell or ssh session for remote access, type the commands module load Anaconda jupyter-ip.sh This should yield some output that ends with something similar to this To access the notebook, open this file in a browser: file:///home/exouser/.local/share/jupyter/runtime/nbserver-100997-open.html Or copy and paste one of these URLs: http://neatly-trusting-chow-gui:8888/?token=723fa5a01f6dc27b0ec655846572513757e921aaf247cbb7 or http://149.165.154.8:8888/?token=723fa5a01f6dc27b0ec655846572513757e921aaf247cbb7 where the last line with the 149.165.xxx.xxx IP address will be the one you want to cut and paste into your browser.","title":"Jupyter on Jetstream2"},{"location":"general/jupyter/#jupyter-on-jetstream2","text":"","title":"Jupyter on Jetstream2"},{"location":"general/jupyter/#jupyter-in-the-web-desktop","text":"The easiest way to use a Jupyter notebook on Jetstream2 is to launch an instance using Exosphere , making sure to launch with the web desktop enabled . Then you can open a web desktop session in Exosphere, open a terminal, and type the commands module load anaconda jupyter notebook This should load the Anaconda package and then initialize a new jupyter notebook and open the default browser with the notebook.","title":"Jupyter in the web desktop"},{"location":"general/jupyter/#jupyter-from-the-web-shell-or-ssh-session","text":"We have made a front end script for jupyter that fetches your virtual machine\u2019s IP address and substitutes it into the URL that Jupyter generates for you. Since the VM doesn\u2019t really ever know it\u2019s public IP address, it cannot be set directly in Jupyter. To start Jupyter from the web shell or ssh session for remote access, type the commands module load Anaconda jupyter-ip.sh This should yield some output that ends with something similar to this To access the notebook, open this file in a browser: file:///home/exouser/.local/share/jupyter/runtime/nbserver-100997-open.html Or copy and paste one of these URLs: http://neatly-trusting-chow-gui:8888/?token=723fa5a01f6dc27b0ec655846572513757e921aaf247cbb7 or http://149.165.154.8:8888/?token=723fa5a01f6dc27b0ec655846572513757e921aaf247cbb7 where the last line with the 149.165.xxx.xxx IP address will be the one you want to cut and paste into your browser.","title":"Jupyter from the web shell or ssh session"},{"location":"general/k8scluster/","text":"Building a Kubernetes Cluster \u00b6 A k8s cluster is a collection of nodes running containers. Each node is responsible for running containers that are on that specific node. In this example we use the kubeadm tool to set up a cluster. Our cluster has 3 VMs \u2013 1 control plane and 2 worker nodes. Installation \u00b6 Create three virtual machines and run the installation commands (1-16) on all three. Create configuration file for containerd: cat <<EOF | sudo tee /etc/modules-load.d/containerd.config overlay br_netfilter EOF Load modules: sudo modprobe overlay sudo modprobe br_netfilter Set networking configurations for K8s: cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 EOF Apply new settings: sudo sysctl --system Install containerd: sudo apt-get update && sudo apt-get install -y containerd Create default configuration file for containerd: sudo mkdir -p /etc/containerd Generate default containerd configuration and save to the newly created default file: sudo containerd config default | sudo tee /etc/containerd/config.toml Restart containerd to ensure new configuration file usage: sudo systemctl restart containerd Verify that containerd is running. sudo systemctl status containerd Disable swap: sudo swapoff -a Install dependency packages: sudo apt-get update && sudo apt-get install -y apt-transport-https curl Download and add GPG key: curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - Add Kubernetes to repository list: cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF Update package listings: sudo apt-get update Install Kubernetes packages sudo apt-get install -y kubelet=1.22.0-00 kubeadm=1.22.0-00 kubectl=1.22.0-00 Turn off automatic updates: sudo apt-mark hold kubelet kubeadm kubectl Initialize the cluster \u00b6 Before initializing the cluster make sure the Cgroup driver is systemd on all three VMs. You can check that by running the following command: docker info | grep Cgroup To change the Cgroup drive to systemd you can do the following: cat <<EOF | sudo tee /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\" } EOF You will need to restart Docker and reset kubeadm for the change in step 17. to take effect. sudo systemctl daemon-reload sudo systemctl restart docker sudo kubeadm reset Initialize the Kubernetes cluster on the control plane node using kubeadm: sudo kubeadm init --pod-network-cidr 192.168.0.0/16 --kubernetes-version 1.22.0 Set kubectl access on the control plane : mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config On the Control Plane Node, install Calico Networking: kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml Test access to cluster: kubectl get nodes Join worker nodes to the cluster \u00b6 On the Control Plane Node , create the token and copy the kubeadm join command. ( Note: The join command can also be found in the output from kubeadm init command): kubeadm token create --print-join-command On the Worker Nodes , paste the kubeadm join command to join the cluster. Use sudo to run it as root: sudo kubeadm join ... On the Control Plane Node , view cluster status. ( Note: You may have to wait a few moments to allow all nodes to become ready): kubectl get nodes Clean up \u00b6 On the Control Plabe Node , delete ia resource using kubectl: kubectl delete node <node name> Note: Before using the cluster confirm that all pods in the kube-system are running: kubectl get pods -n kube-system -o wide This command will give you a list of pods in the kube-system namespace, the status of the pod, the node it is running on. If the status of a pod is not Running , you can check the logs for error messages: kubectl logs -n kube-system {name_of_your_pod}","title":"Build a K8s cluster"},{"location":"general/k8scluster/#building-a-kubernetes-cluster","text":"A k8s cluster is a collection of nodes running containers. Each node is responsible for running containers that are on that specific node. In this example we use the kubeadm tool to set up a cluster. Our cluster has 3 VMs \u2013 1 control plane and 2 worker nodes.","title":"Building a Kubernetes Cluster"},{"location":"general/k8scluster/#installation","text":"Create three virtual machines and run the installation commands (1-16) on all three. Create configuration file for containerd: cat <<EOF | sudo tee /etc/modules-load.d/containerd.config overlay br_netfilter EOF Load modules: sudo modprobe overlay sudo modprobe br_netfilter Set networking configurations for K8s: cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 EOF Apply new settings: sudo sysctl --system Install containerd: sudo apt-get update && sudo apt-get install -y containerd Create default configuration file for containerd: sudo mkdir -p /etc/containerd Generate default containerd configuration and save to the newly created default file: sudo containerd config default | sudo tee /etc/containerd/config.toml Restart containerd to ensure new configuration file usage: sudo systemctl restart containerd Verify that containerd is running. sudo systemctl status containerd Disable swap: sudo swapoff -a Install dependency packages: sudo apt-get update && sudo apt-get install -y apt-transport-https curl Download and add GPG key: curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - Add Kubernetes to repository list: cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF Update package listings: sudo apt-get update Install Kubernetes packages sudo apt-get install -y kubelet=1.22.0-00 kubeadm=1.22.0-00 kubectl=1.22.0-00 Turn off automatic updates: sudo apt-mark hold kubelet kubeadm kubectl","title":"Installation"},{"location":"general/k8scluster/#initialize-the-cluster","text":"Before initializing the cluster make sure the Cgroup driver is systemd on all three VMs. You can check that by running the following command: docker info | grep Cgroup To change the Cgroup drive to systemd you can do the following: cat <<EOF | sudo tee /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\" } EOF You will need to restart Docker and reset kubeadm for the change in step 17. to take effect. sudo systemctl daemon-reload sudo systemctl restart docker sudo kubeadm reset Initialize the Kubernetes cluster on the control plane node using kubeadm: sudo kubeadm init --pod-network-cidr 192.168.0.0/16 --kubernetes-version 1.22.0 Set kubectl access on the control plane : mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config On the Control Plane Node, install Calico Networking: kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml Test access to cluster: kubectl get nodes","title":"Initialize the cluster"},{"location":"general/k8scluster/#join-worker-nodes-to-the-cluster","text":"On the Control Plane Node , create the token and copy the kubeadm join command. ( Note: The join command can also be found in the output from kubeadm init command): kubeadm token create --print-join-command On the Worker Nodes , paste the kubeadm join command to join the cluster. Use sudo to run it as root: sudo kubeadm join ... On the Control Plane Node , view cluster status. ( Note: You may have to wait a few moments to allow all nodes to become ready): kubectl get nodes","title":"Join worker nodes to the cluster"},{"location":"general/k8scluster/#clean-up","text":"On the Control Plabe Node , delete ia resource using kubectl: kubectl delete node <node name> Note: Before using the cluster confirm that all pods in the kube-system are running: kubectl get pods -n kube-system -o wide This command will give you a list of pods in the kube-system namespace, the status of the pod, the node it is running on. If the status of a pod is not Running , you can check the logs for error messages: kubectl logs -n kube-system {name_of_your_pod}","title":"Clean up"},{"location":"general/k8smanage/","text":"Managing your application with Kubernetes \u00b6 To manage applications with Kubernetes we use the apply command. This command requires a file or directory of files. When run, the apply command makes the state of the Kubernetes cluster match the state defined in the file/s. Using the Kubernetes CLI, (Kubectl), we can create objects such as Pods, Deployments. etc. by providing a yaml file for that object. Pods \u00b6 A Pod represents a single instance of an app running in the cluster. Here\u2019s an example of a yaml file that defines a pod named, simple-pod.yaml The kind field describes the type of object you want to create. The pod spec must contain at least one container. Image specifies which image will be run in the pod. Finally, we list the port to expose from the container. yaml manifest: apiVersion: v1 kind: Pod metadata: name: nginx-deployment label: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 To create the pod defined in the yaml file above, run the following command. kubectl apply -f simple-pod.yaml command line: kubectl run nginx-deployment --image=nginx --port=80 ReplicaSet \u00b6 ReplicaSet adds or deletes pods as needed. Creating replicas of a pod scales an application horizontally. Replicas are usually created as part of a deployment. Here\u2019s a sample yaml file for creating 2 replicas, to create a ReplicaSet without a deployment. apiVersion: apps/v1 kind: ReplicaSet metadata: name: nginx-replicaset spec: replicas: 2 selector: matchLabels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: -containerPort: 80 command line: kubectl scale --replicas=2 deployment nginx-deployment Deployment \u00b6 Deployment is an object that can provide updates to both pods and ReplicaSets. Deployment object allows you to do rolling updates of a pod, ReplicaSet object does not. A rolling update scales up the new version to the appropriate number of replicas and scales down the old version to zero. yaml manifest: apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 3 selector: matchLabels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: -containerPort: 80 command line: kubectl create deployment nginx-deployment --image=nginx Service \u00b6 Service enables network access from either within the cluster or between external processes. kind: Service metadata: name: nginx-ingress spec: type: NodePort ports: - port: 80 targetPort: 80 protocol: TCP name: http - port: 443 targetPort: 443 protocol: TCP name: https selector: app: nginx command line: kubectl create service nodeport nginx-deployment --tcp=80:80 Autoscaling \u00b6 A Horizontal Pod Autoscaler (HPA) allows you to scale up or down depending on traffic. This can be configured by specifying the CPU or memory states. The master node periodically checks to see if the desired state is met and scales up or down as needed. One way to do this is to enable autoscaling in a yaml file: apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment namespace: default spec: maxReplicas: 10 minReplicas: 5 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: nginx-deployment targetCPUUtilizationPercentage: 10 ... ... command line: kubectl autoscale deploy nginx-deployment --min=5 --max=10 --cpu-percent=50","title":"Managing Applications with Kubernetes"},{"location":"general/k8smanage/#managing-your-application-with-kubernetes","text":"To manage applications with Kubernetes we use the apply command. This command requires a file or directory of files. When run, the apply command makes the state of the Kubernetes cluster match the state defined in the file/s. Using the Kubernetes CLI, (Kubectl), we can create objects such as Pods, Deployments. etc. by providing a yaml file for that object.","title":"Managing your application with Kubernetes"},{"location":"general/k8smanage/#pods","text":"A Pod represents a single instance of an app running in the cluster. Here\u2019s an example of a yaml file that defines a pod named, simple-pod.yaml The kind field describes the type of object you want to create. The pod spec must contain at least one container. Image specifies which image will be run in the pod. Finally, we list the port to expose from the container. yaml manifest: apiVersion: v1 kind: Pod metadata: name: nginx-deployment label: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 To create the pod defined in the yaml file above, run the following command. kubectl apply -f simple-pod.yaml command line: kubectl run nginx-deployment --image=nginx --port=80","title":"Pods"},{"location":"general/k8smanage/#replicaset","text":"ReplicaSet adds or deletes pods as needed. Creating replicas of a pod scales an application horizontally. Replicas are usually created as part of a deployment. Here\u2019s a sample yaml file for creating 2 replicas, to create a ReplicaSet without a deployment. apiVersion: apps/v1 kind: ReplicaSet metadata: name: nginx-replicaset spec: replicas: 2 selector: matchLabels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: -containerPort: 80 command line: kubectl scale --replicas=2 deployment nginx-deployment","title":"ReplicaSet"},{"location":"general/k8smanage/#deployment","text":"Deployment is an object that can provide updates to both pods and ReplicaSets. Deployment object allows you to do rolling updates of a pod, ReplicaSet object does not. A rolling update scales up the new version to the appropriate number of replicas and scales down the old version to zero. yaml manifest: apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 3 selector: matchLabels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: -containerPort: 80 command line: kubectl create deployment nginx-deployment --image=nginx","title":"Deployment"},{"location":"general/k8smanage/#service","text":"Service enables network access from either within the cluster or between external processes. kind: Service metadata: name: nginx-ingress spec: type: NodePort ports: - port: 80 targetPort: 80 protocol: TCP name: http - port: 443 targetPort: 443 protocol: TCP name: https selector: app: nginx command line: kubectl create service nodeport nginx-deployment --tcp=80:80","title":"Service"},{"location":"general/k8smanage/#autoscaling","text":"A Horizontal Pod Autoscaler (HPA) allows you to scale up or down depending on traffic. This can be configured by specifying the CPU or memory states. The master node periodically checks to see if the desired state is met and scales up or down as needed. One way to do this is to enable autoscaling in a yaml file: apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment namespace: default spec: maxReplicas: 10 minReplicas: 5 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: nginx-deployment targetCPUUtilizationPercentage: 10 ... ... command line: kubectl autoscale deploy nginx-deployment --min=5 --max=10 --cpu-percent=50","title":"Autoscaling"},{"location":"general/k8svolumes/","text":"Persistent Volumes in Kubernetes \u00b6 Container filesystems are ephemeral. One way to persist data beyond the lifetime of a pod is by using volumes. PersistentVolume is storage external to the Container but within the same Pod spec. Creating persistent storage requires the following definitions: StorageClass, PersistentVolume, PersistentVolumeClaim, and Pod that uses the PersistentVolumeClaim for storage. Here are some sample definition files. StorageClass: \u00b6 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: localdisk provisioner: kubernetes.io/no-provisioner PersistentVolume \u00b6 apiVersion: v1 kind: PersistentVolume metadata: name: alpine-pv namespace: default spec: storageClassName: localdisk capacity: storage: 1Gi accessModes: - ReadWriteOnce hostPath: path: /var/output PersistentVolumeClaim \u00b6 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: alpine-pv-claim spec: storageClassName: localdisk accessModes: - ReadWriteOnce resources: requests: storage: 100Mi Pod \u00b6 apiVersion: v1 kind: Pod metadata: name: alpine namespace: default spec: volumes: - name: alpine-volume persistentVolumeClaim: claimName: alpine-pv-claim containers: - image: alpine:3.2 command: ['sh', '-c', 'while true; do echo This is a test! > /output/trial_run.txt; sleep 20; done'] imagePullPolicy: IfNotPresent name: alpine volumeMounts: - name: alpine-volume mountPath: /output The steps to create and use volumes are below: On the Control Plane, create the StorageClass, PersistentVolume, PersistentVolumeClaim, and Pod with the kubectl create command. Check if your pod is running and which worker node it is running on: kubectl get pods -o wide The -o wide above list the node the pod is running on. Log into the worker node and verify that /var/output/trial_run.txt exists and is populated. If you delete and recreate the pod, the volume will automatically bind to the new pod with the trial_run.txt file.","title":"Persistent Volumes in Kubernetes"},{"location":"general/k8svolumes/#persistent-volumes-in-kubernetes","text":"Container filesystems are ephemeral. One way to persist data beyond the lifetime of a pod is by using volumes. PersistentVolume is storage external to the Container but within the same Pod spec. Creating persistent storage requires the following definitions: StorageClass, PersistentVolume, PersistentVolumeClaim, and Pod that uses the PersistentVolumeClaim for storage. Here are some sample definition files.","title":"Persistent Volumes in Kubernetes"},{"location":"general/k8svolumes/#storageclass","text":"apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: localdisk provisioner: kubernetes.io/no-provisioner","title":"StorageClass:"},{"location":"general/k8svolumes/#persistentvolume","text":"apiVersion: v1 kind: PersistentVolume metadata: name: alpine-pv namespace: default spec: storageClassName: localdisk capacity: storage: 1Gi accessModes: - ReadWriteOnce hostPath: path: /var/output","title":"PersistentVolume"},{"location":"general/k8svolumes/#persistentvolumeclaim","text":"apiVersion: v1 kind: PersistentVolumeClaim metadata: name: alpine-pv-claim spec: storageClassName: localdisk accessModes: - ReadWriteOnce resources: requests: storage: 100Mi","title":"PersistentVolumeClaim"},{"location":"general/k8svolumes/#pod","text":"apiVersion: v1 kind: Pod metadata: name: alpine namespace: default spec: volumes: - name: alpine-volume persistentVolumeClaim: claimName: alpine-pv-claim containers: - image: alpine:3.2 command: ['sh', '-c', 'while true; do echo This is a test! > /output/trial_run.txt; sleep 20; done'] imagePullPolicy: IfNotPresent name: alpine volumeMounts: - name: alpine-volume mountPath: /output The steps to create and use volumes are below: On the Control Plane, create the StorageClass, PersistentVolume, PersistentVolumeClaim, and Pod with the kubectl create command. Check if your pod is running and which worker node it is running on: kubectl get pods -o wide The -o wide above list the node the pod is running on. Log into the worker node and verify that /var/output/trial_run.txt exists and is populated. If you delete and recreate the pod, the volume will automatically bind to the new pod with the trial_run.txt file.","title":"Pod"},{"location":"general/kubernetes/","text":"Kubernetes \u00b6 Kubernetes (K8s) is an open-source system for automating deployment, scaling, and management of containerized applications. More information on Kubernetes here: Kubernetes Kubernetes topics \u00b6 Building a Kubernetes Cluster Managing applications with Kubernetes Persistent Volumes in Kubernetes Minikube Third Party Kubernetes topics \u00b6 Deploy Kubernetes with kubespray Feedback via Github Deploy ProjectJupyter JupyterHub Some K8s basics : A Pod is the smallest deployable unit of Kubernetes. A Pod can contain one or more containers on a single IP address (internal). Containers within a pod can communicate with each other. Pods are run on Nodes. A Node is a physical or virtual machine and it\u2019s components include kubelet, kube-proxy and Container run-time. Kubelet is the Node agent that manages containers created by Kubernetes. Kube-proxy is used by Service to run a network proxy on each Node. Container runtime is the software responsible for running containers. Ex: Docker, containerd, and CRI-O. Service allows the use of a stable network address for an application, including a persistent ip address and a local DNS entry within the cluster. Service also acts as a load-balancer across all pods allowing the addition removal of pods from a deployment. ReplicaSet is a convenient way to build multiple pods at once. Deployment provides declarative updates for Pods and ReplicaSets (rolling updates) Control Pane is a node that controls/manages worker nodes. The components of the Control Pane are the API Server, the cluster store, the controller manager, and the scheduler. API server \u2013 All communication between the various Kubernetes components must go through the API server Cluster Store or etcd \u2013 Persistently stores the cluster\u2019s configuration, state, and metadata. The Controller Manager runs controller processes that monitor and respond to controller events. Some examples of control loops include; node controller, job controller, end point controller, and replication controller. The Scheduler assigns pods to nodes.","title":"Kubernetes"},{"location":"general/kubernetes/#kubernetes","text":"Kubernetes (K8s) is an open-source system for automating deployment, scaling, and management of containerized applications. More information on Kubernetes here: Kubernetes","title":"Kubernetes"},{"location":"general/kubernetes/#kubernetes-topics","text":"Building a Kubernetes Cluster Managing applications with Kubernetes Persistent Volumes in Kubernetes Minikube","title":"Kubernetes topics"},{"location":"general/kubernetes/#third-party-kubernetes-topics","text":"Deploy Kubernetes with kubespray Feedback via Github Deploy ProjectJupyter JupyterHub Some K8s basics : A Pod is the smallest deployable unit of Kubernetes. A Pod can contain one or more containers on a single IP address (internal). Containers within a pod can communicate with each other. Pods are run on Nodes. A Node is a physical or virtual machine and it\u2019s components include kubelet, kube-proxy and Container run-time. Kubelet is the Node agent that manages containers created by Kubernetes. Kube-proxy is used by Service to run a network proxy on each Node. Container runtime is the software responsible for running containers. Ex: Docker, containerd, and CRI-O. Service allows the use of a stable network address for an application, including a persistent ip address and a local DNS entry within the cluster. Service also acts as a load-balancer across all pods allowing the addition removal of pods from a deployment. ReplicaSet is a convenient way to build multiple pods at once. Deployment provides declarative updates for Pods and ReplicaSets (rolling updates) Control Pane is a node that controls/manages worker nodes. The components of the Control Pane are the API Server, the cluster store, the controller manager, and the scheduler. API server \u2013 All communication between the various Kubernetes components must go through the API server Cluster Store or etcd \u2013 Persistently stores the cluster\u2019s configuration, state, and metadata. The Controller Manager runs controller processes that monitor and respond to controller events. Some examples of control loops include; node controller, job controller, end point controller, and replication controller. The Scheduler assigns pods to nodes.","title":"Third Party Kubernetes topics"},{"location":"general/licenses/","text":"Software Licenses \u00b6 Software licenses are generally the responsibility of the user to obtain, coordinate, implement, and maintain. Some licenses may require users to coordinate with Jetstream staff to allow for proper configuration. Such coordination is handled on a case-by-base basis. Licensed software maintained by staff but available to users are listed below. Compilers and Parallel Libraries \u00b6 The Intel Compiler version on Jetstream2 is Intel OneAPI and no longer requires licensing for academic usage. Specialty software \u00b6 MATLAB is covered on the primary cloud by a site license and has been configured to point to the proper license server. If you need licenses for any other software that is not provided through the Jetstream2 shared software store then you will have to provide your own license. Licenses on regional clouds may vary and will need to be discussed with the regional cloud provider. Please be aware that if you take an image or software from Jetstream and run it somewhere else then the license will not work. Jetstream license servers restrict access to only valid Jetstream IP addresses. It is also the responsibility of the PI and those placed on each allocation to ensure that they are complying with the terms of any software license they are using on Jetstream2.","title":"Software Licenses"},{"location":"general/licenses/#software-licenses","text":"Software licenses are generally the responsibility of the user to obtain, coordinate, implement, and maintain. Some licenses may require users to coordinate with Jetstream staff to allow for proper configuration. Such coordination is handled on a case-by-base basis. Licensed software maintained by staff but available to users are listed below.","title":"Software Licenses"},{"location":"general/licenses/#compilers-and-parallel-libraries","text":"The Intel Compiler version on Jetstream2 is Intel OneAPI and no longer requires licensing for academic usage.","title":"Compilers and Parallel Libraries"},{"location":"general/licenses/#specialty-software","text":"MATLAB is covered on the primary cloud by a site license and has been configured to point to the proper license server. If you need licenses for any other software that is not provided through the Jetstream2 shared software store then you will have to provide your own license. Licenses on regional clouds may vary and will need to be discussed with the regional cloud provider. Please be aware that if you take an image or software from Jetstream and run it somewhere else then the license will not work. Jetstream license servers restrict access to only valid Jetstream IP addresses. It is also the responsibility of the PI and those placed on each allocation to ensure that they are complying with the terms of any software license they are using on Jetstream2.","title":"Specialty software"},{"location":"general/manila/","text":"Manila - Filesystems-as-a-service - on Jetstream2 \u00b6 Manila is the file share service project for OpenStack. Manila provides the management of file shares for example, NFS and CIFS, as a core service to OpenStack. Manila works with a variety of proprietary backend storage arrays and appliances, with open source distributed filesystems, as well as with a base Linux NFS or Samba server. With Manila, you can have multiple instances on Jetstream2 share a filesystem. If you are not using a Jetstream2 Featured image, you\u2019ll need to make sure you have these packages installed on your instance: ceph-commons and ceph-fuse You can set up and manage Manila shares via Horizon and the CLI presently: Manila via Horizon Manila via Openstack CLI Once a Manila share is created, you can mount and use it on your VM managed from any valid Jetstream2 interface. Configuring a VM to use Manila Shares","title":"Manila"},{"location":"general/manila/#manila-filesystems-as-a-service-on-jetstream2","text":"Manila is the file share service project for OpenStack. Manila provides the management of file shares for example, NFS and CIFS, as a core service to OpenStack. Manila works with a variety of proprietary backend storage arrays and appliances, with open source distributed filesystems, as well as with a base Linux NFS or Samba server. With Manila, you can have multiple instances on Jetstream2 share a filesystem. If you are not using a Jetstream2 Featured image, you\u2019ll need to make sure you have these packages installed on your instance: ceph-commons and ceph-fuse You can set up and manage Manila shares via Horizon and the CLI presently: Manila via Horizon Manila via Openstack CLI Once a Manila share is created, you can mount and use it on your VM managed from any valid Jetstream2 interface. Configuring a VM to use Manila Shares","title":"Manila - Filesystems-as-a-service - on Jetstream2"},{"location":"general/manilaVM/","text":"Configuring a VM to use Manila Shares \u00b6 Please note that any items with $something are placeholders for your names that you used when you created the manila shares in Horizon or via the CLI. 1. Create a mount point on your instance \u00b6 mkdir /mnt/ceph 2.Configuring your instance \u00b6 i. Create the file /etc/ceph.$accessTo.secret and add the accessKey Example: vi /etc/ceph.manilashare.secret and add AQAHfhZiwTf/NhAAT5ChE4tDXt3Nq1NyiURbMQ== Substitute the key value from the share creation above. Also, make sure the permissions on the file are rw to the owner only. You can do that with sudo chmod 600 /etc/ceph.manilashare.secret ii. Edit /etc/fstab to include the following line: $path /mnt/ceph ceph name=$accessTo,secretfile=/etc/ceph.$accessTo.secret,x-systemd.device-timeout=30,x-systemd.mount-timeout=30,noatime,_netdev,rw 0 2 $path = ips:ports followed by volume path (/volume/_no-group/\u2026) Please make sure to change the $variables to whatever you set these values to in Manila or the CLI when you created the share Example: 149.165.158.38:6789,149.165.158.22:6789,149.165.158.54:6789,149.165.158.70:6789,149.165.158.86:6789:/volumes/_nogroup/fe4f8ad4-2877-4e23-b5d3-46eb8476750b/ab404bac-9584-45f4-8a34-92dfc61fbb98 /mnt/ceph ceph name=manilashare,secretfile=/etc/ceph/ceph.manilashare.secret,x-systemd.device-timeout=30,x-systemd.mount-timeout=30,noatime,_netdev,rw 0 2 3. Mount the share \u00b6 Mount the manila share created with the following command mount -a If you then run a df -h|grep vol you should see something like this: 149.165.158.38:6789,149.165.158.22:6789,149.165.158.54:6789,149.165.158.70:6789,149.165.158.86:6789:/volumes/_nogroup/fe4f8ad4-2877-4e23-b5d3-46eb8476750b/ab404bac-9584-45f4-8a34-92dfc61fbb98 1.8T 134G 1.7T 2% /mnt/ceph","title":"Using Manila Share on a VM"},{"location":"general/manilaVM/#configuring-a-vm-to-use-manila-shares","text":"Please note that any items with $something are placeholders for your names that you used when you created the manila shares in Horizon or via the CLI.","title":"Configuring a VM to use Manila Shares"},{"location":"general/manilaVM/#1-create-a-mount-point-on-your-instance","text":"mkdir /mnt/ceph","title":"1. Create a mount point on your instance"},{"location":"general/manilaVM/#2configuring-your-instance","text":"i. Create the file /etc/ceph.$accessTo.secret and add the accessKey Example: vi /etc/ceph.manilashare.secret and add AQAHfhZiwTf/NhAAT5ChE4tDXt3Nq1NyiURbMQ== Substitute the key value from the share creation above. Also, make sure the permissions on the file are rw to the owner only. You can do that with sudo chmod 600 /etc/ceph.manilashare.secret ii. Edit /etc/fstab to include the following line: $path /mnt/ceph ceph name=$accessTo,secretfile=/etc/ceph.$accessTo.secret,x-systemd.device-timeout=30,x-systemd.mount-timeout=30,noatime,_netdev,rw 0 2 $path = ips:ports followed by volume path (/volume/_no-group/\u2026) Please make sure to change the $variables to whatever you set these values to in Manila or the CLI when you created the share Example: 149.165.158.38:6789,149.165.158.22:6789,149.165.158.54:6789,149.165.158.70:6789,149.165.158.86:6789:/volumes/_nogroup/fe4f8ad4-2877-4e23-b5d3-46eb8476750b/ab404bac-9584-45f4-8a34-92dfc61fbb98 /mnt/ceph ceph name=manilashare,secretfile=/etc/ceph/ceph.manilashare.secret,x-systemd.device-timeout=30,x-systemd.mount-timeout=30,noatime,_netdev,rw 0 2","title":"2.Configuring your instance"},{"location":"general/manilaVM/#3-mount-the-share","text":"Mount the manila share created with the following command mount -a If you then run a df -h|grep vol you should see something like this: 149.165.158.38:6789,149.165.158.22:6789,149.165.158.54:6789,149.165.158.70:6789,149.165.158.86:6789:/volumes/_nogroup/fe4f8ad4-2877-4e23-b5d3-46eb8476750b/ab404bac-9584-45f4-8a34-92dfc61fbb98 1.8T 134G 1.7T 2% /mnt/ceph","title":"3. Mount the share"},{"location":"general/minikube/","text":"Minikube \u00b6 Minikube allows you to run a Kubernetes cluster locally. Installation \u00b6 Install Minikube on your Virtual Machine: curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 sudo install minikube-linux-amd64 /usr/local/bin/minikube sudo minikube config set vm-driver none Note: If you have VirtualBox installed, you can specify vm-driver=VirtualBox Install kubectl: curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl chmod +x kubectl sudo mv ./kubectl /usr/local/bin/kubectl Start Minikube: minikube start You can check your kubectl installation running the following command: kubectl get pods --all-namespaces This should list all the kube-system pods currently running on the machine. Create a Deployment kubectl create deployment --image nginx my-nginx kubectl expose deployment my-nginx --port=80 --type=NodePort kubectl get svc Start Dashboard To view your minikube dashboard, run the following command. minikube dashboard --url & This might throw an error depending on what\u2019s installed on your machine. To access your dashboard you need to run Kubeproxy. Kubeproxy kubectl proxy --address=0.0.0.0 --accept-hosts='.*' View Minikube Dashboard To view your minikube dashboard from your web browser, run the following command: http://{host_ip}:8001/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ (Replace host_ip with the ip address of your VM) Delete Cluster minikube delete","title":"Minikube"},{"location":"general/minikube/#minikube","text":"Minikube allows you to run a Kubernetes cluster locally.","title":"Minikube"},{"location":"general/minikube/#installation","text":"Install Minikube on your Virtual Machine: curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 sudo install minikube-linux-amd64 /usr/local/bin/minikube sudo minikube config set vm-driver none Note: If you have VirtualBox installed, you can specify vm-driver=VirtualBox Install kubectl: curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl chmod +x kubectl sudo mv ./kubectl /usr/local/bin/kubectl Start Minikube: minikube start You can check your kubectl installation running the following command: kubectl get pods --all-namespaces This should list all the kube-system pods currently running on the machine. Create a Deployment kubectl create deployment --image nginx my-nginx kubectl expose deployment my-nginx --port=80 --type=NodePort kubectl get svc Start Dashboard To view your minikube dashboard, run the following command. minikube dashboard --url & This might throw an error depending on what\u2019s installed on your machine. To access your dashboard you need to run Kubeproxy. Kubeproxy kubectl proxy --address=0.0.0.0 --accept-hosts='.*' View Minikube Dashboard To view your minikube dashboard from your web browser, run the following command: http://{host_ip}:8001/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ (Replace host_ip with the ip address of your VM) Delete Cluster minikube delete","title":"Installation"},{"location":"general/object/","text":"Using the Object Store on Jetstream2 \u00b6 The object store is presently experimental. It WILL be a production service in coming weeks/months, but is under active development presently. The object store is only available via Horizon and the CLI presently. Documentation will continue to evolve. The Jetstream object store utilizes Openstack Swift and is S3 compatible. You can utilize it via Horizon or the command line interface (CLI). From the CLI, you can use the python-swiftclient or the aws s3api or compatible tools. Horizon instructions will be coming soon. Though it does not appear that you can generate the EC2 credentials in Horizon. Prerequisites \u00b6 If the you want to use s3 compatibility, you\u2019ll need to generate EC2 credentials. This assumes you already have the python-openstackclient installed and have generated an application credential for the CLI. If you do not have an application credential openrc and CLI clients installed, please see these pages: Creating an application credential and openrc on Jetstream2 Installing Openstack Clients Optionally, you\u2019ll want an AWS s3api client like the aws command line interface reference client There\u2019s also a tutorial from Andrea Zonca on Using the distributed file format Zarr on Jetstream 2 object storage Using the object store with s3api compatibility \u00b6 Once you have sourced your application credential based openrc and installed the python-openstack and python-swiftclient client you will need to generate your ec2-style credentials. The CLI command is: openstack ec2 credentials create You can save the creds in a config to use from the CLI/programatically. It\u2019s generally kept in the text file ~/.aws/credentials and looks like this: [default] region=RegionOne aws_access_key_id=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx aws_secret_access_key=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx For your ~/.aws/credentials , make sure the value from the credential create under \u201cAccess\u201d matches \u201caws_access_key_id\u201d and the Secret matches \u201caws_secret_access_key\u201d You can retrieve your ec2 credentials later, as well, by doing openstack ec2 credentials list openstack ec2 credentials show *access_key_value* The endpoint you\u2019ll need for S3 operations is https://js2.jetstream-cloud.org:8001/ For s3 style operations, you\u2019ll want to use the s3api functions. Those are documented here: https://docs.aws.amazon.com/cli/latest/reference/s3api/ Using the aws command line interface you can test operations. aws s3api --endpoint-url \"https://js2.jetstream-cloud.org:8001/\" create-bucket --bucket my-unique-bucket-name and to add a file to the bucket: aws s3api --endpoint-url \"https://js2.jetstream-cloud.org:8001/\" put-object --bucket my-unique-bucket-name --key my-file.zip --body my-file.zip and to see bucket contents: aws s3api --endpoint-url \"https://js2.jetstream-cloud.org:8001/\" list-objects --bucket my-unique-bucket-name Trying the object store from the CLI using Swift \u00b6 To use the OpenStack CLI natively with the object store, you\u2019ll need the Swift client if you have not already installed it. You can install it by doing: pip install python-swiftclient Once you have the Swift client installed, you can test it by doing: swift post my-unique-bucket-name which will create a storage container called \u201cmy-unique-bucket-name\u201d. You can then list your buckets by doing: swift list If you want to delete the test bucket, you can do: swift delete my-unique-bucket-name As with all Openstack clients, you can see the full list of commands with swift help You can also add, remove, and otherwise work with swift containers (buckets in the S3 vernacular) in Horizon on the Project \u2192 Object Store \u2192 Containers tab.","title":"Object Store"},{"location":"general/object/#using-the-object-store-on-jetstream2","text":"The object store is presently experimental. It WILL be a production service in coming weeks/months, but is under active development presently. The object store is only available via Horizon and the CLI presently. Documentation will continue to evolve. The Jetstream object store utilizes Openstack Swift and is S3 compatible. You can utilize it via Horizon or the command line interface (CLI). From the CLI, you can use the python-swiftclient or the aws s3api or compatible tools. Horizon instructions will be coming soon. Though it does not appear that you can generate the EC2 credentials in Horizon.","title":"Using the Object Store on Jetstream2"},{"location":"general/object/#prerequisites","text":"If the you want to use s3 compatibility, you\u2019ll need to generate EC2 credentials. This assumes you already have the python-openstackclient installed and have generated an application credential for the CLI. If you do not have an application credential openrc and CLI clients installed, please see these pages: Creating an application credential and openrc on Jetstream2 Installing Openstack Clients Optionally, you\u2019ll want an AWS s3api client like the aws command line interface reference client There\u2019s also a tutorial from Andrea Zonca on Using the distributed file format Zarr on Jetstream 2 object storage","title":"Prerequisites"},{"location":"general/object/#using-the-object-store-with-s3api-compatibility","text":"Once you have sourced your application credential based openrc and installed the python-openstack and python-swiftclient client you will need to generate your ec2-style credentials. The CLI command is: openstack ec2 credentials create You can save the creds in a config to use from the CLI/programatically. It\u2019s generally kept in the text file ~/.aws/credentials and looks like this: [default] region=RegionOne aws_access_key_id=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx aws_secret_access_key=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx For your ~/.aws/credentials , make sure the value from the credential create under \u201cAccess\u201d matches \u201caws_access_key_id\u201d and the Secret matches \u201caws_secret_access_key\u201d You can retrieve your ec2 credentials later, as well, by doing openstack ec2 credentials list openstack ec2 credentials show *access_key_value* The endpoint you\u2019ll need for S3 operations is https://js2.jetstream-cloud.org:8001/ For s3 style operations, you\u2019ll want to use the s3api functions. Those are documented here: https://docs.aws.amazon.com/cli/latest/reference/s3api/ Using the aws command line interface you can test operations. aws s3api --endpoint-url \"https://js2.jetstream-cloud.org:8001/\" create-bucket --bucket my-unique-bucket-name and to add a file to the bucket: aws s3api --endpoint-url \"https://js2.jetstream-cloud.org:8001/\" put-object --bucket my-unique-bucket-name --key my-file.zip --body my-file.zip and to see bucket contents: aws s3api --endpoint-url \"https://js2.jetstream-cloud.org:8001/\" list-objects --bucket my-unique-bucket-name","title":"Using the object store with s3api compatibility"},{"location":"general/object/#trying-the-object-store-from-the-cli-using-swift","text":"To use the OpenStack CLI natively with the object store, you\u2019ll need the Swift client if you have not already installed it. You can install it by doing: pip install python-swiftclient Once you have the Swift client installed, you can test it by doing: swift post my-unique-bucket-name which will create a storage container called \u201cmy-unique-bucket-name\u201d. You can then list your buckets by doing: swift list If you want to delete the test bucket, you can do: swift delete my-unique-bucket-name As with all Openstack clients, you can see the full list of commands with swift help You can also add, remove, and otherwise work with swift containers (buckets in the S3 vernacular) in Horizon on the Project \u2192 Object Store \u2192 Containers tab.","title":"Trying the object store from the CLI using Swift"},{"location":"general/policies/","text":"Acceptable Use Policies and Research and Export Control Guidance \u00b6 Acceptable Use of Jetstream2 \u00b6 Jetstream2 requires compliance with all XSEDE and Indiana University policies, including but not limited to: XSEDE Usage Policy Indiana University IT-02 Misuse and Abuse of Information Technology Resources IT-12 Security of Information Technology Resources Regional clouds may have additional usage and security policies. If you are using one of the regional clouds, please check with your support team there for more information. Protected Data and Jetstream2 \u00b6 Jetstream2 researchers agree to: Ensure that data that must be protected by Federal security or privacy laws (e.g., HIPAA, FERPA, ITAR, classified information, export control, etc.) are not stored on this system unless such storage and usage is specifically authorized by the responsible University administrator and complies with any processes for management of access to such information. For export controlled information, including ITAR information, approval of the University Export Compliance Office is required prior to use of the Jetstream2 systems for storage/processing of export controlled data. The Jetstream2 system is not intended, by default, to meet the security requirements of these laws or regulations and specific usage related controls or restrictions may be required prior to authorization of the use of the Jetstream2 system for such purposes. Ensure that the project does not violate any export control end use restrictions contained in Part 744 of the EAR . Follow all US government guidance on export controls for research and research data. Please see Jetstream2 Export Control Guidance for more information. Jetstream2 and General Research Policies \u00b6 In general, fundamental, publishable research is permitted on Jetstream2 from any non-EAR sanctioned countries. Fundamental research is defined as: \u201cFundamental research means basic and applied research in science and engineering, the results of which ordinarily are published and shared broadly within the scientific community, as distinguished from proprietary research and from Industrial development, design, production, and product utilization, the results of which ordinarily are restricted for proprietary or national security reason.\u201d [National Security Decision Directive (NSDD) 189, National Policy on the Transfer of Scientific, Technical, and Engineering Information] For anything other than fundamental, publishable research, you may wish to consult with your institution\u2019s export control office for any export/sharing restrictions. Specialty System (GPU & Large Memory) Specific Policies \u00b6 Only g3.* flavors should be run on the Jetstream2-GPU resource Only r3.* flavors should be run on the Jetstream2-LargeMemory resource Running standard compute (m3.*) flavors on the specialty resources may result in those instances being deleted without warning At some future point, if GPU or large memory resources are scarce, we may limit runtime to two weeks spans or institute some form VM scheduling service to ensure equitable access to all. We do not anticipate doing this at this time. Allocation Related Policies \u00b6 Jetstream2 requires an active XSEDE allocation for access. If your allocation expires you will no longer be able to access Jetstream2 or your resources. Presently, XSEDE warns PIs monthly, starting at 3 months until allocation expiration, and at the time of expiration. Jetstream2 policy is that we will do the following when allocations expire: At expiration + 1 day - the allocation will be disabled on Jetstream2 and access is no longer possible to allocation users If the allocation has not been renewed (preferred) or extended in 10 days , all VMs on the allocation will be shelved and thus no longer accessible If the allocation has not been renewed (preferred) or extended in 30 days , all resources (VMs, volumes, shares, images, etc) on the allocation will be destroyed and will not be recoverable","title":"Acceptable Usage Policies"},{"location":"general/policies/#acceptable-use-policies-and-research-and-export-control-guidance","text":"","title":"Acceptable Use Policies and Research and Export Control Guidance"},{"location":"general/policies/#acceptable-use-of-jetstream2","text":"Jetstream2 requires compliance with all XSEDE and Indiana University policies, including but not limited to: XSEDE Usage Policy Indiana University IT-02 Misuse and Abuse of Information Technology Resources IT-12 Security of Information Technology Resources Regional clouds may have additional usage and security policies. If you are using one of the regional clouds, please check with your support team there for more information.","title":"Acceptable Use of Jetstream2"},{"location":"general/policies/#protected-data-and-jetstream2","text":"Jetstream2 researchers agree to: Ensure that data that must be protected by Federal security or privacy laws (e.g., HIPAA, FERPA, ITAR, classified information, export control, etc.) are not stored on this system unless such storage and usage is specifically authorized by the responsible University administrator and complies with any processes for management of access to such information. For export controlled information, including ITAR information, approval of the University Export Compliance Office is required prior to use of the Jetstream2 systems for storage/processing of export controlled data. The Jetstream2 system is not intended, by default, to meet the security requirements of these laws or regulations and specific usage related controls or restrictions may be required prior to authorization of the use of the Jetstream2 system for such purposes. Ensure that the project does not violate any export control end use restrictions contained in Part 744 of the EAR . Follow all US government guidance on export controls for research and research data. Please see Jetstream2 Export Control Guidance for more information.","title":"Protected Data and Jetstream2"},{"location":"general/policies/#jetstream2-and-general-research-policies","text":"In general, fundamental, publishable research is permitted on Jetstream2 from any non-EAR sanctioned countries. Fundamental research is defined as: \u201cFundamental research means basic and applied research in science and engineering, the results of which ordinarily are published and shared broadly within the scientific community, as distinguished from proprietary research and from Industrial development, design, production, and product utilization, the results of which ordinarily are restricted for proprietary or national security reason.\u201d [National Security Decision Directive (NSDD) 189, National Policy on the Transfer of Scientific, Technical, and Engineering Information] For anything other than fundamental, publishable research, you may wish to consult with your institution\u2019s export control office for any export/sharing restrictions.","title":"Jetstream2 and General Research Policies"},{"location":"general/policies/#specialty-system-gpu-large-memory-specific-policies","text":"Only g3.* flavors should be run on the Jetstream2-GPU resource Only r3.* flavors should be run on the Jetstream2-LargeMemory resource Running standard compute (m3.*) flavors on the specialty resources may result in those instances being deleted without warning At some future point, if GPU or large memory resources are scarce, we may limit runtime to two weeks spans or institute some form VM scheduling service to ensure equitable access to all. We do not anticipate doing this at this time.","title":"Specialty System (GPU &amp; Large Memory) Specific Policies"},{"location":"general/policies/#allocation-related-policies","text":"Jetstream2 requires an active XSEDE allocation for access. If your allocation expires you will no longer be able to access Jetstream2 or your resources. Presently, XSEDE warns PIs monthly, starting at 3 months until allocation expiration, and at the time of expiration. Jetstream2 policy is that we will do the following when allocations expire: At expiration + 1 day - the allocation will be disabled on Jetstream2 and access is no longer possible to allocation users If the allocation has not been renewed (preferred) or extended in 10 days , all VMs on the allocation will be shelved and thus no longer accessible If the allocation has not been renewed (preferred) or extended in 30 days , all resources (VMs, volumes, shares, images, etc) on the allocation will be destroyed and will not be recoverable","title":"Allocation Related Policies"},{"location":"general/quotas/","text":"Quotas \u00b6 Jetstream2 tries to apportion the memory, storage, and network available to researchers based on both allocation type and Jetstream2 resource node type. Limit Types \u00b6 Quotas are a function of: allocation type: \u00b6 Startup/Research Education Jetstream Trial Allocations (JTA) default allocations for collaborative users in a common research project. special allocations for teaching and workshops. Quotas are set slightly higher because of typical larger user counts Sample allocations with minimal capacity quotas designed for evaluation purposes resource type: \u00b6 CPU (compute) GPU LM (large memory compute) normal bulk compute resources with higher available core counts GPU accelerated resource with additional ceph storage 2X-memory compute resource with additiona ceph storage 75% of JS2 19% of JS2 6% of JS2 3.9 GB per RAM per core 7.8 GB RAM per core 3.9 GB RAM per core STARTUP/RESEARCH defaults * CPU GPU LM 150 cores 585 GB RAM 10 floating IPs 64 cores 250 GB RAM 4 floating IPs 128 cores 1000 GB RAM 2 floating IPs EDUCATION defaults * CPU GPU LM 200 cores 780 GB RAM 25 floating IPs 320 cores 1250 GB RAM 25 floating IPs 128 cores 1000 GB RAM 2 floating IPs JTA defaults CPU GPU LM 2 cores 6 GB RAM 2 floating IPs none none * modifications available with proper documentation","title":"Quotas"},{"location":"general/quotas/#quotas","text":"Jetstream2 tries to apportion the memory, storage, and network available to researchers based on both allocation type and Jetstream2 resource node type.","title":"Quotas"},{"location":"general/quotas/#limit-types","text":"Quotas are a function of:","title":"Limit Types"},{"location":"general/quotas/#allocation-type","text":"Startup/Research Education Jetstream Trial Allocations (JTA) default allocations for collaborative users in a common research project. special allocations for teaching and workshops. Quotas are set slightly higher because of typical larger user counts Sample allocations with minimal capacity quotas designed for evaluation purposes","title":"allocation type:"},{"location":"general/quotas/#resource-type","text":"CPU (compute) GPU LM (large memory compute) normal bulk compute resources with higher available core counts GPU accelerated resource with additional ceph storage 2X-memory compute resource with additiona ceph storage 75% of JS2 19% of JS2 6% of JS2 3.9 GB per RAM per core 7.8 GB RAM per core 3.9 GB RAM per core STARTUP/RESEARCH defaults * CPU GPU LM 150 cores 585 GB RAM 10 floating IPs 64 cores 250 GB RAM 4 floating IPs 128 cores 1000 GB RAM 2 floating IPs EDUCATION defaults * CPU GPU LM 200 cores 780 GB RAM 25 floating IPs 320 cores 1250 GB RAM 25 floating IPs 128 cores 1000 GB RAM 2 floating IPs JTA defaults CPU GPU LM 2 cores 6 GB RAM 2 floating IPs none none * modifications available with proper documentation","title":"resource type:"},{"location":"general/resources/","text":"Jetstream2 Resources \u00b6 As noted on the Key Differences page, Jetstream2 consists of four distinct XSEDE-allocated resources. These resources are: Jetstream2 (CPU only) Jetstream2 Large Memory Jetstream2 GPU Jetstream2 Storage The specifications for each are linked above. With the exception of Jetstream2 Storage, these resources may be allocated individually. Jetstream2 Storage requires the PI apply for or already have an allocation on one of the three Jetstream compute resources. Maximum Startup/Campus Champion Allocation values for each resource are: \u00b6 Jetstream2 CPU - 200,000 SUs Jetstream2 Large Memory - 400,000 SUs Jetstream2 GPU - 600,000 SUs Jetstream2 Storage - 1TB default* * Storage limits may be larger than 1TB per allocation for a startup if well-justified. Limits and important notes: \u00b6 There are no request limits for Education or Research allocations. However, all requests must show the appropriate justification. Please refer to Jetstream2 Education Allocations or the XSEDE Research Allocation page for additional information on how to create an appropriate Education or Research allocation request. There are no restrictions on runtime for the Jetstream2 CPU resource. As long as you have an active allocation and SUs remaining, you may run your Jetstream2 CPU VM(s) continuously or on demand. Jetstream2 GPU and Jetstream2 Large Memory may have runtime restriction placed at a future date. Present policy notes that Jetstream2 Staff may limit runtime on these resources to two weeks at a time. As long as resources are not in contention, we may opt to allow continuous running of VMs/services. Any change in resource limits will be noted via XSEDE User News Jetstream2 Storage \u00b6 Jetstream2 storage is an allocated resource. All allocations will be given a default storage amount (as noted on the Jetstream2 Resources page). This storage is usable by all users on that allocation so the PI may want to institute per user quotas or discuss proper usage etiquette with the members of their allocation. Jetstream2 staff will not institute per user storage quotas, with the exception of the Jetstream2 Trial Allocation. Limits on Jetstream2 Storage Startup allocations are generally limited to 5-10TB max Education allocations are generally limited to 10TB max Research allocations are genereally limited to 40TB max All are subject to proper justification in the allocations process. Maxmimum values may be adjusted with proper justification and if there are adequate resources available. This is entirely at the discretion of the Jetstream2 team. Please refer to the following pages for more information on using Jetstream2 storage under the various interfaces: Using Jetstream2 Storage Under Cacao Using Jetstream2 Storage Under Exosphere Using Jetstream2 Storage Under Horizon Using Jetstream2 Storage Under the CLI Using Jetstream2 Storage with Manila Using Jetstream2 Storage with Object Store","title":"Jetstream2 Resources"},{"location":"general/resources/#jetstream2-resources","text":"As noted on the Key Differences page, Jetstream2 consists of four distinct XSEDE-allocated resources. These resources are: Jetstream2 (CPU only) Jetstream2 Large Memory Jetstream2 GPU Jetstream2 Storage The specifications for each are linked above. With the exception of Jetstream2 Storage, these resources may be allocated individually. Jetstream2 Storage requires the PI apply for or already have an allocation on one of the three Jetstream compute resources.","title":"Jetstream2 Resources"},{"location":"general/resources/#maximum-startupcampus-champion-allocation-values-for-each-resource-are","text":"Jetstream2 CPU - 200,000 SUs Jetstream2 Large Memory - 400,000 SUs Jetstream2 GPU - 600,000 SUs Jetstream2 Storage - 1TB default* * Storage limits may be larger than 1TB per allocation for a startup if well-justified.","title":"Maximum Startup/Campus Champion Allocation values for each resource are:"},{"location":"general/resources/#limits-and-important-notes","text":"There are no request limits for Education or Research allocations. However, all requests must show the appropriate justification. Please refer to Jetstream2 Education Allocations or the XSEDE Research Allocation page for additional information on how to create an appropriate Education or Research allocation request. There are no restrictions on runtime for the Jetstream2 CPU resource. As long as you have an active allocation and SUs remaining, you may run your Jetstream2 CPU VM(s) continuously or on demand. Jetstream2 GPU and Jetstream2 Large Memory may have runtime restriction placed at a future date. Present policy notes that Jetstream2 Staff may limit runtime on these resources to two weeks at a time. As long as resources are not in contention, we may opt to allow continuous running of VMs/services. Any change in resource limits will be noted via XSEDE User News","title":"Limits and important notes:"},{"location":"general/resources/#jetstream2-storage","text":"Jetstream2 storage is an allocated resource. All allocations will be given a default storage amount (as noted on the Jetstream2 Resources page). This storage is usable by all users on that allocation so the PI may want to institute per user quotas or discuss proper usage etiquette with the members of their allocation. Jetstream2 staff will not institute per user storage quotas, with the exception of the Jetstream2 Trial Allocation. Limits on Jetstream2 Storage Startup allocations are generally limited to 5-10TB max Education allocations are generally limited to 10TB max Research allocations are genereally limited to 40TB max All are subject to proper justification in the allocations process. Maxmimum values may be adjusted with proper justification and if there are adequate resources available. This is entirely at the discretion of the Jetstream2 team. Please refer to the following pages for more information on using Jetstream2 storage under the various interfaces: Using Jetstream2 Storage Under Cacao Using Jetstream2 Storage Under Exosphere Using Jetstream2 Storage Under Horizon Using Jetstream2 Storage Under the CLI Using Jetstream2 Storage with Manila Using Jetstream2 Storage with Object Store","title":"Jetstream2 Storage"},{"location":"general/software/","text":"Jetstream2 Software Collection \u00b6 Details on the Jetstream2 Software Collection will be forthcoming. At this time, we anticipate a shared disk that will be automatically mounted by instances. Researchers will then be able to use modules to load any combination of software they need, similar to how HPC systems do it. This list will be ever evolving, but at this time, we anticipate the following software on the shared software store: (Items with a * are available now. The rest will be coming shortly.) Matlab* R* / R Studio / Shiny Intel Compiler (Intel OneAPI)* Singularity/Apptainer* NVIDIA (formerly PGI) Compiler AOCC Compiler Anaconda* (Jupyter is available via Anaconda!) Databases (e.g. Mysql, Postgresql, Mongo) Java Jupyter/JupyterLab/JupyterHub (as part of Anaconda) Items like CUDA and the NVIDIA HPC SDK are not installed on the VMs or in the software collection. We recommend using the NVIDIA containers for GPU-related software from NVIDIA as the ecosystem can often be problematic for individual local installations. The NVIDIA Container Toolkit is installed on all featured images. This is still an ongoing effort so this list will almost certainly change in coming weeks and months.","title":"Jetstream2 Software Collection"},{"location":"general/software/#jetstream2-software-collection","text":"Details on the Jetstream2 Software Collection will be forthcoming. At this time, we anticipate a shared disk that will be automatically mounted by instances. Researchers will then be able to use modules to load any combination of software they need, similar to how HPC systems do it. This list will be ever evolving, but at this time, we anticipate the following software on the shared software store: (Items with a * are available now. The rest will be coming shortly.) Matlab* R* / R Studio / Shiny Intel Compiler (Intel OneAPI)* Singularity/Apptainer* NVIDIA (formerly PGI) Compiler AOCC Compiler Anaconda* (Jupyter is available via Anaconda!) Databases (e.g. Mysql, Postgresql, Mongo) Java Jupyter/JupyterLab/JupyterHub (as part of Anaconda) Items like CUDA and the NVIDIA HPC SDK are not installed on the VMs or in the software collection. We recommend using the NVIDIA containers for GPU-related software from NVIDIA as the ecosystem can often be problematic for individual local installations. The NVIDIA Container Toolkit is installed on all featured images. This is still an ongoing effort so this list will almost certainly change in coming weeks and months.","title":"Jetstream2 Software Collection"},{"location":"general/sudo/","text":"Being root on your virtual machines \u00b6 By default, the user that creates a virtual machine on Jetstream2 has the ability to be the privileged (root) user. This user or superuser has full ability to control all aspects of the operating system. Because of this, it\u2019s recommended that you use this privelege with extreme caution and only be the root user when you absolutely have to. It\u2019s generally a best practice to only invoke single commands as root whenever possible. An example of this might be to look at restricted log files: sudo less /var/log/syslog or installing software (on an Ubuntu system in this example): sudo apt-get install nginx If you need to become the root user, there are a number of ways to do so using the su command. One generally recommended method is to use sudo on the su (superuser) command: sudo su - In the above example, you are switching to the root user. The (-) switch provides you with root\u2019s environment (path and shell variables) rather than simply giving you root user power for a single command while keeping your own environment. This ensures any of your personal settings or configurations do not have any unexpected effects while becoming the root user. If you\u2019re using a Jetstream2 Featured Image you will get a prompt that now identifies you as the root user: root@u20-test:~# Once you have completed the tasks you need to do as root, you should exit the root shell session by doing either control-d or typing exit at the prompt. This should return you to your normal user account. Keep in mind that some actions you take as root are non-recoverable. Always use caution when using sudo or su to perform tasks as root. It\u2019s a best practice to not be root unless you absolutely need to.","title":"Being the root user"},{"location":"general/sudo/#being-root-on-your-virtual-machines","text":"By default, the user that creates a virtual machine on Jetstream2 has the ability to be the privileged (root) user. This user or superuser has full ability to control all aspects of the operating system. Because of this, it\u2019s recommended that you use this privelege with extreme caution and only be the root user when you absolutely have to. It\u2019s generally a best practice to only invoke single commands as root whenever possible. An example of this might be to look at restricted log files: sudo less /var/log/syslog or installing software (on an Ubuntu system in this example): sudo apt-get install nginx If you need to become the root user, there are a number of ways to do so using the su command. One generally recommended method is to use sudo on the su (superuser) command: sudo su - In the above example, you are switching to the root user. The (-) switch provides you with root\u2019s environment (path and shell variables) rather than simply giving you root user power for a single command while keeping your own environment. This ensures any of your personal settings or configurations do not have any unexpected effects while becoming the root user. If you\u2019re using a Jetstream2 Featured Image you will get a prompt that now identifies you as the root user: root@u20-test:~# Once you have completed the tasks you need to do as root, you should exit the root shell session by doing either control-d or typing exit at the prompt. This should return you to your normal user account. Keep in mind that some actions you take as root are non-recoverable. Always use caution when using sudo or su to perform tasks as root. It\u2019s a best practice to not be root unless you absolutely need to.","title":"Being root on your virtual machines"},{"location":"general/terraform/","text":"Terraform on Jetstream2 \u00b6 Terraform is a open-source code as infrastructure tool that works with Jetstream2 and allows you to build and destroy infrastructure quickly and easily. Installing Terraform \u00b6 The documentation to install Terraform is located below. We have linked to the latest documentation from the Terraform team. https://www.terraform.io/downloads Basic Operations \u00b6 The basic setup is as follows: Git clone the terraform repo Download the openrc file into the repo Run source openrc.sh Initalize Terraform with terraform init See what changes Terraform wants to make to your infrastructure with terraform plan Apply the changes with terraform apply Getting started with git \u00b6 If you are unfamiliar with git, the documentation is located here git-scm Git clone \u00b6 Git clone allows you create a copy of a git repo that creates the directory and checks out the active branch. git clone git@github.com:wellsaar/terraform-jetstream.git This command creates a clone of the git repo that contains the code examples that we use below. The master branch of the git repo above should be a good starting point in developing your own terraform code. Running Terraform \u00b6 Make sure that you have setup and downloaded your openrc file. Setting up the openrc.sh for the Jetstream2 CLI You will need to source the openrc file once you have downloaded it in order to setup the variables that Terraform will need to connect to Jetstream2. Terraform Init \u00b6 terraform init This is the first command that should be run after writing a new Terraform configuration or cloning an existing one. This command is used to initialize a working directory containing Terraform configuration files. Terraform Init Documentation Example Terraform Init terraform-jetstream git: (jetstream2) x terraform init Initializing the backend.. Initializing provider plugins. - Reusing previous version of terraform-provider-openstack/openstack from the dependency lock file - Using previously-installed terraform-provider-openstack/openstackv1.42.0 Terraform has been successfully initialized! \u2022 You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. \u2022 If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. In this example, we have already initialized Terraform in this directory, and Terraform is checking the dependency lock file for any updates. You will need to run Terraform init if you make any changes to providers. Terraform Plan \u00b6 terraform plan This command creates an execution plan which allows you to preview the changes that Terraform will change. The screenshot below is only a small section of output of a plan. Terraform Plan Documentation Example Terraform Plan # openstack_networking_subnet_v2.terraform_subnet1 will be created + resource \"openstack_networking_subnet_v2\" \"terraform_subnet1\" { + all_tags = (known after apply) + cidr = \"192.168.0.0/24\" + enable_dhcp = true + gateway_ip = (known after apply) + id = (known after apply) + ip_version = 4 + ipv6_address_mode = (known after apply) + ipv6_ra_mode = (known after apply) + name = \"terraform_subnet1\" + network_id = (known after apply) + no_gateway = false + region = (known after apply) + tenant_id = (known after apply) + allocation_pool { + end = (known after apply) + start = (known after apply) } + allocation_pools { + end = (known after apply) + start = (known after apply) } } Plan: 8 to add, 0 to change, 0 to destroy. Changes to Outputs: + floating_ip_ubuntu20 = [ + (known after apply), ] \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Note: You didn't use the -out option to save this plan, so Terraform can't guarantee to take exactly these actions if you run \"terraform apply\" now. In this example, due to space issues only the networking section is shown. The resource section in the example lists how the resource will be configured, including any tags and the name of the resource. You can see on the Plan: line that 14 items will be added, 0 will be changed and 0 will be destroyed. Terraform Apply \u00b6 terraform apply This command makes changes based on what\u2019s defined in the Terraform files and what\u2019s existing in your infrastructure already. Terraform Apply Documentation Example Terraform Apply # openstack_networking_subnet_v2.terraform_subnet1 will be created + resource \"openstack_networking_subnet_v2\" \"terraform_subnet1\" { + all_tags = (known after apply) + cidr = \"192.168.0.0/24\" + enable_dhcp = true + gateway_ip = (known after apply) + id = (known after apply) + ip_version = 4 + ipv6_address_mode = (known after apply) + ipv6_ra_mode = (known after apply) + name = \"terraform_subnet1\" + network_id = (known after apply) + no_gateway = false + region = (known after apply) + tenant_id = (known after apply) + allocation_pool { + end = (known after apply) + start = (known after apply) } + allocation_pools { + end = (known after apply) + start = (known after apply) } } Plan: 8 to add, 0 to change, 0 to destroy. Changes to Outputs: + floating_ip_ubuntu20 = [ + (known after apply), ] Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes Writing YAML files and Validating YAML files \u00b6 Terraform code is written in YAML (YAML Ain\u2019t Markup Language). Indentation matters with YAML so listed below are some free tools that will help you write in YAML. yamllint atom.io","title":"Terraform on Jetstream2"},{"location":"general/terraform/#terraform-on-jetstream2","text":"Terraform is a open-source code as infrastructure tool that works with Jetstream2 and allows you to build and destroy infrastructure quickly and easily.","title":"Terraform on Jetstream2"},{"location":"general/terraform/#installing-terraform","text":"The documentation to install Terraform is located below. We have linked to the latest documentation from the Terraform team. https://www.terraform.io/downloads","title":"Installing Terraform"},{"location":"general/terraform/#basic-operations","text":"The basic setup is as follows: Git clone the terraform repo Download the openrc file into the repo Run source openrc.sh Initalize Terraform with terraform init See what changes Terraform wants to make to your infrastructure with terraform plan Apply the changes with terraform apply","title":"Basic Operations"},{"location":"general/terraform/#getting-started-with-git","text":"If you are unfamiliar with git, the documentation is located here git-scm","title":"Getting started with git"},{"location":"general/terraform/#git-clone","text":"Git clone allows you create a copy of a git repo that creates the directory and checks out the active branch. git clone git@github.com:wellsaar/terraform-jetstream.git This command creates a clone of the git repo that contains the code examples that we use below. The master branch of the git repo above should be a good starting point in developing your own terraform code.","title":"Git clone"},{"location":"general/terraform/#running-terraform","text":"Make sure that you have setup and downloaded your openrc file. Setting up the openrc.sh for the Jetstream2 CLI You will need to source the openrc file once you have downloaded it in order to setup the variables that Terraform will need to connect to Jetstream2.","title":"Running Terraform"},{"location":"general/terraform/#terraform-init","text":"terraform init This is the first command that should be run after writing a new Terraform configuration or cloning an existing one. This command is used to initialize a working directory containing Terraform configuration files. Terraform Init Documentation Example Terraform Init terraform-jetstream git: (jetstream2) x terraform init Initializing the backend.. Initializing provider plugins. - Reusing previous version of terraform-provider-openstack/openstack from the dependency lock file - Using previously-installed terraform-provider-openstack/openstackv1.42.0 Terraform has been successfully initialized! \u2022 You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. \u2022 If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. In this example, we have already initialized Terraform in this directory, and Terraform is checking the dependency lock file for any updates. You will need to run Terraform init if you make any changes to providers.","title":"Terraform Init"},{"location":"general/terraform/#terraform-plan","text":"terraform plan This command creates an execution plan which allows you to preview the changes that Terraform will change. The screenshot below is only a small section of output of a plan. Terraform Plan Documentation Example Terraform Plan # openstack_networking_subnet_v2.terraform_subnet1 will be created + resource \"openstack_networking_subnet_v2\" \"terraform_subnet1\" { + all_tags = (known after apply) + cidr = \"192.168.0.0/24\" + enable_dhcp = true + gateway_ip = (known after apply) + id = (known after apply) + ip_version = 4 + ipv6_address_mode = (known after apply) + ipv6_ra_mode = (known after apply) + name = \"terraform_subnet1\" + network_id = (known after apply) + no_gateway = false + region = (known after apply) + tenant_id = (known after apply) + allocation_pool { + end = (known after apply) + start = (known after apply) } + allocation_pools { + end = (known after apply) + start = (known after apply) } } Plan: 8 to add, 0 to change, 0 to destroy. Changes to Outputs: + floating_ip_ubuntu20 = [ + (known after apply), ] \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Note: You didn't use the -out option to save this plan, so Terraform can't guarantee to take exactly these actions if you run \"terraform apply\" now. In this example, due to space issues only the networking section is shown. The resource section in the example lists how the resource will be configured, including any tags and the name of the resource. You can see on the Plan: line that 14 items will be added, 0 will be changed and 0 will be destroyed.","title":"Terraform Plan"},{"location":"general/terraform/#terraform-apply","text":"terraform apply This command makes changes based on what\u2019s defined in the Terraform files and what\u2019s existing in your infrastructure already. Terraform Apply Documentation Example Terraform Apply # openstack_networking_subnet_v2.terraform_subnet1 will be created + resource \"openstack_networking_subnet_v2\" \"terraform_subnet1\" { + all_tags = (known after apply) + cidr = \"192.168.0.0/24\" + enable_dhcp = true + gateway_ip = (known after apply) + id = (known after apply) + ip_version = 4 + ipv6_address_mode = (known after apply) + ipv6_ra_mode = (known after apply) + name = \"terraform_subnet1\" + network_id = (known after apply) + no_gateway = false + region = (known after apply) + tenant_id = (known after apply) + allocation_pool { + end = (known after apply) + start = (known after apply) } + allocation_pools { + end = (known after apply) + start = (known after apply) } } Plan: 8 to add, 0 to change, 0 to destroy. Changes to Outputs: + floating_ip_ubuntu20 = [ + (known after apply), ] Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes","title":"Terraform Apply"},{"location":"general/terraform/#writing-yaml-files-and-validating-yaml-files","text":"Terraform code is written in YAML (YAML Ain\u2019t Markup Language). Indentation matters with YAML so listed below are some free tools that will help you write in YAML. yamllint atom.io","title":"Writing YAML files and Validating YAML files"},{"location":"general/usingsoftware/","text":"Using the Jetstream2 Software Collection \u00b6 Jetstream2 utilizes Lmod to load and unload software you may need in your research or education pursuits. The software is served from a shared store utilizing Manila shares that are mounted automatically if you\u2019re using a Jetstream2 Featured Image. All of the instructions on listing, loading, and unloading modules/software packages assume you are using a Jetstream2 featured image created after April 9, 2022. Additional information on using Lmod is available in the User Guide for Lmod Listing available software from the collection \u00b6 You can use module avail to show available software packages. This may show multiple versions of the same package in the future, which will default to the latest version but allow you to load previous versions. [exouser@rocky-lmod-host ~]$ module avail ---------------- /software/r8/modulefiles ------------------ R/4.1.2 (L) intel-OneAPI/2022.1.2 matlab/R2021a ---------------- /usr/share/lmod/lmod/modulefiles/Core ----- lmod settarg Where: L: Module is loaded Loading software from the collection \u00b6 You can load software from the collection using module load . [exouser@rocky-lmod-host ~]$ module load R You can also explicitly load a version if there is more than one: [exouser@rocky-lmod-host ~]$ module load R/4.1.2 See what modules are loaded \u00b6 You can use module list to show what\u2019s loaded: [exouser@rocky-lmod-host ~]$ module list Currently Loaded Modules: 1) R/4.1.2 2) intel-OneAPI/2022.1.2 Saving your loaded software from the collection \u00b6 You can save the current loaded modules so it will automatically load every time you log in to your virtual machine by using module save . This will save to a file called default in and ~/.lmod.d directory that Lmod will create the first time you save your loaded module list. You\u2019ll need to do this on any new virtual machine. Unloading software from the collection \u00b6 You can also unload any loaded modules by using module unload [exouser@rocky-lmod-host ~]$ module list Currently Loaded Modules: 1) R/4.1.2 2) intel-OneAPI/2022.1.2 [exouser@rocky-lmod-host ~]$ module unload R [exouser@rocky-lmod-host ~]$ module list Currently Loaded Modules: 1) intel-OneAPI/2022.1.2 [exouser@rocky-lmod-host ~]$ Manually adding the software collection to a non-featured virtual machine \u00b6 This is an advanced topic for researchers and educators that are more comfortable with the system administration aspects of Linux. We generally advise using the Featured Images that have the mounts and Lmod already configured for you. Do the following as the root user: Create the file /etc/ceph.js2softwarero.secret with the contents AQAYuStiw81/ABAAsvzI5h53WOC5K1vzmGB66g== Make the mount point you want to use (we highly suggest it be /software ) mkdir /software Add the following entry to the end of your /etc/fstab (make sure the mountpoint you created in the last step matches) # JS2 Software Collection mount 149.165.158.38:6789,149.165.158.22:6789,149.165.158.54:6789,149.165.158.70:6789,149.165.158.86:6789:/volumes/_nogroup/b7112570-f7cb-4bd2-8c0e-39b08609b9fd/01aa9d72-69bf-4250-9245-2eaddcdb251d /software ceph name=js2softwarero,secretfile=/etc/ceph.js2softwarero.secret,x-systemd.device-timeout=31,x-systemd.mount-timeout=30,noatime,_netdev,ro 0 Then mount the filesystem with mount -a If you then run a df -h|grep vol you should see something like this: 149.165.158.38:6789,149.165.158.22:6789,149.165.158.54:6789,149.165.158.70:6789,149.165.158.86:6789:/volumes/_nogroup/fe4f8ad4-2877-4e23-b5d3-46eb8476750b/ab404bac-9584-45f4-8a34-92dfc61fbb98 1.8T 134G 1.7T 2% /mnt/ceph You\u2019ll also need to install Lmod on your VM. The major distributions include this either in default software repositories or in the Powertools repo for CentOS/Alma/Rocky8. You\u2019ll need to point your modulepath config to: /software/xx/modulefiles replacing the xx with u20 , u18 , r8 , or c7 depending on the Linux variant you\u2019re using.","title":"Using the Jetstream2 Software Collection"},{"location":"general/usingsoftware/#using-the-jetstream2-software-collection","text":"Jetstream2 utilizes Lmod to load and unload software you may need in your research or education pursuits. The software is served from a shared store utilizing Manila shares that are mounted automatically if you\u2019re using a Jetstream2 Featured Image. All of the instructions on listing, loading, and unloading modules/software packages assume you are using a Jetstream2 featured image created after April 9, 2022. Additional information on using Lmod is available in the User Guide for Lmod","title":"Using the Jetstream2 Software Collection"},{"location":"general/usingsoftware/#listing-available-software-from-the-collection","text":"You can use module avail to show available software packages. This may show multiple versions of the same package in the future, which will default to the latest version but allow you to load previous versions. [exouser@rocky-lmod-host ~]$ module avail ---------------- /software/r8/modulefiles ------------------ R/4.1.2 (L) intel-OneAPI/2022.1.2 matlab/R2021a ---------------- /usr/share/lmod/lmod/modulefiles/Core ----- lmod settarg Where: L: Module is loaded","title":"Listing available software from the collection"},{"location":"general/usingsoftware/#loading-software-from-the-collection","text":"You can load software from the collection using module load . [exouser@rocky-lmod-host ~]$ module load R You can also explicitly load a version if there is more than one: [exouser@rocky-lmod-host ~]$ module load R/4.1.2","title":"Loading software from the collection"},{"location":"general/usingsoftware/#see-what-modules-are-loaded","text":"You can use module list to show what\u2019s loaded: [exouser@rocky-lmod-host ~]$ module list Currently Loaded Modules: 1) R/4.1.2 2) intel-OneAPI/2022.1.2","title":"See what modules are loaded"},{"location":"general/usingsoftware/#saving-your-loaded-software-from-the-collection","text":"You can save the current loaded modules so it will automatically load every time you log in to your virtual machine by using module save . This will save to a file called default in and ~/.lmod.d directory that Lmod will create the first time you save your loaded module list. You\u2019ll need to do this on any new virtual machine.","title":"Saving your loaded software from the collection"},{"location":"general/usingsoftware/#unloading-software-from-the-collection","text":"You can also unload any loaded modules by using module unload [exouser@rocky-lmod-host ~]$ module list Currently Loaded Modules: 1) R/4.1.2 2) intel-OneAPI/2022.1.2 [exouser@rocky-lmod-host ~]$ module unload R [exouser@rocky-lmod-host ~]$ module list Currently Loaded Modules: 1) intel-OneAPI/2022.1.2 [exouser@rocky-lmod-host ~]$","title":"Unloading software from the collection"},{"location":"general/usingsoftware/#manually-adding-the-software-collection-to-a-non-featured-virtual-machine","text":"This is an advanced topic for researchers and educators that are more comfortable with the system administration aspects of Linux. We generally advise using the Featured Images that have the mounts and Lmod already configured for you. Do the following as the root user: Create the file /etc/ceph.js2softwarero.secret with the contents AQAYuStiw81/ABAAsvzI5h53WOC5K1vzmGB66g== Make the mount point you want to use (we highly suggest it be /software ) mkdir /software Add the following entry to the end of your /etc/fstab (make sure the mountpoint you created in the last step matches) # JS2 Software Collection mount 149.165.158.38:6789,149.165.158.22:6789,149.165.158.54:6789,149.165.158.70:6789,149.165.158.86:6789:/volumes/_nogroup/b7112570-f7cb-4bd2-8c0e-39b08609b9fd/01aa9d72-69bf-4250-9245-2eaddcdb251d /software ceph name=js2softwarero,secretfile=/etc/ceph.js2softwarero.secret,x-systemd.device-timeout=31,x-systemd.mount-timeout=30,noatime,_netdev,ro 0 Then mount the filesystem with mount -a If you then run a df -h|grep vol you should see something like this: 149.165.158.38:6789,149.165.158.22:6789,149.165.158.54:6789,149.165.158.70:6789,149.165.158.86:6789:/volumes/_nogroup/fe4f8ad4-2877-4e23-b5d3-46eb8476750b/ab404bac-9584-45f4-8a34-92dfc61fbb98 1.8T 134G 1.7T 2% /mnt/ceph You\u2019ll also need to install Lmod on your VM. The major distributions include this either in default software repositories or in the Powertools repo for CentOS/Alma/Rocky8. You\u2019ll need to point your modulepath config to: /software/xx/modulefiles replacing the xx with u20 , u18 , r8 , or c7 depending on the Linux variant you\u2019re using.","title":"Manually adding the software collection to a non-featured virtual machine"},{"location":"general/virtualclusters/","text":"Virtual Clusters on Jetstream2 \u00b6 A Virtual Cluster is a computer cluster that is made up of a single head node which runs the Slurm system, as well as a number of compute nodes. The head node is responsible for controlling the compute nodes. A large filesystem can be shared between all of the nodes. The head node is accessible to users via a public IP address and the compute nodes are created on-demand as jobs are submitted to the queue. Slurm is a resource management system that helps organize and manage computer resources. It allocates resources to different tasks or jobs, making sure that everything runs as smoothly as possible. Options to create your own virtual cluster on Jetstream2 \u00b6 Exosphere\u2019s push-button cluster feature (easy, less configurable) On the command-line, running shell scripts (trickier, more flexible). See the XCRI Toolkit Docs for more information. References \u00b6 Virtual Clusters in the Jetstream Cloud: A story of elasticized HPC Presentation on Virtual Clusters","title":"Virtual Clusters on Jetstream2"},{"location":"general/virtualclusters/#virtual-clusters-on-jetstream2","text":"A Virtual Cluster is a computer cluster that is made up of a single head node which runs the Slurm system, as well as a number of compute nodes. The head node is responsible for controlling the compute nodes. A large filesystem can be shared between all of the nodes. The head node is accessible to users via a public IP address and the compute nodes are created on-demand as jobs are submitted to the queue. Slurm is a resource management system that helps organize and manage computer resources. It allocates resources to different tasks or jobs, making sure that everything runs as smoothly as possible.","title":"Virtual Clusters on Jetstream2"},{"location":"general/virtualclusters/#options-to-create-your-own-virtual-cluster-on-jetstream2","text":"Exosphere\u2019s push-button cluster feature (easy, less configurable) On the command-line, running shell scripts (trickier, more flexible). See the XCRI Toolkit Docs for more information.","title":"Options to create your own virtual cluster on Jetstream2"},{"location":"general/virtualclusters/#references","text":"Virtual Clusters in the Jetstream Cloud: A story of elasticized HPC Presentation on Virtual Clusters","title":"References"},{"location":"general/vmsizes/","text":"Virtual Machine Sizes and Configurations \u00b6 Jetstream2 can be used in several different virtual machine (VM) sizes which are charged in service units (SUs) based on how much of the total system resource is used. The basic unit of VM allocation for Jetstream is based on a virtual CPU (vCPU) hour: 1 service unit (SU) is equivalent to 1 vCPU for 1 hour of wall clock time . The table below outlines the VM sizes created for Jetstream2. Please note that these are all separate resources. Jetstream2 CPU is the default resource. To use Large Memory or GPU resources, you must have an allocation for those resources. While the root disk sizes here are fixed, there is an option called \u201cboot from volume\u201d that will allow you to specify a larger root disk using quota from your storage allocation. Instructions for that are in the user interface sections. Jetstream2 CPU \u00b6 VM Size vCPUs RAM (GB) Local Storage (GB) SU cost per hour m3.tiny 1 3 20 1 m3.small 2 6 20 2 m3.quad 4 15 20 4 m3.medium 8 30 60 8 m3.large 16 60 60 16 m3.xl 32 125 60 32 m3.2xl 64 250 60 64 m3.3xl* 128 500 60 128 * m3.3xl will not be available by default. It will only be available by request and with proper justification Jetstream2 Large Memory \u00b6 Jetstream2 Large Memory nodes charge 2 SUs per vCPU hour or 2 SUs per core per hour. VM Size vCPUs RAM (GB) Local Storage (GB) SU cost per hour r3.large 64 500 60 128 r3.xl 128 1000 60 256 Jetstream2 GPU \u00b6 Jetstream2 GPU nodes charge 4 SUs per vCPU hour or 4 SUs per core per hour. Additionally, there are four NVIDIA A100 GPUs on each node. These GPUs are subdivided using NVIDIA virtual GPU (vGPU) into up to 5 slices to allow more researchers and students to make use of the GPU resource. 5 GPU slices = 1 NVIDIA 40GB Ampere A100 GPU VM Size vCPUs RAM(GB) Local Storage (GB) GPU Slices/GPU Ram SU cost / hour g3.small 4 15 60 1 slice / 5gb RAM 16 g3.medium 8 30 60 2 slices / 10gb RAM 32 g3.large 16 60 60 4 slices / 20gb RAM 64 g3.xl 32 125 60 5 slices / 40gb RAM 128 This flavor information may be subject to changes in the future. Example of SU estimation: \u00b6 First determine the VM resource appropriate to your needs (CPU only, large memory, GPU): If your work requires 24 GB of RAM and 60 GB of local storage: you would request 8 SUs per hour to cover a single m3.medium VM instance. If your work requires 10 GB of local storage in 1 core using 3 GB of RAM: you would request 2 SUs per hour for an m3.small VM instance. If your work requires 1TB of RAM: you would request 256 SUs per hour for an r3.xl instance on Jetstream Large Memory If you work requires 20gb of GPU RAM: you would request 64 SUs per hour for a g3.large instance on Jetstream GPU You then would calculate for the appropriate resource (refer to the tables above): For Jetstream2 CPU, you would then multiply by the number of hours you will use that size VM in the next year and multiply by the number of VMs you will need. For Jetstream2 Large Memory and GPU, either refer to the SU cost per hour in the last column, or multiply hours times 2 for LM or 4 for GPU To calculate the number of SUs you will need in the next year, first estimate the number of hours you expect to work on a particular project. For example, if you typically work 40 hours per week and expect to spend 25% of your time on this project that would be 10 hours per week. Next, calculate the total number of hours per year for this project: Total hours = 10 hours per week * 52 weeks per year Total hours = 520 Finally, calculate the total SUs for the year for a single VM instance: Total SUs = 520 hours per year * vCPUs e.g. For a Medium VM instance: Total SUs = 520 hours per year * 8vCPUs Total SUs = 4160 If your project requires more than 1 VM instance, multiply the total SUs by the number of VMs that you will need: Total SUs needed for 3 medium size VMs = 3 * 4160 Total SUs = 12480 The calculations above assume that your VM is shutdown properly. For instructions see: Cacao instance management actions Exosphere instance management actions Horizon instance management actions Command line instance management actions SU Estimation for Infrastructure or \u201cAlways On\u201d allocations \u00b6 For jobs that may need to run for extended periods or as \u201calways on\u201d infrastructure, you can take this approach: VM cost (SUs) x 24 hours/day x 365 days = single VM cost per year or as an example for each resource, an m3.large, r3.large, and g3.large each running for a year: m3.large (16 cores) x 24 hours/day x 365 days = 140,160 SUs r3.large (64 cores x 2 SUs/hour) x 24 hours/day x 365 days = 1,121,280 SUs g3.large (16 cores x 4 SUs/hour) x 24 hours/day x 365 days = 560,640 SUs Startup allocations are subject to limits. Please refer to Jetstream2 Resources for Startup Allocation limits. For information on submitting a Research Allocation Request, please see https://portal.xsede.org/successful-requests. Note that all allocations above the startup level on Jetstream2 CPU require a strong justification for the time being requested.","title":"VM Sizes and Configurations"},{"location":"general/vmsizes/#virtual-machine-sizes-and-configurations","text":"Jetstream2 can be used in several different virtual machine (VM) sizes which are charged in service units (SUs) based on how much of the total system resource is used. The basic unit of VM allocation for Jetstream is based on a virtual CPU (vCPU) hour: 1 service unit (SU) is equivalent to 1 vCPU for 1 hour of wall clock time . The table below outlines the VM sizes created for Jetstream2. Please note that these are all separate resources. Jetstream2 CPU is the default resource. To use Large Memory or GPU resources, you must have an allocation for those resources. While the root disk sizes here are fixed, there is an option called \u201cboot from volume\u201d that will allow you to specify a larger root disk using quota from your storage allocation. Instructions for that are in the user interface sections.","title":"Virtual Machine Sizes and Configurations"},{"location":"general/vmsizes/#jetstream2-cpu","text":"VM Size vCPUs RAM (GB) Local Storage (GB) SU cost per hour m3.tiny 1 3 20 1 m3.small 2 6 20 2 m3.quad 4 15 20 4 m3.medium 8 30 60 8 m3.large 16 60 60 16 m3.xl 32 125 60 32 m3.2xl 64 250 60 64 m3.3xl* 128 500 60 128 * m3.3xl will not be available by default. It will only be available by request and with proper justification","title":"Jetstream2 CPU"},{"location":"general/vmsizes/#jetstream2-large-memory","text":"Jetstream2 Large Memory nodes charge 2 SUs per vCPU hour or 2 SUs per core per hour. VM Size vCPUs RAM (GB) Local Storage (GB) SU cost per hour r3.large 64 500 60 128 r3.xl 128 1000 60 256","title":"Jetstream2 Large Memory"},{"location":"general/vmsizes/#jetstream2-gpu","text":"Jetstream2 GPU nodes charge 4 SUs per vCPU hour or 4 SUs per core per hour. Additionally, there are four NVIDIA A100 GPUs on each node. These GPUs are subdivided using NVIDIA virtual GPU (vGPU) into up to 5 slices to allow more researchers and students to make use of the GPU resource. 5 GPU slices = 1 NVIDIA 40GB Ampere A100 GPU VM Size vCPUs RAM(GB) Local Storage (GB) GPU Slices/GPU Ram SU cost / hour g3.small 4 15 60 1 slice / 5gb RAM 16 g3.medium 8 30 60 2 slices / 10gb RAM 32 g3.large 16 60 60 4 slices / 20gb RAM 64 g3.xl 32 125 60 5 slices / 40gb RAM 128 This flavor information may be subject to changes in the future.","title":"Jetstream2 GPU"},{"location":"general/vmsizes/#example-of-su-estimation","text":"First determine the VM resource appropriate to your needs (CPU only, large memory, GPU): If your work requires 24 GB of RAM and 60 GB of local storage: you would request 8 SUs per hour to cover a single m3.medium VM instance. If your work requires 10 GB of local storage in 1 core using 3 GB of RAM: you would request 2 SUs per hour for an m3.small VM instance. If your work requires 1TB of RAM: you would request 256 SUs per hour for an r3.xl instance on Jetstream Large Memory If you work requires 20gb of GPU RAM: you would request 64 SUs per hour for a g3.large instance on Jetstream GPU You then would calculate for the appropriate resource (refer to the tables above): For Jetstream2 CPU, you would then multiply by the number of hours you will use that size VM in the next year and multiply by the number of VMs you will need. For Jetstream2 Large Memory and GPU, either refer to the SU cost per hour in the last column, or multiply hours times 2 for LM or 4 for GPU To calculate the number of SUs you will need in the next year, first estimate the number of hours you expect to work on a particular project. For example, if you typically work 40 hours per week and expect to spend 25% of your time on this project that would be 10 hours per week. Next, calculate the total number of hours per year for this project: Total hours = 10 hours per week * 52 weeks per year Total hours = 520 Finally, calculate the total SUs for the year for a single VM instance: Total SUs = 520 hours per year * vCPUs e.g. For a Medium VM instance: Total SUs = 520 hours per year * 8vCPUs Total SUs = 4160 If your project requires more than 1 VM instance, multiply the total SUs by the number of VMs that you will need: Total SUs needed for 3 medium size VMs = 3 * 4160 Total SUs = 12480 The calculations above assume that your VM is shutdown properly. For instructions see: Cacao instance management actions Exosphere instance management actions Horizon instance management actions Command line instance management actions","title":"Example of SU estimation:"},{"location":"general/vmsizes/#su-estimation-for-infrastructure-or-always-on-allocations","text":"For jobs that may need to run for extended periods or as \u201calways on\u201d infrastructure, you can take this approach: VM cost (SUs) x 24 hours/day x 365 days = single VM cost per year or as an example for each resource, an m3.large, r3.large, and g3.large each running for a year: m3.large (16 cores) x 24 hours/day x 365 days = 140,160 SUs r3.large (64 cores x 2 SUs/hour) x 24 hours/day x 365 days = 1,121,280 SUs g3.large (16 cores x 4 SUs/hour) x 24 hours/day x 365 days = 560,640 SUs Startup allocations are subject to limits. Please refer to Jetstream2 Resources for Startup Allocation limits. For information on submitting a Research Allocation Request, please see https://portal.xsede.org/successful-requests. Note that all allocations above the startup level on Jetstream2 CPU require a strong justification for the time being requested.","title":"SU Estimation for Infrastructure or \"Always On\" allocations"},{"location":"general/windows/","text":"Microsoft Windows on Jetstream2 \u00b6 Microsoft Windows is not officially supported on Jetstream2. We will be making a limited number of images available for experimental use. While these images will be created by one of the Jetstream2 parters in conjunction with Jetstream2 staff, it is NOT supported nor guaranteed to work. It is unlikely that the Jetstream2 staff will be able to do more than general VM troubleshooting as we do not have Windows expertise on staff. No license will be installed so you will need to bring your own license. It is not known at this time whether GPUs will work on Microsoft-based instances. We will test this as time permits.","title":"Microsoft Windows on Jetstream2"},{"location":"general/windows/#microsoft-windows-on-jetstream2","text":"Microsoft Windows is not officially supported on Jetstream2. We will be making a limited number of images available for experimental use. While these images will be created by one of the Jetstream2 parters in conjunction with Jetstream2 staff, it is NOT supported nor guaranteed to work. It is unlikely that the Jetstream2 staff will be able to do more than general VM troubleshooting as we do not have Windows expertise on staff. No license will be installed so you will need to bring your own license. It is not known at this time whether GPUs will work on Microsoft-based instances. We will test this as time permits.","title":"Microsoft Windows on Jetstream2"},{"location":"general/xsede/","text":"XSEDE Service Units and Jetstream2 \u00b6 Jetstream2 allocations are measured in XSEDE Service Units (SUs). SUs are consumed at a rate of: Jetstream2 (CPU) - 1 SU per vCPU_core-hour (use of one virtual core of a CPU per hour). Jetstream2-LM (Large Memory) - 2 SUs per vCPU_core-hour Jetstream2-GPU - 4 SUs per vCPU_core-hour Please refer to VM Sizes and configurations to see available VM flavors and per hour cost on Jetstream2. SUSPENDED instances will be charged .75 of their normal SU value. (75%) STOPPED instances will be charge 0.50 of their normal SU value. (50%) SHELVED instances will not be charged SUs. (0%) For Large Memory and GPU allocations, the vCPU core hour cost is 2x and 4x respectively as noted above. The reason for continuing to charge for VMs that are not in a usable state is that they still consume resorces if they are suspended or stopped. In those states, they still occupy allocable/usable space on the hypervisor, preventing other users from using those resources. For instructions on managing instances, please see: Cacao instance management actions Exosphere instance management actions Horizon instance management actions Command line instance management actions","title":"XSEDE Service Units and Jetstream2"},{"location":"general/xsede/#xsede-service-units-and-jetstream2","text":"Jetstream2 allocations are measured in XSEDE Service Units (SUs). SUs are consumed at a rate of: Jetstream2 (CPU) - 1 SU per vCPU_core-hour (use of one virtual core of a CPU per hour). Jetstream2-LM (Large Memory) - 2 SUs per vCPU_core-hour Jetstream2-GPU - 4 SUs per vCPU_core-hour Please refer to VM Sizes and configurations to see available VM flavors and per hour cost on Jetstream2. SUSPENDED instances will be charged .75 of their normal SU value. (75%) STOPPED instances will be charge 0.50 of their normal SU value. (50%) SHELVED instances will not be charged SUs. (0%) For Large Memory and GPU allocations, the vCPU core hour cost is 2x and 4x respectively as noted above. The reason for continuing to charge for VMs that are not in a usable state is that they still consume resorces if they are suspended or stopped. In those states, they still occupy allocable/usable space on the hypervisor, preventing other users from using those resources. For instructions on managing instances, please see: Cacao instance management actions Exosphere instance management actions Horizon instance management actions Command line instance management actions","title":"XSEDE Service Units and Jetstream2"},{"location":"migration/how_to_migrate/","text":"How to Migrate \u00b6 Overview \u00b6 In order to migrate from Jetstream1 (JS1) to Jetstream2 (JS2) you\u2019ll need to understand: Allocations How to move data Where to find help Allocations \u00b6 Just as in JS1, you\u2019ll need an allocation of compute ( normal , large memory , GPU ) and storage resources, awarded by NSF and currently managed by XSEDE . The allocation process is described here: Allocation Overivew You can take one of two approaches: A supplement to a current research allocation -OR- A completely new allocation Supplement Allocation \u00b6 If you already have a JS1 allocation, then we encourage you to request a supplement at a STARTUP amount of resources for the compute , large-memory-compute , GPU , and storage resources on JS2. This will enable you to create the Code, Performance, & Scaling estimates you\u2019ll need for a successful project renewal on JS2. Supplement Renewal New Allocation \u00b6 Of course, if your research will be changing, you can always apply for a new allocation of resources on JS2. We recommend you still start with a Startup level in order to create the Code, Performance, & Scaling estimates you\u2019ll need later for a Research allocation. Allocation Overview Startup Allocation How to move your data \u00b6 If you\u2019ve already been using JS1, you\u2019ll likely wish to retain your old VMs and data, found within both VMs and in external volumes. Below, we provide instructions for moving your VMs. As time permits, we anticipate the development of tools to help facilitate and partially automate migration. Volumes on JS1 do not persist on JS2 because they are completely separate systems and do not share storage. Three approaches \u00b6 There are essentially three approaches to accomplish data retention: Recreate your work In order to get you going the fastest, take advantage of all the new features of JS2, and avoid any legacy configuration differences, it\u2019s often advisable to simply create new VMs and bring in fresh software and data. Information about creating new VMs can be found for each type of user interface: General Instance Management Instructions for tansfering files from external locations to JS2 VMs can be found here: File Transfer Copy your JS1 work Similar to recreating your work, you can save some steps after starting new VMs by copying your existing software from your current VM on JS1: File Transfer CAUTION::JS1-compatibility Network configurations and any instance management tools and scripts you\u2019ve used previously will likely require updating to current values appropriate for JS2. INFO::transfer speeds Copying data from JS1 to JS2, particularly from within the same regional provider, will generally have good performance relative to a transfers across the internet. Transfer your work You can create snapshots of your existing JS1 VMs and request the Help Desk Support team copy these snapshots as well as data volumes to Jetstream2. Data volumes : This is fairly straightforward and is described below: How to preserve JS1 VMs and data VMs : You can also follow the steps at How to preserve JS1 VMs and data CAUTION::JS1-compatibility Please be aware that configurational differences between JS1 and JS2 generally prevent straight forward re-deployment of a JS1 VM on JS2. It may be more advisable to transfer the VM and mount the snapshot as a external volume on a new JS2 VM. How to preserve Jetstream1 VMs and data \u00b6 Identify if your VM or volume used the Atmosphere or API/CLI interface Atmosphere Atmosphere : VMs: Follow the instructions here to update and image your VM NOTE : It is strongly recommended that users upgrade the operating system before imaging. For CentOS based systems, it\u2019s sudo yum update For Ubuntu based systems, do sudo apt-get update and then sudo apt-get upgrade Next Go to the IMAGES tab Click on the Image you created Click on the Version you want Click on Copy for either IU or TACC to grab the UUID of the image Submit a ticket with that UUID to help@jetstream-cloud.org to ask staff to copy your image to Jetstream2. Volumes: Click on the PROJECTS tab Click on the Project Folder Scroll down to VOLUMES and click on the desired volume Click on Copy to grab the UUID of the image. Submit a ticket with that VUID to help@jetstream-cloud.org to ask staff to copy your volume to Jetstream2. API API : VMs: Follow the instructions here to create a snapshot of your instance Make note of the UUID Submit a ticket with that UUID to help@jetstream-cloud.org to ask staff to copy your image to Jetstream2. Volumes: Use the command: openstack volume list Note the Volume UID Submit a ticket with that VUID to help@jetstream-cloud.org to ask staff to copy your image to Jetstream2. Where to find help documentation \u00b6 JS1 : Documentation for JS1 will temporarily remain during the summer of 2022 at https://wiki.jetstream-cloud.org PLEASE NOTE that this documentation will likely NOT be updated. JS2 : The latest JS2 documentation will be maintained at docs.jetsteam-cloud.org NEWS : Announcements will be made at the XSEDE User News : https://www.xsede.org/news/user-news site To ensure that you are receiving all updates to your inbox, login to the page to manage your XSEDE User News subscriptions. All Jetstream users should be added when they are placed on an allocation, but you may wish to verify so that you don\u2019t miss any important migration updates. Updates will all be made Jetstream2 System Status and Information and shared via XSEDE User News : https://www.xsede.org/news/user-news site .","title":"HOW-TO"},{"location":"migration/how_to_migrate/#how-to-migrate","text":"","title":"How to Migrate"},{"location":"migration/how_to_migrate/#overview","text":"In order to migrate from Jetstream1 (JS1) to Jetstream2 (JS2) you\u2019ll need to understand: Allocations How to move data Where to find help","title":"Overview"},{"location":"migration/how_to_migrate/#allocations","text":"Just as in JS1, you\u2019ll need an allocation of compute ( normal , large memory , GPU ) and storage resources, awarded by NSF and currently managed by XSEDE . The allocation process is described here: Allocation Overivew You can take one of two approaches: A supplement to a current research allocation -OR- A completely new allocation","title":"Allocations "},{"location":"migration/how_to_migrate/#supplement-allocation","text":"If you already have a JS1 allocation, then we encourage you to request a supplement at a STARTUP amount of resources for the compute , large-memory-compute , GPU , and storage resources on JS2. This will enable you to create the Code, Performance, & Scaling estimates you\u2019ll need for a successful project renewal on JS2. Supplement Renewal","title":"Supplement Allocation "},{"location":"migration/how_to_migrate/#new-allocation","text":"Of course, if your research will be changing, you can always apply for a new allocation of resources on JS2. We recommend you still start with a Startup level in order to create the Code, Performance, & Scaling estimates you\u2019ll need later for a Research allocation. Allocation Overview Startup Allocation","title":"New Allocation "},{"location":"migration/how_to_migrate/#how-to-move-your-data","text":"If you\u2019ve already been using JS1, you\u2019ll likely wish to retain your old VMs and data, found within both VMs and in external volumes. Below, we provide instructions for moving your VMs. As time permits, we anticipate the development of tools to help facilitate and partially automate migration. Volumes on JS1 do not persist on JS2 because they are completely separate systems and do not share storage.","title":"How to move your data "},{"location":"migration/how_to_migrate/#three-approaches","text":"There are essentially three approaches to accomplish data retention: Recreate your work In order to get you going the fastest, take advantage of all the new features of JS2, and avoid any legacy configuration differences, it\u2019s often advisable to simply create new VMs and bring in fresh software and data. Information about creating new VMs can be found for each type of user interface: General Instance Management Instructions for tansfering files from external locations to JS2 VMs can be found here: File Transfer Copy your JS1 work Similar to recreating your work, you can save some steps after starting new VMs by copying your existing software from your current VM on JS1: File Transfer CAUTION::JS1-compatibility Network configurations and any instance management tools and scripts you\u2019ve used previously will likely require updating to current values appropriate for JS2. INFO::transfer speeds Copying data from JS1 to JS2, particularly from within the same regional provider, will generally have good performance relative to a transfers across the internet. Transfer your work You can create snapshots of your existing JS1 VMs and request the Help Desk Support team copy these snapshots as well as data volumes to Jetstream2. Data volumes : This is fairly straightforward and is described below: How to preserve JS1 VMs and data VMs : You can also follow the steps at How to preserve JS1 VMs and data CAUTION::JS1-compatibility Please be aware that configurational differences between JS1 and JS2 generally prevent straight forward re-deployment of a JS1 VM on JS2. It may be more advisable to transfer the VM and mount the snapshot as a external volume on a new JS2 VM.","title":"Three approaches"},{"location":"migration/how_to_migrate/#how-to-preserve-jetstream1-vms-and-data","text":"Identify if your VM or volume used the Atmosphere or API/CLI interface Atmosphere Atmosphere : VMs: Follow the instructions here to update and image your VM NOTE : It is strongly recommended that users upgrade the operating system before imaging. For CentOS based systems, it\u2019s sudo yum update For Ubuntu based systems, do sudo apt-get update and then sudo apt-get upgrade Next Go to the IMAGES tab Click on the Image you created Click on the Version you want Click on Copy for either IU or TACC to grab the UUID of the image Submit a ticket with that UUID to help@jetstream-cloud.org to ask staff to copy your image to Jetstream2. Volumes: Click on the PROJECTS tab Click on the Project Folder Scroll down to VOLUMES and click on the desired volume Click on Copy to grab the UUID of the image. Submit a ticket with that VUID to help@jetstream-cloud.org to ask staff to copy your volume to Jetstream2. API API : VMs: Follow the instructions here to create a snapshot of your instance Make note of the UUID Submit a ticket with that UUID to help@jetstream-cloud.org to ask staff to copy your image to Jetstream2. Volumes: Use the command: openstack volume list Note the Volume UID Submit a ticket with that VUID to help@jetstream-cloud.org to ask staff to copy your image to Jetstream2.","title":"How to preserve Jetstream1 VMs and data "},{"location":"migration/how_to_migrate/#where-to-find-help-documentation","text":"JS1 : Documentation for JS1 will temporarily remain during the summer of 2022 at https://wiki.jetstream-cloud.org PLEASE NOTE that this documentation will likely NOT be updated. JS2 : The latest JS2 documentation will be maintained at docs.jetsteam-cloud.org NEWS : Announcements will be made at the XSEDE User News : https://www.xsede.org/news/user-news site To ensure that you are receiving all updates to your inbox, login to the page to manage your XSEDE User News subscriptions. All Jetstream users should be added when they are placed on an allocation, but you may wish to verify so that you don\u2019t miss any important migration updates. Updates will all be made Jetstream2 System Status and Information and shared via XSEDE User News : https://www.xsede.org/news/user-news site .","title":"Where to find help documentation "},{"location":"migration/migration_overview/","text":"Migrating to Jetstream2 from Jetstream1 \u00b6 Jetstream 2 will support and encourages the migration of research from Jetstream1 (JS1) to the superior capabilities of Jetstream2 (JS2). In this section, we document information relative to that migration. Jetstream 1 & 2 availability \u00b6 Jetstream1 will remain online through the end of July 2022. We are no longer accepting any new allocations on Jetstream1 as Jetstream2 has entered operations phase. There will be no extension beyond July 31 for Jetstream1. All allocations will be disabled and denied access at that time. We strongly recommend users migrate to JS2 as soon as possible. VMs on JS1 will continue running as long as you have a valid allocation there BUT only until it is decommissioned. The TACC cloud of JS1 will likely be taken down in stages prior to the IU cloud. What Researchers should know before migrating \u00b6 Researchers on JS1 wishing to migrate to JS2 should take note of: Critical differences between JS2 and JS1 for: Architecture User interfaces Authentication Software Collection How to migrate from JS1 to JS2 Allocations How to move data Where to find help Critical differences JS2 and JS1 \u00b6 Architecture \u00b6 We highly recommend that you read about the key differences between the architecture of JS2 and JS1 . The biggest difference is that Jetstream2 will consist of one primary cloud (hosted at Indiana University) and multiple regional clouds. User Interfaces (UI) \u00b6 Jetstream2 has 3 Graphical User Interfaces and a Command Line Interface (CLI). User Interfaces will feel familiar, particularly if you\u2019ve tried the API side of JS1 before as we continue to have Exosphere, Horizon, and a Command Line Interface For our Atmosphere users, we are recommending Exosphere as it gives much of the power of Horizon while maintaining the simplicity of Atmosphere. Exosphere: https://docs.jetstream-cloud.org/ui/exo/exo/ Horizon : https://docs.jetstream-cloud.org/ui/horizon/intro/ CLI : https://docs.jetstream-cloud.org/ui/cli/overview/ Cacao (aka Atmsophere2) : https://docs.jetstream-cloud.org/ui/cacao/overview/ Note: Cacao is still in development on JS2 and not in its final production state. Application Credentials \u00b6 One important differences to note for our existing users is that we\u2019ve enhanced our security profile and are using Application Credentials for our interfaces. Each user interface above has similar, but slightly different ways to access those Application Credentials , so please be aware, and carefully follow the instructions. Important note for CLI users Jetstream2 use of Application Credentials has expanded to include CLI access. This means CLI users well also authenticate using XSEDE credentials. Please see Setting up the openrc.sh for the Jetstream2 CLI for instructions generating application credentials and an openrc for Jetstream2. Jetstream2 Software Collection \u00b6 Rather than pack each and every virtual machine with research software, Jetstream2 has created a shared directory of packages that you can load and unload as desired using the LMOD system. The use of the JS2 Software Collection is described here: https://docs.jetstream-cloud.org/general/usingsoftware/#using-the-jetstream2-software-collection","title":"Overview"},{"location":"migration/migration_overview/#migrating-to-jetstream2-from-jetstream1","text":"Jetstream 2 will support and encourages the migration of research from Jetstream1 (JS1) to the superior capabilities of Jetstream2 (JS2). In this section, we document information relative to that migration.","title":"Migrating to Jetstream2 from Jetstream1"},{"location":"migration/migration_overview/#jetstream-1-2-availability","text":"Jetstream1 will remain online through the end of July 2022. We are no longer accepting any new allocations on Jetstream1 as Jetstream2 has entered operations phase. There will be no extension beyond July 31 for Jetstream1. All allocations will be disabled and denied access at that time. We strongly recommend users migrate to JS2 as soon as possible. VMs on JS1 will continue running as long as you have a valid allocation there BUT only until it is decommissioned. The TACC cloud of JS1 will likely be taken down in stages prior to the IU cloud.","title":"Jetstream 1 &amp; 2 availability"},{"location":"migration/migration_overview/#what-researchers-should-know-before-migrating","text":"Researchers on JS1 wishing to migrate to JS2 should take note of: Critical differences between JS2 and JS1 for: Architecture User interfaces Authentication Software Collection How to migrate from JS1 to JS2 Allocations How to move data Where to find help","title":"What Researchers should know before migrating"},{"location":"migration/migration_overview/#critical-differences-js2-and-js1","text":"","title":"Critical differences JS2 and JS1 "},{"location":"migration/migration_overview/#architecture","text":"We highly recommend that you read about the key differences between the architecture of JS2 and JS1 . The biggest difference is that Jetstream2 will consist of one primary cloud (hosted at Indiana University) and multiple regional clouds.","title":"Architecture "},{"location":"migration/migration_overview/#user-interfaces-ui","text":"Jetstream2 has 3 Graphical User Interfaces and a Command Line Interface (CLI). User Interfaces will feel familiar, particularly if you\u2019ve tried the API side of JS1 before as we continue to have Exosphere, Horizon, and a Command Line Interface For our Atmosphere users, we are recommending Exosphere as it gives much of the power of Horizon while maintaining the simplicity of Atmosphere. Exosphere: https://docs.jetstream-cloud.org/ui/exo/exo/ Horizon : https://docs.jetstream-cloud.org/ui/horizon/intro/ CLI : https://docs.jetstream-cloud.org/ui/cli/overview/ Cacao (aka Atmsophere2) : https://docs.jetstream-cloud.org/ui/cacao/overview/ Note: Cacao is still in development on JS2 and not in its final production state.","title":"User Interfaces (UI)"},{"location":"migration/migration_overview/#application-credentials","text":"One important differences to note for our existing users is that we\u2019ve enhanced our security profile and are using Application Credentials for our interfaces. Each user interface above has similar, but slightly different ways to access those Application Credentials , so please be aware, and carefully follow the instructions. Important note for CLI users Jetstream2 use of Application Credentials has expanded to include CLI access. This means CLI users well also authenticate using XSEDE credentials. Please see Setting up the openrc.sh for the Jetstream2 CLI for instructions generating application credentials and an openrc for Jetstream2.","title":"Application Credentials"},{"location":"migration/migration_overview/#jetstream2-software-collection","text":"Rather than pack each and every virtual machine with research software, Jetstream2 has created a shared directory of packages that you can load and unload as desired using the LMOD system. The use of the JS2 Software Collection is described here: https://docs.jetstream-cloud.org/general/usingsoftware/#using-the-jetstream2-software-collection","title":"Jetstream2 Software Collection"},{"location":"overview/config/","text":"Configuration and specifications \u00b6 Primary Cloud Specifications \u00b6 Compute Nodes (384 nodes) \u00b6 System Configuration Aggregate information Per Node (Compute Node) Machine type Dell Dell PowerEdge C6525 Operating system Ubuntu Ubuntu Processor cores 49,152 128 CPUs 768 (AMD Milan 7713) 2 RAM 192 TiB 512 GiB Network 100 Gbps x 4 to Internet2 100 Gpbs to switch Storage 14 PB Total Ceph Storage 240gb SSD Large Memory Nodes (32 nodes) \u00b6 System Configuration Aggregate information Per Node (Large Memory Node) Machine type Dell Dell PowerEdge R7525 Operating system Ubuntu Ubuntu Processor cores 4,096 128 CPUs 64 (AMD Milan 7713) 2 RAM 32 TiB 1024 GiB Network 100 Gbps x 4 to Internet2 100 Gpbs to switch Storage 14 PB Total Ceph Storage 480gb SSD GPU Nodes (90 nodes) \u00b6 System Configuration Aggregate information Per Node (GPU Node) Machine type Dell Dell PowerEdge XE8545 Operating system Ubuntu Ubuntu Processor cores 11,520 128 CPUs 180 (AMD Milan 7713) 2 RAM 45 TiB 512 GiB GPUs 360 (NVIDIA A100 SXM4 40GB) 4 Network 100 Gbps x 4 to Internet2 100 Gpbs to switch Storage 14 PB Total Ceph Storage 960gb SSD","title":"Configuration and specifications"},{"location":"overview/config/#configuration-and-specifications","text":"","title":"Configuration and specifications"},{"location":"overview/config/#primary-cloud-specifications","text":"","title":"Primary Cloud Specifications"},{"location":"overview/config/#compute-nodes-384-nodes","text":"System Configuration Aggregate information Per Node (Compute Node) Machine type Dell Dell PowerEdge C6525 Operating system Ubuntu Ubuntu Processor cores 49,152 128 CPUs 768 (AMD Milan 7713) 2 RAM 192 TiB 512 GiB Network 100 Gbps x 4 to Internet2 100 Gpbs to switch Storage 14 PB Total Ceph Storage 240gb SSD","title":"Compute Nodes (384 nodes)"},{"location":"overview/config/#large-memory-nodes-32-nodes","text":"System Configuration Aggregate information Per Node (Large Memory Node) Machine type Dell Dell PowerEdge R7525 Operating system Ubuntu Ubuntu Processor cores 4,096 128 CPUs 64 (AMD Milan 7713) 2 RAM 32 TiB 1024 GiB Network 100 Gbps x 4 to Internet2 100 Gpbs to switch Storage 14 PB Total Ceph Storage 480gb SSD","title":"Large Memory Nodes (32 nodes)"},{"location":"overview/config/#gpu-nodes-90-nodes","text":"System Configuration Aggregate information Per Node (GPU Node) Machine type Dell Dell PowerEdge XE8545 Operating system Ubuntu Ubuntu Processor cores 11,520 128 CPUs 180 (AMD Milan 7713) 2 RAM 45 TiB 512 GiB GPUs 360 (NVIDIA A100 SXM4 40GB) 4 Network 100 Gbps x 4 to Internet2 100 Gpbs to switch Storage 14 PB Total Ceph Storage 960gb SSD","title":"GPU Nodes (90 nodes)"},{"location":"overview/keydiff/","text":"Key Differences Between Jetstream1 and Jetstream2 \u00b6 Jetstream1 utilized two primary clouds, geographically dispersed and powered by OpenStack , to provide thousands of concurrent virtual machines to researchers, educators, and students from hundreds of institutions across the United States. Jetstream1 featured Atmosphere, a user-friendly graphical user environment, as the means of access for the majority of users. Jetstream1 also allowed API access for infrastructure-driven projects like science gateways. There will be quite a few similarities between Jetstream1 and Jetstream2. Jetstream2 will be powered by a more recent version of OpenStack and will still have a primary mission of providing virtual machine services to research and education faculty, staff, and students. Jetstream2, however, will build on what Jetstream1 delivered and provide a number of improvements and new services. This page will try to identify the major differences. The biggest difference is that Jetstream2 will consist of one primary cloud and multiple regional clouds. As you can see by the image below, the primary cloud will be hosted at Indiana University in Bloomington, IN with regional clouds at various institutions across the United States. Another huge distinction is that instead of different domains \u2013 default (Atmosphere) and tacc (API) domains, there is one namespace for Jetstream2. You can change between the CLI, Horizon, Exosphere, and Cacao as you see fit to manage your resources Allocations will only be awarded on the primary cloud by default. Hardware \u00b6 Jetstream2 will also bring multiple classes of research computing hardware . Jetstream2 will still have hundreds of CPU-based compute nodes for general purpose virtual machines/computing. Jetstream2 will also feature a small number of large memory nodes with up to 1 terabyte of RAM. Jetstream2 will also make available 90 nodes of GPU-enabled nodes with four NVIDIA A100 GPUs . These will be subdivided using NVIDIA virtual GPU (vGPU) to allow Jetstream2 allocations to utilize from 1/8th of a GPU to an entire GPU in their instances to allow everything from educational use requiring a minimal amount of GPU processing power to a full GPU for research workloads. Interfaces \u00b6 Jetstream2 will also have multiple user interfaces. Atmosphere has evolved into a new tool called Containerized Atmosphere for Continuous Analysis Orchestration (CACAO or simply Cacao) , which is built on the principles of Atmosphere (abstracting complicated functions such as firewalls and virtual networking). Jetstream2 will also provide API services utilizing both the OpenStack Horizon GUI and a robust command line interface (CLI). Because Jetstream2 will no longer have separate operating domains for Cacao and API operations, those utilizing Jetstream2 can switch between interfaces easily, seeing all virtual machines and other assets created in any interface. This single namespace also allows for third-party interfaces that can manage any OpenStack created resource to be used with Jetstream2. At the time of deployment, Jetstream2 will feature one such third-party interface called Exosphere . Containers and Orchestration \u00b6 Jetstream2 will bring support for containers to the forefront of services. It will also support managing and scaling container-based workloads via the cloud-native functionality of OpenStack Magnum as has been demonstrated with Jetstream1. Users will be able to deploy Docker Swarm, Apache Mesos, or Kubernetes container orchestration engines to manage and run their container-based research workloads. In addition, the features of Cacao will provide similar functionality to individuals who have no desire to access the OpenStack API directly. Both approaches will allow researchers and educators to scale their workloads dynamically according to their needs. Additional Services \u00b6 Services such as OpenStack Heat will be available for researchers and developers, as well. OpenStack Heat is a service that allows individuals to instantiate complex resources with dependencies via a declarative YAML-based language. Similar to Magnum, other OpenStack services such as Trove and Sahara also leverage Heat to provide relational and non-relational databases and to provision data-intensive application clusters. Further, tools such as HashiCorp\u2019s Terraform programmable infrastructure, with the ability to deploy on Jetstream2, private clouds, and commercial clouds easily and consistently, will allow developers and researchers to have their environements where they need and when they need it. These capabilities build on one of the fundamental aspects of cloud computing that was demonstrated in abundance with Jetstream1: the ability of users to create, manage, and orchestrate use of tools autonomously, based on need, without involving sysadmins to install or enable new software. Virtual Clusters \u00b6 In addition to the ability for individuals to control their infrastructure programmatically, Jetstream2 will provide the capability to spin up elastic HPC virtual clusters (VCs) at the push of a button. These have been tested extensively on Jetstream1, with about thirty VCs running in production at different times. These Slurm-powered virtual clusters allow individuals to transition easily between cloud and HPC resources, acting as both a test-bed environment for custom software, and a highly-available production resource for projects with modest computational needs. The deployment process for these resources in Jetstream2 will be streamlined, allowing individuals to deploy an instance, acting as a head node, that is ready to accept jobs. Once jobs are submitted, worker instances will be automatically created and destroyed as needed. The Singularity/Apptainer container runtime environment will be built into these VCs, allowing individuals to use containerized scientific software without lengthy installation processes.","title":"Key Differences Between Jetstream 1 and 2"},{"location":"overview/keydiff/#key-differences-between-jetstream1-and-jetstream2","text":"Jetstream1 utilized two primary clouds, geographically dispersed and powered by OpenStack , to provide thousands of concurrent virtual machines to researchers, educators, and students from hundreds of institutions across the United States. Jetstream1 featured Atmosphere, a user-friendly graphical user environment, as the means of access for the majority of users. Jetstream1 also allowed API access for infrastructure-driven projects like science gateways. There will be quite a few similarities between Jetstream1 and Jetstream2. Jetstream2 will be powered by a more recent version of OpenStack and will still have a primary mission of providing virtual machine services to research and education faculty, staff, and students. Jetstream2, however, will build on what Jetstream1 delivered and provide a number of improvements and new services. This page will try to identify the major differences. The biggest difference is that Jetstream2 will consist of one primary cloud and multiple regional clouds. As you can see by the image below, the primary cloud will be hosted at Indiana University in Bloomington, IN with regional clouds at various institutions across the United States. Another huge distinction is that instead of different domains \u2013 default (Atmosphere) and tacc (API) domains, there is one namespace for Jetstream2. You can change between the CLI, Horizon, Exosphere, and Cacao as you see fit to manage your resources Allocations will only be awarded on the primary cloud by default.","title":"Key Differences Between Jetstream1 and Jetstream2"},{"location":"overview/keydiff/#hardware","text":"Jetstream2 will also bring multiple classes of research computing hardware . Jetstream2 will still have hundreds of CPU-based compute nodes for general purpose virtual machines/computing. Jetstream2 will also feature a small number of large memory nodes with up to 1 terabyte of RAM. Jetstream2 will also make available 90 nodes of GPU-enabled nodes with four NVIDIA A100 GPUs . These will be subdivided using NVIDIA virtual GPU (vGPU) to allow Jetstream2 allocations to utilize from 1/8th of a GPU to an entire GPU in their instances to allow everything from educational use requiring a minimal amount of GPU processing power to a full GPU for research workloads.","title":"Hardware"},{"location":"overview/keydiff/#interfaces","text":"Jetstream2 will also have multiple user interfaces. Atmosphere has evolved into a new tool called Containerized Atmosphere for Continuous Analysis Orchestration (CACAO or simply Cacao) , which is built on the principles of Atmosphere (abstracting complicated functions such as firewalls and virtual networking). Jetstream2 will also provide API services utilizing both the OpenStack Horizon GUI and a robust command line interface (CLI). Because Jetstream2 will no longer have separate operating domains for Cacao and API operations, those utilizing Jetstream2 can switch between interfaces easily, seeing all virtual machines and other assets created in any interface. This single namespace also allows for third-party interfaces that can manage any OpenStack created resource to be used with Jetstream2. At the time of deployment, Jetstream2 will feature one such third-party interface called Exosphere .","title":"Interfaces"},{"location":"overview/keydiff/#containers-and-orchestration","text":"Jetstream2 will bring support for containers to the forefront of services. It will also support managing and scaling container-based workloads via the cloud-native functionality of OpenStack Magnum as has been demonstrated with Jetstream1. Users will be able to deploy Docker Swarm, Apache Mesos, or Kubernetes container orchestration engines to manage and run their container-based research workloads. In addition, the features of Cacao will provide similar functionality to individuals who have no desire to access the OpenStack API directly. Both approaches will allow researchers and educators to scale their workloads dynamically according to their needs.","title":"Containers and Orchestration"},{"location":"overview/keydiff/#additional-services","text":"Services such as OpenStack Heat will be available for researchers and developers, as well. OpenStack Heat is a service that allows individuals to instantiate complex resources with dependencies via a declarative YAML-based language. Similar to Magnum, other OpenStack services such as Trove and Sahara also leverage Heat to provide relational and non-relational databases and to provision data-intensive application clusters. Further, tools such as HashiCorp\u2019s Terraform programmable infrastructure, with the ability to deploy on Jetstream2, private clouds, and commercial clouds easily and consistently, will allow developers and researchers to have their environements where they need and when they need it. These capabilities build on one of the fundamental aspects of cloud computing that was demonstrated in abundance with Jetstream1: the ability of users to create, manage, and orchestrate use of tools autonomously, based on need, without involving sysadmins to install or enable new software.","title":"Additional Services"},{"location":"overview/keydiff/#virtual-clusters","text":"In addition to the ability for individuals to control their infrastructure programmatically, Jetstream2 will provide the capability to spin up elastic HPC virtual clusters (VCs) at the push of a button. These have been tested extensively on Jetstream1, with about thirty VCs running in production at different times. These Slurm-powered virtual clusters allow individuals to transition easily between cloud and HPC resources, acting as both a test-bed environment for custom software, and a highly-available production resource for projects with modest computational needs. The deployment process for these resources in Jetstream2 will be streamlined, allowing individuals to deploy an instance, acting as a head node, that is ready to accept jobs. Once jobs are submitted, worker instances will be automatically created and destroyed as needed. The Singularity/Apptainer container runtime environment will be built into these VCs, allowing individuals to use containerized scientific software without lengthy installation processes.","title":"Virtual Clusters"},{"location":"overview/network/","text":"Jetstream2 Network Configuration and Policies \u00b6 Summary of Network Configuration and Policies \u00b6 Hardware Configuration: \u00b6 The Jetstream2 primary cloud configuation features: 100 Gbps network connectivity from the compute hosts to the cloud\u2019s internal network infrastructure 4x40 Gbps uplinks from the cloud infrastructure to the data center infrastructure (2x100 planned) 100 Gbps connectivity from the site infrastructure to the Internet2 backbone 100 Gbps connectivity to the XSEDE research network via virtualized link Individual instances have full access to this infrastructure with no added speed limits. It is important to note that connections coming from commercial/commodity internet will likely not be as fast as those coming from Internet2 sites. Persistent IPs: \u00b6 A key difference between Jetstream1 and Jetstream2 is that no special or additional access is required to get a persistent IP address. Some of the GUI interfaces like Exosphere and Cacao release IPs by default when a VM is deleted. Horizon and the Jetstream2 CLI require you to explicitly release the IP address. We do ask that you release any unused IPs back to the public pool. There are a finite number of IPs available and allocations hoarding them may inadvertently cause issues for other Jetstream2 researchers. The Jetstream administration team reserves the right to release any IP addresses not associated with a VM as needed. Network Security Policies: \u00b6 In general, Jetstream2 does not restrict inbound or outbound access to virtual machines. There are a handful of blocks at the instutional level that are outside of the control of the Jetstream2 staff. In general, though, the most common Unix service ports (eg. 22/ssh, 80/http, 443/https, etc) are not restricted in any way. Whether they are open by default will be dependent on which user interface you\u2019re launching your VM with. Please refer to the Security FAQ for additional information. In-depth exploration of Jetstream2 Networking \u00b6 This section describes the network architecture of Jetstream2\u2019s primary cloud at Indiana University. Regional sites may be configured differently. There are three kinds of networks used on Jetstream2. The Cluster Network , which carries all of the tenant/workload/data traffic. It underlays the Neutron overlay networks for cloud workloads. It also carries all storage traffic and OpenStack control plane traffic. The Neutron overlay networks , an entirely software-defined (virtual) networking stack which runs on top of the cluster network. The Management Network , which is fully physically separate from the cluster network. It is used for managing hosts and their iDRAC s. This document primarily looks at the Cluster Network and may delve into the Neutron overlay networks . The Management Network is not in the scope of user-facing documentation. Cluster Network \u00b6 BGP in the Data Center is recommended reading. It provides background and orientation to Clos networks and BGP the way that it is used here. It is a short, accessible read for someone with general IT networking knowledge. For a quicker version, read chapters 1, 2, 4, and 6. Skim chapters 3 and 5. The Jetstream2 network is a two-tier spine-and-leaf network in a fat tree or Clos topology. There are no inter-switch links (ISLs). Traffic that transits the network will generally traverse a leaf switch, a spine switch, and another leaf switch. Two of the leaf switches connect the Cluster network to the campus WAN and internet. These happen to be the two leaf switches that the control plane nodes are also connected to. Physical Switches \u00b6 Switches use the Cumulus Linux operating system. Switch types in the cluster network: Model Topology Layer Switch Quantity Port speed Port Qty Per Switch Mellanox SN4600 Spine 6 100 GbE 64 Mellanox SN2700 Leaf 37 100 GbE 32 Dual-attached Servers \u00b6 Each host (server) is dual-attached to the Cluster network via two separate leaf switches, using two 100 GbE interfaces. Dual-attaching serves both purposes of redundancy and load-balancing (via equal-cost multipathing). Redundancy is particularly important here, because many workloads that run on Jetstream2 will not be \u2018cloud-native\u2019 or tolerant of partial-cluster network connectivity failure. Many users will hand-configure their analyses and services on a single instance (virtual machine) that runs on a single host in a single cabinet. Dual-attaching every physical server means that a single network switch failure will not cause a complete or prolonged outage for these non-cloud-native workloads. The physical compute node will quickly notice that one of its network links is offline, and re-route all network traffic to use the remaining functional link. Server cross-wiring \u00b6 Throughout the cluster, a given host may be variously connected to: 2 switches in the same cabinet, or 2 switches in an adjacent cabinet, or 1 switch in the same cabinet and 1 in an adjacent cabinet. The benefit of this cross-wiring between cabinets is increased utilization of switch ports, and reduction of the number of switches needed. This is especially true in Jetstream2 because different types of hosts (e.g. compute, GPU, storage) are different physical sizes, i.e. different densities in a cabinet. The cost of cross-wiring is limited cabinet mobility, because hosts in one cabinet may have many connections to a switch in the next cabinet over, but cabinets are not expected to be moved over the lifetime of Jetstream2. Border Gateway Protocol \u00b6 Jetstream2 uses Border Gateway Protocol (BGP) to route and deliver packets across the entire cluster network. BGP was originally created to handle traffic between large network service providers (a.k.a. carriers), but is also adapted for use within data centers. In this application, BGP replaces use of MLAG to route traffic over redundant physical paths between switches. Jetstream1 used MLAG, but Jetstream2 does not, for the following reasons: MLAG is fragile when making config changes \u201cRigorous and conservative interface state management needed. Temporary loops or duplicates not acceptable\u201d ( source ) MLAG does not maximize use of aggregate throughout supported by redundant physical links. MLAG requires inter-switch links (ISLs), which would require many (about 96) additional links between switches, which would require more cabling and purchasing switches with more physical ports. Use of BGP solves all of these drawbacks with MLAG. (See also MLAG on Linux: Lessons Learned . For a comparison of BGP and other internal routing protocols like Open Shortest Path First (OSPF), see pages 22 and 24 of BGP in the Data Center (labeled as pages 14 and 16)). Some more configuration details: On the Cluster network, peering between all nodes uses eBGP (external BGP), because each host and each leaf (but not spine) switch has its own individually-assigned private autonomous system number (ASN). iBGP (internal BGP) is not used. The spine switches all share a common ASN, 64512. This avoids path hunting problems as described in pages 26-29 of BGP in the Data Center (labeled as pages 18-21). Equal-cost multipathing allows packet forwarding to a single destination to be load-balanced across multiple network paths. BGP Unnumbered \u00b6 In the cluster network, there is no network bridging or explicitly-defined shared subnets between connected switches (or between switches and hosts). There is only routing of packets between hosts. BGP requires underlying TCP/IP connectivity in order to create a connection, so how can that happen if we have no explicitly-defined subnets or bridges? In very short, each physical 100 GbE interface on each node (host or switch) has only a link-local IPv6 address assigned. IPv6 Router Advertisement (RA) is a link-level protocol that periodically announces an interface\u2019s IPv6 addresses, including the link-local address. This is how each node identifies the IP of its peer. RFC 5549 allows a system to advertise IPv4 routes, and route IPv4 packets, over a pure IPv6 network. It also adds an \u201cextended nexthop\u201d capability to BGP, which negotiates use of RFC 5549 over a BGP peering session. This strategy is described in greater detail in chapter 4 of BGP in the Data Center . The result is that there is no need to define a shared subnet between any nodes. There is also no need to explicitly define neighbor-specific information in a node\u2019s configuration. Two peers connected with a network cable will discover each others\u2019 link-local addresses, exchange IPv6 route advertisements, and initiate a BGP route exchange. Routing on the Host \u00b6 In traditional enterprise networking, packet routing (at layer 3 in the OSI model) is generally performed off the host , i.e., by router hardware that is physically separate from servers running the workloads. Sometimes there is purpose-dedicated router hardware, and sometimes routing is a role performed by the ethernet switches. In either case, servers do not make most packet routing decisions. Servers are bridged to routers at layer 2, on a common subnetwork. Servers determine where to send packets using ARP (Address Resolution Protocol). This is not how the Jetstream2 Cluster network operates. Instead, the BGP routing fabric is extended all the way to each server. Each host has its own BGP autonomous system number (ASN), and runs the same FRRouting software as the switches. The host\u2019s primary IPv4 address (in private, RFC 1918 space) will appear on the loopback interface, and it is configured as a /32 in CIDR notation \u2013 as its very own single-IP subnetwork, not bridged anywhere else! A server advertises BGP routes for its own internal IPv4 address. These routes are advertised to both switches that the server is attached to, via IPv6 link-local connectivity. This overall strategy is called \u201cRouting on the Host\u201d, and it is described in chapter 6 of BGP in the Data Center . Again, in the cluster network there is no bridging between physical nodes, only routing. Some more background reading (if you don\u2019t like the book): - Linux Routing On The Host With FRR ( archived version ) - Independence from L2 Data Centers - Border Gateway Protocol ( archived version ) Conclusion \u00b6 All of this BGP unnumbered architecture supports the \u2018Abstraction layer\u2019 of one or more IPv4 addresses on each host, and the ability to route packets from any host to any other host by its IPv4 address. At this level of abstraction and above, there is no concern with (or visibility of) the BGP routing layer or link-local IPv6 addresses. The Neutron overlay networks do not see it, or know about its complexity. Areas of future discussion: Neutron Overlay Networks \u00b6 TODO Layer 2 (Data Link) \u00b6 VXLAN / Open vSwitch / Linux Bridge? Layer 3 (Network) \u00b6 Neutron Distributed Virtual Routing (DVR) Typically with DVR, egress traffic leaves a compute node directly to the WAN, while ingress traffic passes through a network node. See also: - https://docs.openstack.org/neutron/latest/admin/config-service-subnets.html","title":"Network configuration and considerations"},{"location":"overview/network/#jetstream2-network-configuration-and-policies","text":"","title":"Jetstream2 Network Configuration and Policies"},{"location":"overview/network/#summary-of-network-configuration-and-policies","text":"","title":"Summary of Network Configuration and Policies"},{"location":"overview/network/#hardware-configuration","text":"The Jetstream2 primary cloud configuation features: 100 Gbps network connectivity from the compute hosts to the cloud\u2019s internal network infrastructure 4x40 Gbps uplinks from the cloud infrastructure to the data center infrastructure (2x100 planned) 100 Gbps connectivity from the site infrastructure to the Internet2 backbone 100 Gbps connectivity to the XSEDE research network via virtualized link Individual instances have full access to this infrastructure with no added speed limits. It is important to note that connections coming from commercial/commodity internet will likely not be as fast as those coming from Internet2 sites.","title":"Hardware Configuration:"},{"location":"overview/network/#persistent-ips","text":"A key difference between Jetstream1 and Jetstream2 is that no special or additional access is required to get a persistent IP address. Some of the GUI interfaces like Exosphere and Cacao release IPs by default when a VM is deleted. Horizon and the Jetstream2 CLI require you to explicitly release the IP address. We do ask that you release any unused IPs back to the public pool. There are a finite number of IPs available and allocations hoarding them may inadvertently cause issues for other Jetstream2 researchers. The Jetstream administration team reserves the right to release any IP addresses not associated with a VM as needed.","title":"Persistent IPs:"},{"location":"overview/network/#network-security-policies","text":"In general, Jetstream2 does not restrict inbound or outbound access to virtual machines. There are a handful of blocks at the instutional level that are outside of the control of the Jetstream2 staff. In general, though, the most common Unix service ports (eg. 22/ssh, 80/http, 443/https, etc) are not restricted in any way. Whether they are open by default will be dependent on which user interface you\u2019re launching your VM with. Please refer to the Security FAQ for additional information.","title":"Network Security Policies:"},{"location":"overview/network/#in-depth-exploration-of-jetstream2-networking","text":"This section describes the network architecture of Jetstream2\u2019s primary cloud at Indiana University. Regional sites may be configured differently. There are three kinds of networks used on Jetstream2. The Cluster Network , which carries all of the tenant/workload/data traffic. It underlays the Neutron overlay networks for cloud workloads. It also carries all storage traffic and OpenStack control plane traffic. The Neutron overlay networks , an entirely software-defined (virtual) networking stack which runs on top of the cluster network. The Management Network , which is fully physically separate from the cluster network. It is used for managing hosts and their iDRAC s. This document primarily looks at the Cluster Network and may delve into the Neutron overlay networks . The Management Network is not in the scope of user-facing documentation.","title":"In-depth exploration of Jetstream2 Networking"},{"location":"overview/network/#cluster-network","text":"BGP in the Data Center is recommended reading. It provides background and orientation to Clos networks and BGP the way that it is used here. It is a short, accessible read for someone with general IT networking knowledge. For a quicker version, read chapters 1, 2, 4, and 6. Skim chapters 3 and 5. The Jetstream2 network is a two-tier spine-and-leaf network in a fat tree or Clos topology. There are no inter-switch links (ISLs). Traffic that transits the network will generally traverse a leaf switch, a spine switch, and another leaf switch. Two of the leaf switches connect the Cluster network to the campus WAN and internet. These happen to be the two leaf switches that the control plane nodes are also connected to.","title":"Cluster Network"},{"location":"overview/network/#physical-switches","text":"Switches use the Cumulus Linux operating system. Switch types in the cluster network: Model Topology Layer Switch Quantity Port speed Port Qty Per Switch Mellanox SN4600 Spine 6 100 GbE 64 Mellanox SN2700 Leaf 37 100 GbE 32","title":"Physical Switches"},{"location":"overview/network/#dual-attached-servers","text":"Each host (server) is dual-attached to the Cluster network via two separate leaf switches, using two 100 GbE interfaces. Dual-attaching serves both purposes of redundancy and load-balancing (via equal-cost multipathing). Redundancy is particularly important here, because many workloads that run on Jetstream2 will not be \u2018cloud-native\u2019 or tolerant of partial-cluster network connectivity failure. Many users will hand-configure their analyses and services on a single instance (virtual machine) that runs on a single host in a single cabinet. Dual-attaching every physical server means that a single network switch failure will not cause a complete or prolonged outage for these non-cloud-native workloads. The physical compute node will quickly notice that one of its network links is offline, and re-route all network traffic to use the remaining functional link.","title":"Dual-attached Servers"},{"location":"overview/network/#server-cross-wiring","text":"Throughout the cluster, a given host may be variously connected to: 2 switches in the same cabinet, or 2 switches in an adjacent cabinet, or 1 switch in the same cabinet and 1 in an adjacent cabinet. The benefit of this cross-wiring between cabinets is increased utilization of switch ports, and reduction of the number of switches needed. This is especially true in Jetstream2 because different types of hosts (e.g. compute, GPU, storage) are different physical sizes, i.e. different densities in a cabinet. The cost of cross-wiring is limited cabinet mobility, because hosts in one cabinet may have many connections to a switch in the next cabinet over, but cabinets are not expected to be moved over the lifetime of Jetstream2.","title":"Server cross-wiring"},{"location":"overview/network/#border-gateway-protocol","text":"Jetstream2 uses Border Gateway Protocol (BGP) to route and deliver packets across the entire cluster network. BGP was originally created to handle traffic between large network service providers (a.k.a. carriers), but is also adapted for use within data centers. In this application, BGP replaces use of MLAG to route traffic over redundant physical paths between switches. Jetstream1 used MLAG, but Jetstream2 does not, for the following reasons: MLAG is fragile when making config changes \u201cRigorous and conservative interface state management needed. Temporary loops or duplicates not acceptable\u201d ( source ) MLAG does not maximize use of aggregate throughout supported by redundant physical links. MLAG requires inter-switch links (ISLs), which would require many (about 96) additional links between switches, which would require more cabling and purchasing switches with more physical ports. Use of BGP solves all of these drawbacks with MLAG. (See also MLAG on Linux: Lessons Learned . For a comparison of BGP and other internal routing protocols like Open Shortest Path First (OSPF), see pages 22 and 24 of BGP in the Data Center (labeled as pages 14 and 16)). Some more configuration details: On the Cluster network, peering between all nodes uses eBGP (external BGP), because each host and each leaf (but not spine) switch has its own individually-assigned private autonomous system number (ASN). iBGP (internal BGP) is not used. The spine switches all share a common ASN, 64512. This avoids path hunting problems as described in pages 26-29 of BGP in the Data Center (labeled as pages 18-21). Equal-cost multipathing allows packet forwarding to a single destination to be load-balanced across multiple network paths.","title":"Border Gateway Protocol"},{"location":"overview/network/#bgp-unnumbered","text":"In the cluster network, there is no network bridging or explicitly-defined shared subnets between connected switches (or between switches and hosts). There is only routing of packets between hosts. BGP requires underlying TCP/IP connectivity in order to create a connection, so how can that happen if we have no explicitly-defined subnets or bridges? In very short, each physical 100 GbE interface on each node (host or switch) has only a link-local IPv6 address assigned. IPv6 Router Advertisement (RA) is a link-level protocol that periodically announces an interface\u2019s IPv6 addresses, including the link-local address. This is how each node identifies the IP of its peer. RFC 5549 allows a system to advertise IPv4 routes, and route IPv4 packets, over a pure IPv6 network. It also adds an \u201cextended nexthop\u201d capability to BGP, which negotiates use of RFC 5549 over a BGP peering session. This strategy is described in greater detail in chapter 4 of BGP in the Data Center . The result is that there is no need to define a shared subnet between any nodes. There is also no need to explicitly define neighbor-specific information in a node\u2019s configuration. Two peers connected with a network cable will discover each others\u2019 link-local addresses, exchange IPv6 route advertisements, and initiate a BGP route exchange.","title":"BGP Unnumbered"},{"location":"overview/network/#routing-on-the-host","text":"In traditional enterprise networking, packet routing (at layer 3 in the OSI model) is generally performed off the host , i.e., by router hardware that is physically separate from servers running the workloads. Sometimes there is purpose-dedicated router hardware, and sometimes routing is a role performed by the ethernet switches. In either case, servers do not make most packet routing decisions. Servers are bridged to routers at layer 2, on a common subnetwork. Servers determine where to send packets using ARP (Address Resolution Protocol). This is not how the Jetstream2 Cluster network operates. Instead, the BGP routing fabric is extended all the way to each server. Each host has its own BGP autonomous system number (ASN), and runs the same FRRouting software as the switches. The host\u2019s primary IPv4 address (in private, RFC 1918 space) will appear on the loopback interface, and it is configured as a /32 in CIDR notation \u2013 as its very own single-IP subnetwork, not bridged anywhere else! A server advertises BGP routes for its own internal IPv4 address. These routes are advertised to both switches that the server is attached to, via IPv6 link-local connectivity. This overall strategy is called \u201cRouting on the Host\u201d, and it is described in chapter 6 of BGP in the Data Center . Again, in the cluster network there is no bridging between physical nodes, only routing. Some more background reading (if you don\u2019t like the book): - Linux Routing On The Host With FRR ( archived version ) - Independence from L2 Data Centers - Border Gateway Protocol ( archived version )","title":"Routing on the Host"},{"location":"overview/network/#conclusion","text":"All of this BGP unnumbered architecture supports the \u2018Abstraction layer\u2019 of one or more IPv4 addresses on each host, and the ability to route packets from any host to any other host by its IPv4 address. At this level of abstraction and above, there is no concern with (or visibility of) the BGP routing layer or link-local IPv6 addresses. The Neutron overlay networks do not see it, or know about its complexity. Areas of future discussion:","title":"Conclusion"},{"location":"overview/network/#neutron-overlay-networks","text":"TODO","title":"Neutron Overlay Networks"},{"location":"overview/network/#layer-2-data-link","text":"VXLAN / Open vSwitch / Linux Bridge?","title":"Layer 2 (Data Link)"},{"location":"overview/network/#layer-3-network","text":"Neutron Distributed Virtual Routing (DVR) Typically with DVR, egress traffic leaves a compute node directly to the WAN, while ingress traffic passes through a network node. See also: - https://docs.openstack.org/neutron/latest/admin/config-service-subnets.html","title":"Layer 3 (Network)"},{"location":"overview/overview-doc/","text":"Jetstream2 System Overview \u00b6 Jetstream2 is a transformative update to the NSF\u2019s science and engineering cloud infrastructure, will provide 8 petaFLOPS of virtual supercomputing power to simplify data analysis, boost discovery, and increase availability of AI resources. It is an NSF-funded, user-friendly cloud environment designed to allow \u201calways on\u201d research infrastructure and to give researchers access to interactive computing and data analysis resources on demand, whenever and wherever they want to analyze their data. While it shares many common features and abilities with other research computing resources, it is not a traditional High Performance Computing (HPC) or High Throughput Computing (HTC) environment. It provides a library of virtual machines and shared software designed to create research infrastructure or to perform discipline-specific scientific analysis. Software creators and researchers will be able to create their own customized virtual machines (VM) or their own private computing system within Jetstream2. Jetstream features multiple user interfaces, including the following web-based user interfaces : Exosphere Horizon Cacao as well as the OpenStack Command Line Interface (CLI) and the OpenStack Software Development Kit (SDK) ) The operational software environment is based on OpenStack . Accessing Jetstream \u00b6 Jetstream2 is primarily accessible through either the Exosphere or Cacao web interfaces using XSEDE credentials via Globus Auth . Jetstream2 is not accessible via XSEDE\u2019s Single Sign-On Login Hub. Newly created XSEDE accounts must be added to a Jetstream2 specific allocation by the PI or Resource manager in order to access Jetstream2. Jetstream2 is meant primarily for interactive research, small scale processing on demand , or as the backend to science gateways to send research jobs to other HPC or HTC resources, or for general infrastructure for research or research-related development. Jetstream2 may be used for prototyping , for creating tailored workflows to either use at smaller scale with a handful of CPUs or to port to larger environments after doing your proof of concept work at a smaller level. About Jetstream2 \u00b6 Consisting of five computational systems, Jetstream2\u2019s primary system will be located at Indiana University, with four smaller regional systems deployed nationwide at partners Arizona State University, Cornell University, the University of Hawai\u2019i, and the Texas Advanced Computing Center. The 8 petaFLOPS cloud computing system will use next-generation AMD EPYC processors and NVIDIA A100 GPUs, and 17.2 petabytes of storage. Within the Pervasive Technology Institute the project will be led by UITS Research Technologies with key contributions from the Cyberinfrastructure Integration Research Center and the Center for Applied Cybersecurity Research. Additional Jetstream2 partners include the University of Arizona, Johns Hopkins University, and the University Corporation for Atmospheric Research, with Dell Inc. as the primary supplier. For more information, please see: NSF Award 2005506 - Jetstream 2: Accelerating Science and Engineering On-Demand Jetstream System Specifications","title":"System Overview"},{"location":"overview/overview-doc/#jetstream2-system-overview","text":"Jetstream2 is a transformative update to the NSF\u2019s science and engineering cloud infrastructure, will provide 8 petaFLOPS of virtual supercomputing power to simplify data analysis, boost discovery, and increase availability of AI resources. It is an NSF-funded, user-friendly cloud environment designed to allow \u201calways on\u201d research infrastructure and to give researchers access to interactive computing and data analysis resources on demand, whenever and wherever they want to analyze their data. While it shares many common features and abilities with other research computing resources, it is not a traditional High Performance Computing (HPC) or High Throughput Computing (HTC) environment. It provides a library of virtual machines and shared software designed to create research infrastructure or to perform discipline-specific scientific analysis. Software creators and researchers will be able to create their own customized virtual machines (VM) or their own private computing system within Jetstream2. Jetstream features multiple user interfaces, including the following web-based user interfaces : Exosphere Horizon Cacao as well as the OpenStack Command Line Interface (CLI) and the OpenStack Software Development Kit (SDK) ) The operational software environment is based on OpenStack .","title":"Jetstream2 System Overview"},{"location":"overview/overview-doc/#accessing-jetstream","text":"Jetstream2 is primarily accessible through either the Exosphere or Cacao web interfaces using XSEDE credentials via Globus Auth . Jetstream2 is not accessible via XSEDE\u2019s Single Sign-On Login Hub. Newly created XSEDE accounts must be added to a Jetstream2 specific allocation by the PI or Resource manager in order to access Jetstream2. Jetstream2 is meant primarily for interactive research, small scale processing on demand , or as the backend to science gateways to send research jobs to other HPC or HTC resources, or for general infrastructure for research or research-related development. Jetstream2 may be used for prototyping , for creating tailored workflows to either use at smaller scale with a handful of CPUs or to port to larger environments after doing your proof of concept work at a smaller level.","title":"Accessing Jetstream"},{"location":"overview/overview-doc/#about-jetstream2","text":"Consisting of five computational systems, Jetstream2\u2019s primary system will be located at Indiana University, with four smaller regional systems deployed nationwide at partners Arizona State University, Cornell University, the University of Hawai\u2019i, and the Texas Advanced Computing Center. The 8 petaFLOPS cloud computing system will use next-generation AMD EPYC processors and NVIDIA A100 GPUs, and 17.2 petabytes of storage. Within the Pervasive Technology Institute the project will be led by UITS Research Technologies with key contributions from the Cyberinfrastructure Integration Research Center and the Center for Applied Cybersecurity Research. Additional Jetstream2 partners include the University of Arizona, Johns Hopkins University, and the University Corporation for Atmospheric Research, with Dell Inc. as the primary supplier. For more information, please see: NSF Award 2005506 - Jetstream 2: Accelerating Science and Engineering On-Demand Jetstream System Specifications","title":"About Jetstream2"},{"location":"overview/status/","text":"Jetstream2 system status and information \u00b6 Overall JS2 system status Jetstream2 is maintaining a system status and announcement site at https://jetstream.status.io/ Please visit that site for detailed system status information, planned maintenance announcements, and to subscribe to our downtime/outages mailing list. XSEDE User News and Announcements \u00b6 XSEDE User News ) also is a source for XSEDE and Jetstream2 related news. We highly recommend that you consider adding the following news catagories via XSEDE User News: Jetstream2 (Indiana) Conferences Education & Outreach General User News Training The XSEDE lists are generally low traffic and they do not share your information. You can change your preferences at any time via the Manage XSEDE User News Email Subscription page. ACCESS User News and Announcements \u00b6 Links and information for ACCESS user news will be posted here as they become available.","title":"Jetstream2 Status and News"},{"location":"overview/status/#jetstream2-system-status-and-information","text":"Overall JS2 system status Jetstream2 is maintaining a system status and announcement site at https://jetstream.status.io/ Please visit that site for detailed system status information, planned maintenance announcements, and to subscribe to our downtime/outages mailing list.","title":"Jetstream2 system status and information"},{"location":"overview/status/#xsede-user-news-and-announcements","text":"XSEDE User News ) also is a source for XSEDE and Jetstream2 related news. We highly recommend that you consider adding the following news catagories via XSEDE User News: Jetstream2 (Indiana) Conferences Education & Outreach General User News Training The XSEDE lists are generally low traffic and they do not share your information. You can change your preferences at any time via the Manage XSEDE User News Email Subscription page.","title":"XSEDE User News and Announcements"},{"location":"overview/status/#access-user-news-and-announcements","text":"Links and information for ACCESS user news will be posted here as they become available.","title":"ACCESS User News and Announcements"},{"location":"ui/cacao/alloc/","text":"Cacao Overview \u226b Choosing your Allocation Choosing your Allocation \u00b6 Your account should have an Openstack project for every active XSEDE project-allocation. Jetstream2 uses the Openstack project to launch and recognize utilization against your XSEDE project-allocation. Within Cacao, you can assign the XSEDE project-allocation in the deployment wizard when you select the appropriate credentials. Note, a Jetstream2 application credential will automatically assign the Openstack project, which is tied to the application credential. Jetstream password credentials allow you to change the XSEDE project-allocation. Now that you know how to choose your allocation, you can learn more about credentials and deployments .","title":"Choosing Your Allocation"},{"location":"ui/cacao/alloc/#choosing-your-allocation","text":"Your account should have an Openstack project for every active XSEDE project-allocation. Jetstream2 uses the Openstack project to launch and recognize utilization against your XSEDE project-allocation. Within Cacao, you can assign the XSEDE project-allocation in the deployment wizard when you select the appropriate credentials. Note, a Jetstream2 application credential will automatically assign the Openstack project, which is tied to the application credential. Jetstream password credentials allow you to change the XSEDE project-allocation. Now that you know how to choose your allocation, you can learn more about credentials and deployments .","title":"Choosing your Allocation"},{"location":"ui/cacao/credentials/","text":"Cacao Overview \u226b Credentials Credentials \u00b6 Cacao allows you to manage various types of cloud and access credentials. Some credentials are required before using a cloud, such as the Jetstream Password Credential or the Jetstream","title":"Credentials"},{"location":"ui/cacao/credentials/#credentials","text":"Cacao allows you to manage various types of cloud and access credentials. Some credentials are required before using a cloud, such as the Jetstream Password Credential or the Jetstream","title":"Credentials"},{"location":"ui/cacao/deployments/","text":"Cacao Overview \u226b Deployments Deployments (Launching instances) \u00b6","title":"Deployments (Launching a VM)"},{"location":"ui/cacao/deployments/#deployments-launching-instances","text":"","title":"Deployments (Launching instances)"},{"location":"ui/cacao/getting_started/","text":"Cacao Overview \u226b Getting started for new users Getting started for new users \u00b6 Cacao is a space for you to organize and manage your Jetstream cloud resources. Once you obtain your Jetstream allocation, this guide will help you get started. 1. Login to Cacao \u00b6 Cacao uses Globus XSEDE credentials for identity In your browser, connect to https://cacao.jetstream-cloud.org << coming soon Click \u201cSign-In\u201d. If you are not currently logged into globus, you should select XSEDE Enter your XSEDE login credentials You may need to authenticate with your two-factor 1. Add your Jetstream 2 credentials \u00b6 Access to any Jetstream 2 cloud will begin with adding your Openstack credentials, which may be different from your XSEDE credentials. Click on the Credentials menu Click on Add Credential button Select Jetstream Password Credential Select Cloud Enter Openstack Project Name; this is the default project to create resources Enter your Openstack Username Enter your Openstack Password Click on the Add button Add a Public SSH Key \u00b6 SSH Keys are used to login to virtual machines (sometimes called \u2018servers\u2019 or \u2018instances\u2019) after they are launched. Click on the Credentials menu Click on the Add Credential button Select Public SSH Key Enter a name for your public ssh key Paste in your public ssh key Click on the Add button 2. Create your first workspace \u00b6 You can use Workspaces to organize your deployments. Click on the Deployments menu Click on the Add Workspace button Enter your Workspace name Enter a Description Enter a Default Cloud, which is used to deploy your resources Click on the Create Workspace Button 3. Create your first deployment \u00b6 Click on the Deployments menu Click on the Add Deployment button Select Workspace, then Next button Select the \u201copenstack-single-image-new\u201d Template, then Next Button Select your Openstack credential, then wait a few seconds for the project list to be retrieved Select your Openstack project to launch your deployment Enter your deployment values A. Deployment name B. Select your image C. Number of instances D. Size (also called \u2018flavor\u2019) Click Next button Review the deployment settings, then click Submit button 4. Learn about other Cacao features \u00b6","title":"Getting Started For New Cacao Users"},{"location":"ui/cacao/getting_started/#getting-started-for-new-users","text":"Cacao is a space for you to organize and manage your Jetstream cloud resources. Once you obtain your Jetstream allocation, this guide will help you get started.","title":"Getting started for new users"},{"location":"ui/cacao/getting_started/#1-login-to-cacao","text":"Cacao uses Globus XSEDE credentials for identity In your browser, connect to https://cacao.jetstream-cloud.org << coming soon Click \u201cSign-In\u201d. If you are not currently logged into globus, you should select XSEDE Enter your XSEDE login credentials You may need to authenticate with your two-factor","title":"1. Login to Cacao"},{"location":"ui/cacao/getting_started/#1-add-your-jetstream-2-credentials","text":"Access to any Jetstream 2 cloud will begin with adding your Openstack credentials, which may be different from your XSEDE credentials. Click on the Credentials menu Click on Add Credential button Select Jetstream Password Credential Select Cloud Enter Openstack Project Name; this is the default project to create resources Enter your Openstack Username Enter your Openstack Password Click on the Add button","title":"1. Add your Jetstream 2 credentials"},{"location":"ui/cacao/getting_started/#add-a-public-ssh-key","text":"SSH Keys are used to login to virtual machines (sometimes called \u2018servers\u2019 or \u2018instances\u2019) after they are launched. Click on the Credentials menu Click on the Add Credential button Select Public SSH Key Enter a name for your public ssh key Paste in your public ssh key Click on the Add button","title":"Add a Public SSH Key"},{"location":"ui/cacao/getting_started/#2-create-your-first-workspace","text":"You can use Workspaces to organize your deployments. Click on the Deployments menu Click on the Add Workspace button Enter your Workspace name Enter a Description Enter a Default Cloud, which is used to deploy your resources Click on the Create Workspace Button","title":"2. Create your first workspace"},{"location":"ui/cacao/getting_started/#3-create-your-first-deployment","text":"Click on the Deployments menu Click on the Add Deployment button Select Workspace, then Next button Select the \u201copenstack-single-image-new\u201d Template, then Next Button Select your Openstack credential, then wait a few seconds for the project list to be retrieved Select your Openstack project to launch your deployment Enter your deployment values A. Deployment name B. Select your image C. Number of instances D. Size (also called \u2018flavor\u2019) Click Next button Review the deployment settings, then click Submit button","title":"3. Create your first deployment"},{"location":"ui/cacao/getting_started/#4-learn-about-other-cacao-features","text":"","title":"4. Learn about other Cacao features"},{"location":"ui/cacao/glossary/","text":"Cacao Overview \u226b Cacao Glossary Cacao Glossary \u00b6 This glossary defines common terms you might see in Cacao A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | Z | W | Z A \u00b6 Application credential An application credential, specifically an Openstack application credential , is a credential that allows a user to login to an Openstack cloud without the user\u2019s password. C \u00b6 Continuous analysis Continuous analysis is the ability to execute workflows triggered by events, such as data changes or code changes D \u00b6 Deployment A deployment is one or more resources, such as instances, created on one or more clouds. I \u00b6 Instance An instance is another name for a virtual machine or server (in Openstack) M \u00b6 Meta cloud orchestration The ability to provision resources in different types of clouds P \u00b6 Provider A provider is another name for cloud or cloud provider, where cloud resources are created R \u00b6 Resource A resource or cloud resource is a general term used for any object \u2013 instance, volume, container \u2013 in a cloud T \u00b6 Template A template refers to a set of instructions or description of what cloud resources to create on a cloud. A template in Cacao requires a template engine to process the hte instructions or description. Currently, Cacao\u2019s primary templates for OpenStack use Hashicorp Terraform. W \u00b6 Workspace A workspace is a way to group one or more deployments.","title":"Glossary"},{"location":"ui/cacao/glossary/#cacao-glossary","text":"This glossary defines common terms you might see in Cacao A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | Z | W | Z","title":"Cacao Glossary"},{"location":"ui/cacao/glossary/#a","text":"Application credential An application credential, specifically an Openstack application credential , is a credential that allows a user to login to an Openstack cloud without the user\u2019s password.","title":"A"},{"location":"ui/cacao/glossary/#c","text":"Continuous analysis Continuous analysis is the ability to execute workflows triggered by events, such as data changes or code changes","title":"C"},{"location":"ui/cacao/glossary/#d","text":"Deployment A deployment is one or more resources, such as instances, created on one or more clouds.","title":"D"},{"location":"ui/cacao/glossary/#i","text":"Instance An instance is another name for a virtual machine or server (in Openstack)","title":"I"},{"location":"ui/cacao/glossary/#m","text":"Meta cloud orchestration The ability to provision resources in different types of clouds","title":"M"},{"location":"ui/cacao/glossary/#p","text":"Provider A provider is another name for cloud or cloud provider, where cloud resources are created","title":"P"},{"location":"ui/cacao/glossary/#r","text":"Resource A resource or cloud resource is a general term used for any object \u2013 instance, volume, container \u2013 in a cloud","title":"R"},{"location":"ui/cacao/glossary/#t","text":"Template A template refers to a set of instructions or description of what cloud resources to create on a cloud. A template in Cacao requires a template engine to process the hte instructions or description. Currently, Cacao\u2019s primary templates for OpenStack use Hashicorp Terraform.","title":"T"},{"location":"ui/cacao/glossary/#w","text":"Workspace A workspace is a way to group one or more deployments.","title":"W"},{"location":"ui/cacao/intro/","text":"Cacao Overview \u226b What is Cacao What is Cacao? \u00b6 Cacao is a multi cloud orchestration service for researchers and educators that eliminates the complexity of using multiple clouds. By focusing on getting stuff done, Cacao helps transform research and education in a multi-cloud world. Cacao is built and maintained by CyVerse , the NSF research project that created Atmosphere. Why Cacao? \u00b6 Cacao helps you use clouds with ease of use, flexibility, and collaboration. Ease of use \u00b6 Cacao makes it convenient to use your XSEDE Jetstream 2 allocation on any Jetstream cloud. You can organize your Openstack resources \u2013 servers, volumes, containers, and more \u2013 into workspaces. At the basic level, you can use Cacao just as you did with Atmosphere on Jetstream 1: launch a vm with the tools you use, do your work, and shutdown when you\u2019re done. Flexibility (coming soon!) \u00b6 Cacao helps adapt the cloud to fit your needs. Under the hood, your cloud resources are created using pre-defined Hashicorp Terraform templates . If you don\u2019t know Terraform, don\u2019t worry \u2013 you don\u2019t need to know Terraform to use Cacao; however, you can create your own Openstack Terraform templates if you wish to level-up your Cacao workflow game. Future features in Cacao will include support for AWS/GCP/Azure+Terraform as well as non-Terraform-based templates, such as Kubernetes and Argo templates. Cacao will also allow you to activate your resources when your data or workflow code changes and shutdown your resources after executing your workflow, in a Continuous Analysis way. Collaboration (coming soon!) \u00b6 Cacao makes collaboration in the cloud, less cloudy. You will be able share resources and workflows with the people you want. What next? \u00b6 If you are a new user to Cacao, get started in using Cacao","title":"What is Cacao"},{"location":"ui/cacao/intro/#what-is-cacao","text":"Cacao is a multi cloud orchestration service for researchers and educators that eliminates the complexity of using multiple clouds. By focusing on getting stuff done, Cacao helps transform research and education in a multi-cloud world. Cacao is built and maintained by CyVerse , the NSF research project that created Atmosphere.","title":"What is Cacao?"},{"location":"ui/cacao/intro/#why-cacao","text":"Cacao helps you use clouds with ease of use, flexibility, and collaboration.","title":"Why Cacao?"},{"location":"ui/cacao/intro/#ease-of-use","text":"Cacao makes it convenient to use your XSEDE Jetstream 2 allocation on any Jetstream cloud. You can organize your Openstack resources \u2013 servers, volumes, containers, and more \u2013 into workspaces. At the basic level, you can use Cacao just as you did with Atmosphere on Jetstream 1: launch a vm with the tools you use, do your work, and shutdown when you\u2019re done.","title":"Ease of use"},{"location":"ui/cacao/intro/#flexibility-coming-soon","text":"Cacao helps adapt the cloud to fit your needs. Under the hood, your cloud resources are created using pre-defined Hashicorp Terraform templates . If you don\u2019t know Terraform, don\u2019t worry \u2013 you don\u2019t need to know Terraform to use Cacao; however, you can create your own Openstack Terraform templates if you wish to level-up your Cacao workflow game. Future features in Cacao will include support for AWS/GCP/Azure+Terraform as well as non-Terraform-based templates, such as Kubernetes and Argo templates. Cacao will also allow you to activate your resources when your data or workflow code changes and shutdown your resources after executing your workflow, in a Continuous Analysis way.","title":"Flexibility (coming soon!)"},{"location":"ui/cacao/intro/#collaboration-coming-soon","text":"Cacao makes collaboration in the cloud, less cloudy. You will be able share resources and workflows with the people you want.","title":"Collaboration (coming soon!)"},{"location":"ui/cacao/intro/#what-next","text":"If you are a new user to Cacao, get started in using Cacao","title":"What next?"},{"location":"ui/cacao/login/","text":"Cacao Overview \u226b Logging Into An Instance Logging Into An Instance \u00b6 Cacao provides easy access to your instance after it is launched as part of a deployment: * Command line access via SSH * Command line access using Web Shell * Web Desktop","title":"Logging Into An Instance"},{"location":"ui/cacao/login/#logging-into-an-instance","text":"Cacao provides easy access to your instance after it is launched as part of a deployment: * Command line access via SSH * Command line access using Web Shell * Web Desktop","title":"Logging Into An Instance"},{"location":"ui/cacao/manage/","text":"Instance Management Actions \u00b6 Coming soon!","title":"Instance Management"},{"location":"ui/cacao/manage/#instance-management-actions","text":"Coming soon!","title":"Instance Management Actions"},{"location":"ui/cacao/overview/","text":"Everything you wanted to know about Cacao to do more research in Jetstream2 \u00b6 Intro to Cacao \u00b6 What is Cacao Cacao glossary Cacao UI basics Getting started for new users \u00b6 Getting started for new Cacao users Cacao 101 \u00b6 Credentials Workspaces Deployments Choosing your allocation Logging Into An Instance (coming soon!) Storage (coming soon!) Providers (coming soon!) Templates (coming soon!) Your Accout (coming soon!)","title":"Overview"},{"location":"ui/cacao/overview/#everything-you-wanted-to-know-about-cacao-to-do-more-research-in-jetstream2","text":"","title":"Everything you wanted to know about Cacao to do more research in Jetstream2"},{"location":"ui/cacao/overview/#intro-to-cacao","text":"What is Cacao Cacao glossary Cacao UI basics","title":"Intro to Cacao"},{"location":"ui/cacao/overview/#getting-started-for-new-users","text":"Getting started for new Cacao users","title":"Getting started for new users"},{"location":"ui/cacao/overview/#cacao-101","text":"Credentials Workspaces Deployments Choosing your allocation Logging Into An Instance (coming soon!) Storage (coming soon!) Providers (coming soon!) Templates (coming soon!) Your Accout (coming soon!)","title":"Cacao 101"},{"location":"ui/cacao/storage/","text":"Cacao Overview \u226b Using Storage with Cacao Using Storage with Cacao \u00b6","title":"Storage"},{"location":"ui/cacao/storage/#using-storage-with-cacao","text":"","title":"Using Storage with Cacao"},{"location":"ui/cacao/troubleshooting/","text":"Cacao Troubleshooting \u00b6 Coming soon!","title":"Cacao Troubleshooting"},{"location":"ui/cacao/troubleshooting/#cacao-troubleshooting","text":"Coming soon!","title":"Cacao Troubleshooting"},{"location":"ui/cacao/ui_basics/","text":"Cacao Overview \u226b Cacao UI basics Cacao UI basics \u00b6 Welcome to Cacao, a multi-cloud orchestration service. Cacao helps transform research and education in a multi-cloud world. This guide will cover how to navigate the user interface of Cacao. Sidebar \u00b6 The sidebar is the main navigation for the Cacao user interface. Home \u00b6 The Home view provides a dashboard for your account, including a summary of your providers, allocation, and resources used Deployments \u00b6 Deployments displays your workspaces and deployemnts. Credentials \u00b6 Credentials provides access to your the various credentials used to access clouds and resources, including ssh key and openstack credentials Storage (still baking) \u00b6 Storage displays your storage resources Providers (still baking) \u00b6 Providers displays details about your clouds. Templates (still baking) \u00b6 Templates provides advanced users access to import or share templates they create. Help \u00b6 Help provides access to help resources Your Account \u00b6 Your Account menu shows your account information and access to your settings and preferences.","title":"UI Basics"},{"location":"ui/cacao/ui_basics/#cacao-ui-basics","text":"Welcome to Cacao, a multi-cloud orchestration service. Cacao helps transform research and education in a multi-cloud world. This guide will cover how to navigate the user interface of Cacao.","title":"Cacao UI basics"},{"location":"ui/cacao/ui_basics/#sidebar","text":"The sidebar is the main navigation for the Cacao user interface.","title":"Sidebar"},{"location":"ui/cacao/ui_basics/#home","text":"The Home view provides a dashboard for your account, including a summary of your providers, allocation, and resources used","title":"Home"},{"location":"ui/cacao/ui_basics/#deployments","text":"Deployments displays your workspaces and deployemnts.","title":"Deployments"},{"location":"ui/cacao/ui_basics/#credentials","text":"Credentials provides access to your the various credentials used to access clouds and resources, including ssh key and openstack credentials","title":"Credentials"},{"location":"ui/cacao/ui_basics/#storage-still-baking","text":"Storage displays your storage resources","title":"Storage (still baking)"},{"location":"ui/cacao/ui_basics/#providers-still-baking","text":"Providers displays details about your clouds.","title":"Providers (still baking)"},{"location":"ui/cacao/ui_basics/#templates-still-baking","text":"Templates provides advanced users access to import or share templates they create.","title":"Templates (still baking)"},{"location":"ui/cacao/ui_basics/#help","text":"Help provides access to help resources","title":"Help"},{"location":"ui/cacao/ui_basics/#your-account","text":"Your Account menu shows your account information and access to your settings and preferences.","title":"Your Account"},{"location":"ui/cacao/workspaces/","text":"Cacao Overview \u226b Workspaces Workspaces \u00b6","title":"Workspaces"},{"location":"ui/cacao/workspaces/#workspaces","text":"","title":"Workspaces"},{"location":"ui/cli/clients/","text":"Installing Openstack Clients \u00b6 Mac specific steps Windows specific steps Common/Linux specific steps Mac specific steps \u00b6 This will help you get the Openstack clients working on Mac OS X 10.11.x and higher. It may work on recent older versions of Mac OS X but it has not been tested. At this time, Python 3 is still not shipping on OS X. The latest Openstack clients require it. Follow the instructions below at your own risk. Task Command Install Homebrew on your system yet (this might take a few minutes): /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" Using brew we\u2019re going to install Python 3: brew install python Now Python 3 is installed we can install the OpenStack command line tools: sudo pip3 install python-openstackclient Windows specific steps \u00b6 We recommend that Windows users install Windows Subsystem for Linux and install Ubuntu within it. Microsoft has a learning module for this. The page Enable Windows Subsystem for Linux and install a distribution can walk you through that. Once installed, you can verify python3 is installed by doing: which python3 If you get an error, you may need to install Python3 by doing: sudo apt install python3 python3-pip Then you should be able to proceed to the Linux/common steps below Common/Linux steps \u00b6 Note: Python3 is required . This should already be installed by your operating system. Openstack CLI clients MUST be installed with Python3\u2019s pip/pip3! Task Command Install the OpenStack clients pip install python-openstackclient Additional clients that may also be useful depending on your custom needs are: python-swiftclient, python-heatclient, python-magnumclient, python-manilaclient For current users, clients that you likely no longer need to install are: python-keystoneclient , python-novaclient , python-neutronclient , python-cinderclient , python-glanceclient Set up your OpenStack credentials See Setting up openrc.sh for details. source openrc.sh Test an Open Stack command openstack flavor list Following future OpenStack updates, all installed pip modules can be updated with this command: pip3 list --outdated --format=freeze | grep -v '^\\-e' | cut -d = -f 1 | xargs -n1 pip3 install -U Optional steps \u00b6 Though not strictly necessary, we recommend using virtualenv to increase the stability of the openstack cli tools. Task Command cd to your preferred directory Create a directory for the project mkdir <project_name> Change to the project directory cd <project_name> Install the venv packages sudo python3 -m pip install --user virtualenv Start the VirtualEnvironment software python3 -m venv env <project_name> Activate the VirtualEnvironment for the project source <project_name>/bin/activate","title":"Installing Openstack Clients"},{"location":"ui/cli/clients/#installing-openstack-clients","text":"Mac specific steps Windows specific steps Common/Linux specific steps","title":"Installing Openstack Clients"},{"location":"ui/cli/clients/#mac-specific-steps","text":"This will help you get the Openstack clients working on Mac OS X 10.11.x and higher. It may work on recent older versions of Mac OS X but it has not been tested. At this time, Python 3 is still not shipping on OS X. The latest Openstack clients require it. Follow the instructions below at your own risk. Task Command Install Homebrew on your system yet (this might take a few minutes): /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" Using brew we\u2019re going to install Python 3: brew install python Now Python 3 is installed we can install the OpenStack command line tools: sudo pip3 install python-openstackclient","title":"Mac specific steps"},{"location":"ui/cli/clients/#windows-specific-steps","text":"We recommend that Windows users install Windows Subsystem for Linux and install Ubuntu within it. Microsoft has a learning module for this. The page Enable Windows Subsystem for Linux and install a distribution can walk you through that. Once installed, you can verify python3 is installed by doing: which python3 If you get an error, you may need to install Python3 by doing: sudo apt install python3 python3-pip Then you should be able to proceed to the Linux/common steps below","title":"Windows specific steps"},{"location":"ui/cli/clients/#commonlinux-steps","text":"Note: Python3 is required . This should already be installed by your operating system. Openstack CLI clients MUST be installed with Python3\u2019s pip/pip3! Task Command Install the OpenStack clients pip install python-openstackclient Additional clients that may also be useful depending on your custom needs are: python-swiftclient, python-heatclient, python-magnumclient, python-manilaclient For current users, clients that you likely no longer need to install are: python-keystoneclient , python-novaclient , python-neutronclient , python-cinderclient , python-glanceclient Set up your OpenStack credentials See Setting up openrc.sh for details. source openrc.sh Test an Open Stack command openstack flavor list Following future OpenStack updates, all installed pip modules can be updated with this command: pip3 list --outdated --format=freeze | grep -v '^\\-e' | cut -d = -f 1 | xargs -n1 pip3 install -U","title":"Common/Linux steps"},{"location":"ui/cli/clients/#optional-steps","text":"Though not strictly necessary, we recommend using virtualenv to increase the stability of the openstack cli tools. Task Command cd to your preferred directory Create a directory for the project mkdir <project_name> Change to the project directory cd <project_name> Install the venv packages sudo python3 -m pip install --user virtualenv Start the VirtualEnvironment software python3 -m venv env <project_name> Activate the VirtualEnvironment for the project source <project_name>/bin/activate","title":"Optional steps"},{"location":"ui/cli/deleting/","text":"Deleting items in the CLI \u00b6 When you\u2019re done with a VM, you\u2019ll want to remove the floating IP and return it to the pool if you aren\u2019t going to reuse it. Then you can delete an instance you no longer need. We\u2019re providing instructions for deleting routers, subnets, and networks, as well, though you can keep those and reuse them. Network infrastructure does not incur any SU charges to leave though it will count against Openstack resource quotas. There are a very limited number of IPs available. Please return any you are not using! Clean up what was created above OpenStack Command Remove the public IP address from the instance openstack server remove floating ip your-unneeded-instance your.ip.number.here Return the public IP address to the pool of IP numbers openstack floating ip delete your.ip.number.here Delete an instance openstack server delete your-unneeded-instance Note: You often want to create infrastructure such as networks, subnets, routers, security groups/rules, etc. only once and not delete them. These entities are reusable by all members of your project except for any keypairs you\u2019ve created. Clean up other entities Openstack Command Deleting network infrastructure Disconnect the router from the gateway openstack router unset --external-gateway my-router-name Delete the subnet from the router openstack router remove subnet my-router-name my-subnet-name Delete the router openstack router delete my-router-name Delete the subnet openstack subnet delete my-subnet-name Delete the network openstack network delete my-network-name Deleting security infrastructure Delete a security group rule openstack security group delete secgroup-rule-name Delete a security group openstack security group delete secgroup-name Deleting other items Deleting a keypair openstack keypair delete my-keypair-name","title":"Deleting Infrastructure in the CLI"},{"location":"ui/cli/deleting/#deleting-items-in-the-cli","text":"When you\u2019re done with a VM, you\u2019ll want to remove the floating IP and return it to the pool if you aren\u2019t going to reuse it. Then you can delete an instance you no longer need. We\u2019re providing instructions for deleting routers, subnets, and networks, as well, though you can keep those and reuse them. Network infrastructure does not incur any SU charges to leave though it will count against Openstack resource quotas. There are a very limited number of IPs available. Please return any you are not using! Clean up what was created above OpenStack Command Remove the public IP address from the instance openstack server remove floating ip your-unneeded-instance your.ip.number.here Return the public IP address to the pool of IP numbers openstack floating ip delete your.ip.number.here Delete an instance openstack server delete your-unneeded-instance Note: You often want to create infrastructure such as networks, subnets, routers, security groups/rules, etc. only once and not delete them. These entities are reusable by all members of your project except for any keypairs you\u2019ve created. Clean up other entities Openstack Command Deleting network infrastructure Disconnect the router from the gateway openstack router unset --external-gateway my-router-name Delete the subnet from the router openstack router remove subnet my-router-name my-subnet-name Delete the router openstack router delete my-router-name Delete the subnet openstack subnet delete my-subnet-name Delete the network openstack network delete my-network-name Deleting security infrastructure Delete a security group rule openstack security group delete secgroup-rule-name Delete a security group openstack security group delete secgroup-name Deleting other items Deleting a keypair openstack keypair delete my-keypair-name","title":"Deleting items in the CLI"},{"location":"ui/cli/launch/","text":"Launch and Access Your Instance \u00b6 Creating a Virtual Machine \u00b6 Create and launch a VM OpenStack Command See what flavors (sizes) are available openstack flavor list See what Images are available The Jetstream team makes Featured- * images available from which to build. openstack image list Create and boot an instance Notes: Make sure your SSH keyname matches your-instance-name is the name of the instance; make it something meaningful for you. Choose an appropriate flavor from the list in the first command Choose an appropriate image from the list in the second command openstack server create my-server-name \\ --flavor FLAVOR \\ --image IMAGE-NAME \\ --key-name my-keypair-name \\ --security-group my-security-group-name \\ --wait Optional If you have multiple networks, you\u2019ll need to also include this line See Create a network in the CLI for more information --nic net-id=my-network-name Create a public IP address for an instance openstack floating ip create public Add that public IP address with that instance openstack server add floating ip my-server-name your.ip.number.here Logging into your Virtual Machine \u00b6 Once your instance is up and has a floating IP, you are ready to ssh in and use it. If your ssh key is one of the default names (e.g. id_rsa or id_ed25519) and is in your ~/.ssh dir, you won\u2019t need to specify the location of the key. Otherwise, you\u2019ll need to use the ssh option -i path/key . For example: ssh -i ~/.ssh/my-custom-key-name user@ip.number Each distribution has a different default user. We will show examples for each without the -i path to your ssh key, assuming you have ~/.ssh/id_rsa or ~/.ssh/id_ed25519 as your default key: For Ubuntu 18 or 20: ssh ubuntu@your.ip.number.here For CentOS 7: ssh centos@your.ip.number.here For Rocky 8: ssh rocky@your.ip.number.here You should be able to access and use your VM now! Please see Instance Management Actions in the CLI for all instance management actions.","title":"Launching a Virtual Machine"},{"location":"ui/cli/launch/#launch-and-access-your-instance","text":"","title":"Launch and Access Your Instance"},{"location":"ui/cli/launch/#creating-a-virtual-machine","text":"Create and launch a VM OpenStack Command See what flavors (sizes) are available openstack flavor list See what Images are available The Jetstream team makes Featured- * images available from which to build. openstack image list Create and boot an instance Notes: Make sure your SSH keyname matches your-instance-name is the name of the instance; make it something meaningful for you. Choose an appropriate flavor from the list in the first command Choose an appropriate image from the list in the second command openstack server create my-server-name \\ --flavor FLAVOR \\ --image IMAGE-NAME \\ --key-name my-keypair-name \\ --security-group my-security-group-name \\ --wait Optional If you have multiple networks, you\u2019ll need to also include this line See Create a network in the CLI for more information --nic net-id=my-network-name Create a public IP address for an instance openstack floating ip create public Add that public IP address with that instance openstack server add floating ip my-server-name your.ip.number.here","title":"Creating a Virtual Machine"},{"location":"ui/cli/launch/#logging-into-your-virtual-machine","text":"Once your instance is up and has a floating IP, you are ready to ssh in and use it. If your ssh key is one of the default names (e.g. id_rsa or id_ed25519) and is in your ~/.ssh dir, you won\u2019t need to specify the location of the key. Otherwise, you\u2019ll need to use the ssh option -i path/key . For example: ssh -i ~/.ssh/my-custom-key-name user@ip.number Each distribution has a different default user. We will show examples for each without the -i path to your ssh key, assuming you have ~/.ssh/id_rsa or ~/.ssh/id_ed25519 as your default key: For Ubuntu 18 or 20: ssh ubuntu@your.ip.number.here For CentOS 7: ssh centos@your.ip.number.here For Rocky 8: ssh rocky@your.ip.number.here You should be able to access and use your VM now! Please see Instance Management Actions in the CLI for all instance management actions.","title":"Logging into your Virtual Machine"},{"location":"ui/cli/manage/","text":"Instance Management Actions in the CLI \u00b6 Once you have an instance, you may need to stop, start, or do other actions with it. You may wish to review XSEDE Service Units and Jetstream2 for more information about the \u201ccosts\u201d of the various states of Jetstream2 virtual machines. View the console log \u00b6 Sometimes you may need to look at the console log for troubleshooting purposes or even just to see if the boot completed normally. You can do this with openstack console log show my-server-name-or-UUID Use the web console \u00b6 You can also get a web-based console login if you\u2019ve created an account with a password or set your root password. openstack console url show --spice my-server-name-or-UUID Rebooting \u00b6 If your instance is running and you need to reboot/restart it, you can do so from the instance itself by doing: sudo shutdown -r now You can also do it from the CLI with the command: openstack server reboot my-server-name-or-UUID openstack server reboot my-server-name-or-UUID --hard Stopping and starting \u00b6 Stopping an instance is equivalent to powering down your laptop. We recommend this as a means to conserve SUs when you are done working but plan to use your VM in the next day or two. You can stop the instance from it\u2019s own command line by doing sudo shutdown -h now You can accomplish the same thing from the CLI with: openstack server stop my-server-name-or-UUID and start it again with: openstack server start my-server-name-or-UUID Note that state is not retained and that resources are still reserved on the compute host so that when you decide restart the instance, resources are available to activate the instance. Suspending and resuming \u00b6 Another option for conserving some SUs is suspending your instance. Suspending is similar to closing the lid on your laptop. We generally do not recommend using suspend, but if you opt to use it, we would recommend only using it when you want to conserve some SUs but plan to come back to work with your VM very soon. openstack server suspend my-server-name-or-UUID and then resume it again with: openstack server resume my-server-name-or-UUID Note that resources are still reserved on the compute host for when you decide restart the instance. Shelving and unshelving \u00b6 The last management option is shelving. This shuts down the instance down and moves to storage. Memory state is not maintained. Contents of your root disk are maintained. This is the most economical state for an unused VM as there are no SU charges for shelved VMs since it removes the VM from the hypervisor entirely. We recommend using shelving when you are done with a VM for multiple days or weeks. openstack server shelve my-server-name-or-UUID and unshelve it with: openstack server unshelve my-server-name-or-UUID","title":"Instance Management"},{"location":"ui/cli/manage/#instance-management-actions-in-the-cli","text":"Once you have an instance, you may need to stop, start, or do other actions with it. You may wish to review XSEDE Service Units and Jetstream2 for more information about the \u201ccosts\u201d of the various states of Jetstream2 virtual machines.","title":"Instance Management Actions in the CLI"},{"location":"ui/cli/manage/#view-the-console-log","text":"Sometimes you may need to look at the console log for troubleshooting purposes or even just to see if the boot completed normally. You can do this with openstack console log show my-server-name-or-UUID","title":"View the console log"},{"location":"ui/cli/manage/#use-the-web-console","text":"You can also get a web-based console login if you\u2019ve created an account with a password or set your root password. openstack console url show --spice my-server-name-or-UUID","title":"Use the web console"},{"location":"ui/cli/manage/#rebooting","text":"If your instance is running and you need to reboot/restart it, you can do so from the instance itself by doing: sudo shutdown -r now You can also do it from the CLI with the command: openstack server reboot my-server-name-or-UUID openstack server reboot my-server-name-or-UUID --hard","title":"Rebooting"},{"location":"ui/cli/manage/#stopping-and-starting","text":"Stopping an instance is equivalent to powering down your laptop. We recommend this as a means to conserve SUs when you are done working but plan to use your VM in the next day or two. You can stop the instance from it\u2019s own command line by doing sudo shutdown -h now You can accomplish the same thing from the CLI with: openstack server stop my-server-name-or-UUID and start it again with: openstack server start my-server-name-or-UUID Note that state is not retained and that resources are still reserved on the compute host so that when you decide restart the instance, resources are available to activate the instance.","title":"Stopping and starting"},{"location":"ui/cli/manage/#suspending-and-resuming","text":"Another option for conserving some SUs is suspending your instance. Suspending is similar to closing the lid on your laptop. We generally do not recommend using suspend, but if you opt to use it, we would recommend only using it when you want to conserve some SUs but plan to come back to work with your VM very soon. openstack server suspend my-server-name-or-UUID and then resume it again with: openstack server resume my-server-name-or-UUID Note that resources are still reserved on the compute host for when you decide restart the instance.","title":"Suspending and resuming"},{"location":"ui/cli/manage/#shelving-and-unshelving","text":"The last management option is shelving. This shuts down the instance down and moves to storage. Memory state is not maintained. Contents of your root disk are maintained. This is the most economical state for an unused VM as there are no SU charges for shelved VMs since it removes the VM from the hypervisor entirely. We recommend using shelving when you are done with a VM for multiple days or weeks. openstack server shelve my-server-name-or-UUID and unshelve it with: openstack server unshelve my-server-name-or-UUID","title":"Shelving and unshelving"},{"location":"ui/cli/managing-ssh-keys/","text":"Managing SSH keys from the CLI \u00b6 Secure shell (SSH) keys are crucial to securely accessing your virtual machines. Jetstream2 only supports ssh key access to CLI and Horizon virtual machines. You can only add a key pair to an instance at the time of its creation, not afterwards, so it is important not to overlook this step. You\u2019ll want to have a key pair created before hand and uploaded to the Jetstream2 cloud. While several formats are supported, we recommend using RSA or ED25519 format keys. Source your openrc if you haven\u2019t already! Refer to creating an openrc if you still need to create an openrc file for Jetstream2. We\u2019re assuming it is openrc.sh for this example. source openrc.sh Uploading an existing key \u00b6 If you have an existing SSH key, you can upload it to the Jetstream2 cloud (note: key filenames may vary - this example assumes id_rsa.pub) : cd ~/.ssh openstack keypair create --public-key id_rsa.pub my-api-key Make sure to ONLY upload the public key! You can change the my-api-key to something more descriptive. Create a new key and upload it \u00b6 If you don\u2019t have an SSH key, you\u2019ll want to create a new key and upload it. If you don\u2019t have the .ssh dir, you can create it: mkdir .ssh ; chmod 700 .ssh Then you can create your key ssh-keygen -b 2048 -t rsa -C \"Identifying comment for this ssh key\" -f ~/.ssh/id_rsa or ssh-keygen -o -a 100 -t ed25519 -C \"Identifying comment for this ssh key\" -f ~/.ssh/id_ed25519 The first will create a 2048 bit RSA cryptography key with the comment you specify and the filename id_rsa (which is the default). The second will create an Ed25519 elliptical cryptography key. The comment is optional but we do recommend it to keep track of keys. You only need to do one of these, though you can create both. You may also leave off the -f file option and ssh-keygen will prompt you for the filename. openstack keypair create --public-key id_rsa.pub my-api-key or openstack keypair create --public-key id_ed25519.pub my-api-key Make sure to ONLY upload the public key! You should change the my-api-key to something more descriptive.","title":"Managing SSH Keys"},{"location":"ui/cli/managing-ssh-keys/#managing-ssh-keys-from-the-cli","text":"Secure shell (SSH) keys are crucial to securely accessing your virtual machines. Jetstream2 only supports ssh key access to CLI and Horizon virtual machines. You can only add a key pair to an instance at the time of its creation, not afterwards, so it is important not to overlook this step. You\u2019ll want to have a key pair created before hand and uploaded to the Jetstream2 cloud. While several formats are supported, we recommend using RSA or ED25519 format keys. Source your openrc if you haven\u2019t already! Refer to creating an openrc if you still need to create an openrc file for Jetstream2. We\u2019re assuming it is openrc.sh for this example. source openrc.sh","title":"Managing SSH keys from the CLI"},{"location":"ui/cli/managing-ssh-keys/#uploading-an-existing-key","text":"If you have an existing SSH key, you can upload it to the Jetstream2 cloud (note: key filenames may vary - this example assumes id_rsa.pub) : cd ~/.ssh openstack keypair create --public-key id_rsa.pub my-api-key Make sure to ONLY upload the public key! You can change the my-api-key to something more descriptive.","title":"Uploading an existing key"},{"location":"ui/cli/managing-ssh-keys/#create-a-new-key-and-upload-it","text":"If you don\u2019t have an SSH key, you\u2019ll want to create a new key and upload it. If you don\u2019t have the .ssh dir, you can create it: mkdir .ssh ; chmod 700 .ssh Then you can create your key ssh-keygen -b 2048 -t rsa -C \"Identifying comment for this ssh key\" -f ~/.ssh/id_rsa or ssh-keygen -o -a 100 -t ed25519 -C \"Identifying comment for this ssh key\" -f ~/.ssh/id_ed25519 The first will create a 2048 bit RSA cryptography key with the comment you specify and the filename id_rsa (which is the default). The second will create an Ed25519 elliptical cryptography key. The comment is optional but we do recommend it to keep track of keys. You only need to do one of these, though you can create both. You may also leave off the -f file option and ssh-keygen will prompt you for the filename. openstack keypair create --public-key id_rsa.pub my-api-key or openstack keypair create --public-key id_ed25519.pub my-api-key Make sure to ONLY upload the public key! You should change the my-api-key to something more descriptive.","title":"Create a new key and upload it"},{"location":"ui/cli/manila/","text":"To use Manila via Openstack CLI \u00b6 On a terminal that has the Openstack Clients installed and the appropriate login credentials you can do the following: 1. Create a share \u00b6 openstack share create --name $manila-share-name cephfs $vol-size --os-share-api-version 2.63 Example: openstack share create --name manila-share-cli cephfs 10 --os-share-api-version 2.63 Metadata for the share created above: +---------------------------------------+--------------------------------------+ | Field | Value | +---------------------------------------+--------------------------------------+ | access_rules_status | active | | availability_zone | None | | create_share_from_snapshot_support | False | | created_at | 2022-03-01T03:40:48.468421 | | description | None | | has_replicas | False | | id | 23c511b2-66e7-4986-b6a6-231b490210d4 | | is_public | False | | metadata | {} | | mount_snapshot_support | False | | name | manila-share-cli | | progress | None | | project_id | 55d7efb46dd945a2b86f7ce8aa657e1a | | replication_type | None | | revert_to_snapshot_support | False | | share_group_id | None | | share_network_id | None | | share_proto | CEPHFS | | share_type | de7b9e68-2357-4837-880f-858d7358b05c | | share_type_name | cephfsnativetype | | size | 10 | | snapshot_id | None | | snapshot_support | False | | source_share_group_snapshot_member_id | None | | status | creating | | task_state | None | | user_id | a9e55b395bcb494aaf5938f5f8382e71 | | volume_type | cephfsnativetype | +---------------------------------------+--------------------------------------+ id is the share_id. You can use this to look up information about your share. See step 4. 2. Create an access rule \u00b6 openstack share access create $manila-share-name cephx $anyName --os-share-api-version 2.63 Example: openstack share access create manila-share-cli cephx manilaAccess --os-share-api-version 2.63 Metadata for the access rule: +--------------+--------------------------------------+ | Field | Value | +--------------+--------------------------------------+ | id | 95067b4f-f77c-4b76-be12-ac5c3a8e8897 | | share_id | 23c511b2-66e7-4986-b6a6-231b490210d4 | | access_level | rw | | access_to | manilaAccess | | access_type | cephx | | state | queued_to_apply | | access_key | None | | created_at | 2022-03-01T13:41:09.984126 | | updated_at | None | | properties | | +--------------+--------------------------------------+ Make a note of the id value. This is the access rule id . In the above example it is 95067b4f-f77c-4b76-be12-ac5c3a8e8897 . You can look up the access rule id in openstack to get your access_key . 3. Get access key \u00b6 openstack share access show $access-rule-id --os-share-api-version 2.63 Example: openstack share access show 95067b4f-f77c-4b76-be12-ac5c3a8e8897 --os-share-api-version 2.63 Metadata for the access rule: +--------------+------------------------------------------+ | Field | Value | +--------------+------------------------------------------+ | id | 95067b4f-f77c-4b76-be12-ac5c3a8e8897 | | share_id | 23c511b2-66e7-4986-b6a6-231b490210d4 | | access_level | rw | | access_to | manilaAccess | | access_type | cephx | | state | active | | access_key | AQB2Ih5iyQKpChAAIKunDZ6Ztr1VfNn+AFxlGA== | | created_at | 2022-03-01T13:41:09.984126 | | updated_at | 2022-03-01T13:41:10.227543 | | properties | | +--------------+------------------------------------------+ The access rule is active and you can use the access_key generated above. 4. View share information \u00b6 openstack share show $share_id --os-share-api-version 2.63 Example: openstack share show 23c511b2-66e7-4986-b6a6-231b490210d4 --os-share-api-version 2.63 Metadata for your share will now look a little different: +---------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +---------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------+ | access_rules_status | active | | availability_zone | nova | | create_share_from_snapshot_support | False | | created_at | 2022-03-01T03:40:48.468421 | | description | None | | export_locations | | | | id = 1138760f-2ba4-4d9f-ad8b-312d39c4e4b8 | | | path = 149.165.158.38:6789,149.165.158.22:6789,149.165.158.54:6789,149.165.158.70:6789,149.165.158.86:6789:/volumes/_nogroup/1ca2d54e- | | | 16a5-43b8-90de-75a91c1b96e9/fba3f935-5047-4eef-8b4d-3c27f356c2c7 | | | preferred = False | | has_replicas | False | | id | 23c511b2-66e7-4986-b6a6-231b490210d4 | | is_public | False | | mount_snapshot_support | False | | name | manila-share-cli | | progress | 100% | | project_id | 55d7efb46dd945a2b86f7ce8aa657e1a | | properties | | | replication_type | None | | revert_to_snapshot_support | False | | share_group_id | None | | share_network_id | None | | share_proto | CEPHFS | | share_type | de7b9e68-2357-4837-880f-858d7358b05c | | share_type_name | cephfsnativetype | | size | 10 | | snapshot_id | None | | snapshot_support | False | | source_share_group_snapshot_member_id | None | | status | available | | task_state | None | | user_id | a9e55b395bcb494aaf5938f5f8382e71 | | volume_type | cephfsnativetype | +---------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------+ You will need the path of your export_locations In the above example it is: 149.165.158.38:6789,149.165.158.22:6789,149.165.158.54:6789,149.165.158.70:6789,149.165.158.86:6789:/volumes/_nogroup/1ca2d54e-16a5-43b8-90de-75a91c1b96e9/fba3f935-5047-4eef-8b4d-3c27f356c2c7 Important things to note down: Share id (Step 1) Access rule id (Step 2) Acccess key (Step 3) Export location path (Step 4) Using Manila Share on a VM \u00b6 This is the same whether you\u2019re using Horizon or the CLI. Please refer to Configuring a VM to use Manila Shares","title":"Manila Shares in the CLI"},{"location":"ui/cli/manila/#to-use-manila-via-openstack-cli","text":"On a terminal that has the Openstack Clients installed and the appropriate login credentials you can do the following:","title":"To use Manila via Openstack CLI"},{"location":"ui/cli/manila/#1-create-a-share","text":"openstack share create --name $manila-share-name cephfs $vol-size --os-share-api-version 2.63 Example: openstack share create --name manila-share-cli cephfs 10 --os-share-api-version 2.63 Metadata for the share created above: +---------------------------------------+--------------------------------------+ | Field | Value | +---------------------------------------+--------------------------------------+ | access_rules_status | active | | availability_zone | None | | create_share_from_snapshot_support | False | | created_at | 2022-03-01T03:40:48.468421 | | description | None | | has_replicas | False | | id | 23c511b2-66e7-4986-b6a6-231b490210d4 | | is_public | False | | metadata | {} | | mount_snapshot_support | False | | name | manila-share-cli | | progress | None | | project_id | 55d7efb46dd945a2b86f7ce8aa657e1a | | replication_type | None | | revert_to_snapshot_support | False | | share_group_id | None | | share_network_id | None | | share_proto | CEPHFS | | share_type | de7b9e68-2357-4837-880f-858d7358b05c | | share_type_name | cephfsnativetype | | size | 10 | | snapshot_id | None | | snapshot_support | False | | source_share_group_snapshot_member_id | None | | status | creating | | task_state | None | | user_id | a9e55b395bcb494aaf5938f5f8382e71 | | volume_type | cephfsnativetype | +---------------------------------------+--------------------------------------+ id is the share_id. You can use this to look up information about your share. See step 4.","title":"1. Create a share"},{"location":"ui/cli/manila/#2-create-an-access-rule","text":"openstack share access create $manila-share-name cephx $anyName --os-share-api-version 2.63 Example: openstack share access create manila-share-cli cephx manilaAccess --os-share-api-version 2.63 Metadata for the access rule: +--------------+--------------------------------------+ | Field | Value | +--------------+--------------------------------------+ | id | 95067b4f-f77c-4b76-be12-ac5c3a8e8897 | | share_id | 23c511b2-66e7-4986-b6a6-231b490210d4 | | access_level | rw | | access_to | manilaAccess | | access_type | cephx | | state | queued_to_apply | | access_key | None | | created_at | 2022-03-01T13:41:09.984126 | | updated_at | None | | properties | | +--------------+--------------------------------------+ Make a note of the id value. This is the access rule id . In the above example it is 95067b4f-f77c-4b76-be12-ac5c3a8e8897 . You can look up the access rule id in openstack to get your access_key .","title":"2. Create an access rule"},{"location":"ui/cli/manila/#3-get-access-key","text":"openstack share access show $access-rule-id --os-share-api-version 2.63 Example: openstack share access show 95067b4f-f77c-4b76-be12-ac5c3a8e8897 --os-share-api-version 2.63 Metadata for the access rule: +--------------+------------------------------------------+ | Field | Value | +--------------+------------------------------------------+ | id | 95067b4f-f77c-4b76-be12-ac5c3a8e8897 | | share_id | 23c511b2-66e7-4986-b6a6-231b490210d4 | | access_level | rw | | access_to | manilaAccess | | access_type | cephx | | state | active | | access_key | AQB2Ih5iyQKpChAAIKunDZ6Ztr1VfNn+AFxlGA== | | created_at | 2022-03-01T13:41:09.984126 | | updated_at | 2022-03-01T13:41:10.227543 | | properties | | +--------------+------------------------------------------+ The access rule is active and you can use the access_key generated above.","title":"3. Get access key"},{"location":"ui/cli/manila/#4-view-share-information","text":"openstack share show $share_id --os-share-api-version 2.63 Example: openstack share show 23c511b2-66e7-4986-b6a6-231b490210d4 --os-share-api-version 2.63 Metadata for your share will now look a little different: +---------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +---------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------+ | access_rules_status | active | | availability_zone | nova | | create_share_from_snapshot_support | False | | created_at | 2022-03-01T03:40:48.468421 | | description | None | | export_locations | | | | id = 1138760f-2ba4-4d9f-ad8b-312d39c4e4b8 | | | path = 149.165.158.38:6789,149.165.158.22:6789,149.165.158.54:6789,149.165.158.70:6789,149.165.158.86:6789:/volumes/_nogroup/1ca2d54e- | | | 16a5-43b8-90de-75a91c1b96e9/fba3f935-5047-4eef-8b4d-3c27f356c2c7 | | | preferred = False | | has_replicas | False | | id | 23c511b2-66e7-4986-b6a6-231b490210d4 | | is_public | False | | mount_snapshot_support | False | | name | manila-share-cli | | progress | 100% | | project_id | 55d7efb46dd945a2b86f7ce8aa657e1a | | properties | | | replication_type | None | | revert_to_snapshot_support | False | | share_group_id | None | | share_network_id | None | | share_proto | CEPHFS | | share_type | de7b9e68-2357-4837-880f-858d7358b05c | | share_type_name | cephfsnativetype | | size | 10 | | snapshot_id | None | | snapshot_support | False | | source_share_group_snapshot_member_id | None | | status | available | | task_state | None | | user_id | a9e55b395bcb494aaf5938f5f8382e71 | | volume_type | cephfsnativetype | +---------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------+ You will need the path of your export_locations In the above example it is: 149.165.158.38:6789,149.165.158.22:6789,149.165.158.54:6789,149.165.158.70:6789,149.165.158.86:6789:/volumes/_nogroup/1ca2d54e-16a5-43b8-90de-75a91c1b96e9/fba3f935-5047-4eef-8b4d-3c27f356c2c7 Important things to note down: Share id (Step 1) Access rule id (Step 2) Acccess key (Step 3) Export location path (Step 4)","title":"4. View share information"},{"location":"ui/cli/manila/#using-manila-share-on-a-vm","text":"This is the same whether you\u2019re using Horizon or the CLI. Please refer to Configuring a VM to use Manila Shares","title":"Using Manila Share on a VM"},{"location":"ui/cli/network/","text":"Create a network in the CLI \u00b6 In Jetstream2, if you do not need multiple networks in your allocation, you can skip all of the steps below. However, if there\u2019s more than one network in your allocation already, you will either need to use one of those or create your own network using these steps. This step creates a virtual network for your instances. You\u2019ll create a network, a subnet, and a router and then set the routing so your instances can talk outside of the Jetstream2 networks to the world. You can have multiple networks in an allocation or also multiple subnets on a single network. This effectively isolates instances from each other. If you do not give your instances a public IP address (also called a floating ip by Openstack), your instances will not be reachable from the internet. While this is not a substitute for proper security groups , it is another level of protection for VMs that may used internally only by your VMs (e.g. private database servers). Setup the network OpenStack Command Create a private network openstack network create my-network-name Verify that the private network was created openstack network list Create a subnet within the private network space openstack subnet create --network my-network-name --subnet-range 10.0.0.0/24 my-subnet-name Verify that subnet was created openstack subnet list Create a router openstack router create my-router-name Connect the newly created subnet to the router openstack router add subnet my-router-name my-subnet-name Connect the router to the gateway named \u201cpublic\u201d openstack router set --external-gateway public my-router-name Verify that the router has been connected to the gateway openstack router show my-router-name","title":"Setting up a Network"},{"location":"ui/cli/network/#create-a-network-in-the-cli","text":"In Jetstream2, if you do not need multiple networks in your allocation, you can skip all of the steps below. However, if there\u2019s more than one network in your allocation already, you will either need to use one of those or create your own network using these steps. This step creates a virtual network for your instances. You\u2019ll create a network, a subnet, and a router and then set the routing so your instances can talk outside of the Jetstream2 networks to the world. You can have multiple networks in an allocation or also multiple subnets on a single network. This effectively isolates instances from each other. If you do not give your instances a public IP address (also called a floating ip by Openstack), your instances will not be reachable from the internet. While this is not a substitute for proper security groups , it is another level of protection for VMs that may used internally only by your VMs (e.g. private database servers). Setup the network OpenStack Command Create a private network openstack network create my-network-name Verify that the private network was created openstack network list Create a subnet within the private network space openstack subnet create --network my-network-name --subnet-range 10.0.0.0/24 my-subnet-name Verify that subnet was created openstack subnet list Create a router openstack router create my-router-name Connect the newly created subnet to the router openstack router add subnet my-router-name my-subnet-name Connect the router to the gateway named \u201cpublic\u201d openstack router set --external-gateway public my-router-name Verify that the router has been connected to the gateway openstack router show my-router-name","title":"Create a network in the CLI"},{"location":"ui/cli/openrc/","text":"Setting up application credentials and openrc.sh for the Jetstream2 CLI \u00b6 New openrc format for Jetstream2 CLI! \u00b6 One of the key changes to using Jetstream2\u2019s command line interface (CLI) is that it uses XSEDE credentials for authentication. To do that, you have to create an application credential via the Horizon interface. This will require a different sort of openrc than Jetstream1. This page will walk you through that process. Please make sure to source the new Jetstream2 openrc in a fresh terminal session. If you invoke it in a session that\u2019s had another openrc sourced, you\u2019ll get an error like this: Error authenticating with application credential: Application credentials cannot request a scope. Openrc files are allocation-specific \u00b6 Each allocation you wish to use from the command line will need its own application credential and openrc file. You CANNOT use the openrc generator like in Jetstream1 \u00b6 The openrc generator on the Horizon right side (username) menu will NOT work properly with Jetstream2! Please use the process below to get your application credential based openrc file. Using the Horizon dashboard to generate openrc.sh \u00b6 Action Screenshot Navigate to https://js2.jetstream-cloud.org Make sure it says \"XSEDE Globus Auth\" in the Authenticate Using box. The first time you log in you'll be directed to a Globus page to permit authorization. If you have linked institutional, Google, Orcid, or other credentials, you'll be able to use those to authenticate. We know XSEDE credentials work correctly so we will show that in our example. The next page should be the login screen for your credentials. We're showing the XSEDE login screen as our example. If you're using two-factor auth with your credentials as XSEDE does, you'll likely get a Duo or Authenticator screen here. You should be at the Horizon Dashboard home now. As application credentials are unique to each allocation, if you are on multiple XSEDE allocations, you'll want to verify you're using the correct one and change to the correct one if you are not. You do that by clicking at the top left next to the Jetstream2 logo where it has \"XSEDE * TG-XXXXXXXXX * IU\". That will show allocations under \"Projects\". From here, you'll select Identity and then Application Credentials from the sidebar menu on the left Once on that page, you'll click \"Create Application Credential\" towards the top right (noted by the red arrow) This will bring up the application credential creation screen. The sidebar has descriptions if you need help. We recommend using a descriptive name and to put details in the description so you can easily see what it is for. The Secret is the password or passphrase. We recommend using a strong password here or multi-word passphrase. As the page notes, you will not be able to retrieve it later if you forget it or delete the openrc file you generate. Set the expiration date and time. If you do not set a date, it will default to TODAY as noted on the sidebar. We do not recommend setting the roles, access rules, or selecting unrestricted unless you are an advanced user and understand the full implications of altering these. When you hit \"Create Application Credential\" it will then generate the credential and bring up a confirmation box. Please make sure to save the credential ID and secret if you need them for things other than the openrc. To get the openrc for CLI access, please click the \"Download openrc file\" button referenced by the red arrow in the screenshot. That will download a plain text file for use with the Openstack CLI client We recommend giving your new openrc file a descriptive name (e.g. openrc-TG-TRA111111.sh, using the XSEDE project name or some other meaningful description.)","title":"Setting up openrc.sh"},{"location":"ui/cli/openrc/#setting-up-application-credentials-and-openrcsh-for-the-jetstream2-cli","text":"","title":"Setting up application credentials and openrc.sh for the Jetstream2 CLI"},{"location":"ui/cli/openrc/#new-openrc-format-for-jetstream2-cli","text":"One of the key changes to using Jetstream2\u2019s command line interface (CLI) is that it uses XSEDE credentials for authentication. To do that, you have to create an application credential via the Horizon interface. This will require a different sort of openrc than Jetstream1. This page will walk you through that process. Please make sure to source the new Jetstream2 openrc in a fresh terminal session. If you invoke it in a session that\u2019s had another openrc sourced, you\u2019ll get an error like this: Error authenticating with application credential: Application credentials cannot request a scope.","title":"New openrc format for Jetstream2 CLI!"},{"location":"ui/cli/openrc/#openrc-files-are-allocation-specific","text":"Each allocation you wish to use from the command line will need its own application credential and openrc file.","title":"Openrc files are allocation-specific"},{"location":"ui/cli/openrc/#you-cannot-use-the-openrc-generator-like-in-jetstream1","text":"The openrc generator on the Horizon right side (username) menu will NOT work properly with Jetstream2! Please use the process below to get your application credential based openrc file.","title":"You CANNOT use the openrc generator like in Jetstream1"},{"location":"ui/cli/openrc/#using-the-horizon-dashboard-to-generate-openrcsh","text":"Action Screenshot Navigate to https://js2.jetstream-cloud.org Make sure it says \"XSEDE Globus Auth\" in the Authenticate Using box. The first time you log in you'll be directed to a Globus page to permit authorization. If you have linked institutional, Google, Orcid, or other credentials, you'll be able to use those to authenticate. We know XSEDE credentials work correctly so we will show that in our example. The next page should be the login screen for your credentials. We're showing the XSEDE login screen as our example. If you're using two-factor auth with your credentials as XSEDE does, you'll likely get a Duo or Authenticator screen here. You should be at the Horizon Dashboard home now. As application credentials are unique to each allocation, if you are on multiple XSEDE allocations, you'll want to verify you're using the correct one and change to the correct one if you are not. You do that by clicking at the top left next to the Jetstream2 logo where it has \"XSEDE * TG-XXXXXXXXX * IU\". That will show allocations under \"Projects\". From here, you'll select Identity and then Application Credentials from the sidebar menu on the left Once on that page, you'll click \"Create Application Credential\" towards the top right (noted by the red arrow) This will bring up the application credential creation screen. The sidebar has descriptions if you need help. We recommend using a descriptive name and to put details in the description so you can easily see what it is for. The Secret is the password or passphrase. We recommend using a strong password here or multi-word passphrase. As the page notes, you will not be able to retrieve it later if you forget it or delete the openrc file you generate. Set the expiration date and time. If you do not set a date, it will default to TODAY as noted on the sidebar. We do not recommend setting the roles, access rules, or selecting unrestricted unless you are an advanced user and understand the full implications of altering these. When you hit \"Create Application Credential\" it will then generate the credential and bring up a confirmation box. Please make sure to save the credential ID and secret if you need them for things other than the openrc. To get the openrc for CLI access, please click the \"Download openrc file\" button referenced by the red arrow in the screenshot. That will download a plain text file for use with the Openstack CLI client We recommend giving your new openrc file a descriptive name (e.g. openrc-TG-TRA111111.sh, using the XSEDE project name or some other meaningful description.)","title":"Using the Horizon dashboard to generate openrc.sh"},{"location":"ui/cli/overview/","text":"Command Line Interface (CLI) Overview \u00b6 There are many options and tools for using the OpenStack API from the command line. Follow the instructions in the table below to set up a security policy and network, launch and manage a VM and then remove the entire structure. Note: There is also an API tutorial that the Jetstream team uses here: Jetstream API Tutorial - this tutorial goes into greater detail on some topics and may be of value to those learning the Openstack CLI. Please note that the tutorial above presently reflects using the API on Jetstream1. It will be updated soon for Jetstream2 Important Notes \u00b6 It is possible to create entities with the same name; e.g. you could create two networks with the same name; however, they will have different Universally Unique Indentifiers (UUIDs). When this occurs you may get a cryptic error message about that entity may not exist or that there are multiple entities with that name. In this case, you must address the entity by its UUID. It is important to understand that everything is owned by the project. Other users in your project can see and manipulate entities that you have created. Be careful in your naming and pay attention to the things you are manipulating. Getting Started \u00b6 You should be running the latest version of the clients. We recommend using python-openstack >= 5.0 as it uses Python3 and removes the dependencies on the now deprecated Python2. See Installing Openstack Clients for more information. The next thing you\u2019ll need to do before being able to do any CLI commands is have an appropriate openrc file. Please note that openrc files on Jetstream2 require application credentials . Please refer to the Jetstream2 Openrc File page for information on generating an application credential and openrc file. Source the openrc: \u00b6 source openrc-file-name You can also make the output look nicer in your terminal with the \u2013fit-width option: openstack image show Featured-Ubuntu20 --fit-width You can make that permanent by adding the following to your environment. export CLIFF_FIT_WIDTH=1 You\u2019ll then need to create a security group and network before launching your first VMs. More information may be found here: Create a security group Create a network Create and launch a VM Instance management","title":"Overview"},{"location":"ui/cli/overview/#command-line-interface-cli-overview","text":"There are many options and tools for using the OpenStack API from the command line. Follow the instructions in the table below to set up a security policy and network, launch and manage a VM and then remove the entire structure. Note: There is also an API tutorial that the Jetstream team uses here: Jetstream API Tutorial - this tutorial goes into greater detail on some topics and may be of value to those learning the Openstack CLI. Please note that the tutorial above presently reflects using the API on Jetstream1. It will be updated soon for Jetstream2","title":"Command Line Interface (CLI) Overview"},{"location":"ui/cli/overview/#important-notes","text":"It is possible to create entities with the same name; e.g. you could create two networks with the same name; however, they will have different Universally Unique Indentifiers (UUIDs). When this occurs you may get a cryptic error message about that entity may not exist or that there are multiple entities with that name. In this case, you must address the entity by its UUID. It is important to understand that everything is owned by the project. Other users in your project can see and manipulate entities that you have created. Be careful in your naming and pay attention to the things you are manipulating.","title":"Important Notes"},{"location":"ui/cli/overview/#getting-started","text":"You should be running the latest version of the clients. We recommend using python-openstack >= 5.0 as it uses Python3 and removes the dependencies on the now deprecated Python2. See Installing Openstack Clients for more information. The next thing you\u2019ll need to do before being able to do any CLI commands is have an appropriate openrc file. Please note that openrc files on Jetstream2 require application credentials . Please refer to the Jetstream2 Openrc File page for information on generating an application credential and openrc file.","title":"Getting Started"},{"location":"ui/cli/overview/#source-the-openrc","text":"source openrc-file-name You can also make the output look nicer in your terminal with the \u2013fit-width option: openstack image show Featured-Ubuntu20 --fit-width You can make that permanent by adding the following to your environment. export CLIFF_FIT_WIDTH=1 You\u2019ll then need to create a security group and network before launching your first VMs. More information may be found here: Create a security group Create a network Create and launch a VM Instance management","title":"Source the openrc:"},{"location":"ui/cli/security_group/","text":"Manage Security Groups in the CLI \u00b6 Security groups can be thought of like firewalls. They ultimately control inbound and outbound traffic to your virtual machines. Under the CLI and Horizon, access defaults to all outbound allowed and NO inbound allowed. To allow access to your VM for things like SSH, you will need to create a security group and add rules to it. You can reuse a security group many times, so a best practice is to create groups by related services. For insance, you might create a basic group for ssh and icmp (which is what we will show as an example) and then a separate security group for http and https access if you\u2019re running a web service on your instance. Create a security group that will enable inbound ping & SSH: \u00b6 This will walk you through creating a basic security group on the command line and adding a couple of simple access rules. openstack security group create --description \"ssh and icmp enabled\" my-username-ssh-and-icmp-access This creates the security group named my-username-ssh-and-icmp-access with the description noted above. It becomes the container for holding security group rules. openstack security group rule create --protocol tcp --dst-port 22:22 --remote-ip 0.0.0.0/0 my-username-ssh-and-icmp-access This creates an SSH access rule, allowing inbound TCP protocol to port 22 from any IP number. openstack security group rule create --protocol icmp my-username-ssh-and-icmp-access This creates an ICMP (most notably ping) access rule, allowing inbound ICMP protocol from any IP number. You can then add additional rules or additional security groups. This will allow the most basic of access to your VMs. We do recommend limiting access as much as possible for best security practices. Please refer to the Openstack docs for more information: Openstack Security Groups Also, as with all openstack CLI commands, you can type openstack help item to get more information on the options and syntax.","title":"Setting up a Security Group"},{"location":"ui/cli/security_group/#manage-security-groups-in-the-cli","text":"Security groups can be thought of like firewalls. They ultimately control inbound and outbound traffic to your virtual machines. Under the CLI and Horizon, access defaults to all outbound allowed and NO inbound allowed. To allow access to your VM for things like SSH, you will need to create a security group and add rules to it. You can reuse a security group many times, so a best practice is to create groups by related services. For insance, you might create a basic group for ssh and icmp (which is what we will show as an example) and then a separate security group for http and https access if you\u2019re running a web service on your instance.","title":"Manage Security Groups in the CLI"},{"location":"ui/cli/security_group/#create-a-security-group-that-will-enable-inbound-ping-ssh","text":"This will walk you through creating a basic security group on the command line and adding a couple of simple access rules. openstack security group create --description \"ssh and icmp enabled\" my-username-ssh-and-icmp-access This creates the security group named my-username-ssh-and-icmp-access with the description noted above. It becomes the container for holding security group rules. openstack security group rule create --protocol tcp --dst-port 22:22 --remote-ip 0.0.0.0/0 my-username-ssh-and-icmp-access This creates an SSH access rule, allowing inbound TCP protocol to port 22 from any IP number. openstack security group rule create --protocol icmp my-username-ssh-and-icmp-access This creates an ICMP (most notably ping) access rule, allowing inbound ICMP protocol from any IP number. You can then add additional rules or additional security groups. This will allow the most basic of access to your VMs. We do recommend limiting access as much as possible for best security practices. Please refer to the Openstack docs for more information: Openstack Security Groups Also, as with all openstack CLI commands, you can type openstack help item to get more information on the options and syntax.","title":"Create a security group that will enable inbound ping &amp; SSH:"},{"location":"ui/cli/storage/","text":"Using Storage Under the CLI \u00b6 All allocations receive 1 TB of storage alloation by default. You can use this storage quota in the form of volumes or manila shares. [LINK MANILA INFO HERE] All of the commands below assume that you have an Openstack Client installed and a working openrc file. To view any volumes you might have: openstack volume list To create a 10 GB volume, you can do: openstack volume create --size 10 my-10GVolume Then you can attach it to an instance for use: openstack server add volume VM-Name-Or-UUID Volume-Name-Or-UUID While you can usually assume it will be the next mounted disk (root should be /dev/sdaX in all cases on Jetstream2), you can look on your instance to see where the volume attached by doing: dmesg | grep sd The output of that should usually look something like this: [root@my-vm ~]# dmesg | grep sd [ 1.715421] sd 2:0:0:0: [sda] 16777216 512-byte logical blocks: (8.58 GB/8.00 GiB) [ 1.718439] sd 2:0:0:0: [sda] Write Protect is off [ 1.720066] sd 2:0:0:0: [sda] Mode Sense: 63 00 00 08 [ 1.720455] sd 2:0:0:0: [sda] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA [ 1.725878] sda: sda1 [ 1.727563] sd 2:0:0:0: [sda] Attached SCSI disk [ 2.238056] XFS (sda1): Mounting V5 Filesystem [ 2.410020] XFS (sda1): Ending clean mount [ 7.997131] Installing knfsd (copyright (C) 1996 okir@monad.swb.de). [ 8.539042] sd 2:0:0:0: Attached scsi generic sg0 type 0 [ 8.687877] fbcon: cirrusdrmfb (fb0) is primary device [ 8.719492] cirrus 0000:00:02.0: fb0: cirrusdrmfb frame buffer device [ 246.622485] sd 2:0:0:1: Attached scsi generic sg1 type 0 [ 246.633569] sd 2:0:0:1: [sdb] 20971520 512-byte logical blocks: (10.7 GB/10.0 GiB) [ 246.667567] sd 2:0:0:1: [sdb] Write Protect is off [ 246.667923] sd 2:0:0:1: [sdb] Mode Sense: 63 00 00 08 [ 246.678696] sd 2:0:0:1: [sdb] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA [ 246.793574] sd 2:0:0:1: [sdb] Attached SCSI disk From your instance, you can now create a mount point, view the device, and create the filesystem (using ext4 as the filesystem type for this example). mkdir /vol_b fdisk -l /dev/sdb mkfs.ext4 /dev/sdb If you get the following warning, it\u2019s safe to hit \u2018y\u2019 to proceed. /dev/sdb is entire device, not just one partition! Proceed anyway? (y,n) mount -o noatime /dev/sdb /vol_b Assuming you didn\u2019t get any errors, /dev/sdb should now be mounted on /vol_b Note: Linux has a special mount option for file systems called noatime. If this option is set for a file system in /etc/fstab, then reading accesses will no longer cause the atime information (last access time - don\u2019t mix this up with the last modified time - if a file is changed, the modification date will still be set) that is associated with a file to be updated (in reverse this means that if noatime is not set, each read access will also result in a write operation). Therefore, using noatime can lead to significant performance gains. [root@my-vm ~]# touch /vol_b/foo [root@my-vm ~]# ls -la /vol_b/ total 24 drwxr-xr-x 3 root root 4096 Jul 13 13:37 . dr-xr-xr-x. 18 root root 4096 Jul 13 11:50 .. -rw-r--r-- 1 root root 0 Jul 13 13:37 foo drwx------ 2 root root 16384 Jul 13 13:36 lost+found [root@my-vm ~]# df -h Filesystem Size Used Avail Use% Mounted on /dev/sda1 8.0G 2.4G 5.7G 30% / devtmpfs 902M 0 902M 0% /dev tmpfs 920M 0 920M 0% /dev/shm tmpfs 920M 17M 904M 2% /run tmpfs 920M 0 920M 0% /sys/fs/cgroup tmpfs 184M 0 184M 0% /run/user/0 /dev/sdb 9.8G 37M 9.2G 1% /vol_b To make the volume mount persist, you can add an entry to /etc/fstab similar to this: /dev/sdb /vol_b ext4 defaults,noatime 0 0 You would need to change as needed for a different device id, mount point, and file system type. We do recommend using the noatime option as shown in the example. Once you are done with your volume or want to use it with another VM, if you are not shutting down the VM, you\u2019ll need to unmount it. umount /vol_b To detach it from the VM, you\u2019ll do: openstack server remove volume VM-Name-Or-UUID Volume-Name-Or-UUID Doing an openstack volume list now should show the volume as available: +--------------------------------------+------------------+------------+------+------------------------------+ | ID | Display Name | Status | Size | Attached to | +--------------------------------------+------------------+------------+------+------------------------------+ | af59d4fa-ced2-4049-a062-7b2a15807b7f | my-10GVolume | available | 10 | | +--------------------------------------+------------------+------------+------+------------------------------+ If you want to completely destroy a volume, you can do: openstack volume delete Volume-Name-Or-UUID","title":"Storage Under the CLI"},{"location":"ui/cli/storage/#using-storage-under-the-cli","text":"All allocations receive 1 TB of storage alloation by default. You can use this storage quota in the form of volumes or manila shares. [LINK MANILA INFO HERE] All of the commands below assume that you have an Openstack Client installed and a working openrc file. To view any volumes you might have: openstack volume list To create a 10 GB volume, you can do: openstack volume create --size 10 my-10GVolume Then you can attach it to an instance for use: openstack server add volume VM-Name-Or-UUID Volume-Name-Or-UUID While you can usually assume it will be the next mounted disk (root should be /dev/sdaX in all cases on Jetstream2), you can look on your instance to see where the volume attached by doing: dmesg | grep sd The output of that should usually look something like this: [root@my-vm ~]# dmesg | grep sd [ 1.715421] sd 2:0:0:0: [sda] 16777216 512-byte logical blocks: (8.58 GB/8.00 GiB) [ 1.718439] sd 2:0:0:0: [sda] Write Protect is off [ 1.720066] sd 2:0:0:0: [sda] Mode Sense: 63 00 00 08 [ 1.720455] sd 2:0:0:0: [sda] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA [ 1.725878] sda: sda1 [ 1.727563] sd 2:0:0:0: [sda] Attached SCSI disk [ 2.238056] XFS (sda1): Mounting V5 Filesystem [ 2.410020] XFS (sda1): Ending clean mount [ 7.997131] Installing knfsd (copyright (C) 1996 okir@monad.swb.de). [ 8.539042] sd 2:0:0:0: Attached scsi generic sg0 type 0 [ 8.687877] fbcon: cirrusdrmfb (fb0) is primary device [ 8.719492] cirrus 0000:00:02.0: fb0: cirrusdrmfb frame buffer device [ 246.622485] sd 2:0:0:1: Attached scsi generic sg1 type 0 [ 246.633569] sd 2:0:0:1: [sdb] 20971520 512-byte logical blocks: (10.7 GB/10.0 GiB) [ 246.667567] sd 2:0:0:1: [sdb] Write Protect is off [ 246.667923] sd 2:0:0:1: [sdb] Mode Sense: 63 00 00 08 [ 246.678696] sd 2:0:0:1: [sdb] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA [ 246.793574] sd 2:0:0:1: [sdb] Attached SCSI disk From your instance, you can now create a mount point, view the device, and create the filesystem (using ext4 as the filesystem type for this example). mkdir /vol_b fdisk -l /dev/sdb mkfs.ext4 /dev/sdb If you get the following warning, it\u2019s safe to hit \u2018y\u2019 to proceed. /dev/sdb is entire device, not just one partition! Proceed anyway? (y,n) mount -o noatime /dev/sdb /vol_b Assuming you didn\u2019t get any errors, /dev/sdb should now be mounted on /vol_b Note: Linux has a special mount option for file systems called noatime. If this option is set for a file system in /etc/fstab, then reading accesses will no longer cause the atime information (last access time - don\u2019t mix this up with the last modified time - if a file is changed, the modification date will still be set) that is associated with a file to be updated (in reverse this means that if noatime is not set, each read access will also result in a write operation). Therefore, using noatime can lead to significant performance gains. [root@my-vm ~]# touch /vol_b/foo [root@my-vm ~]# ls -la /vol_b/ total 24 drwxr-xr-x 3 root root 4096 Jul 13 13:37 . dr-xr-xr-x. 18 root root 4096 Jul 13 11:50 .. -rw-r--r-- 1 root root 0 Jul 13 13:37 foo drwx------ 2 root root 16384 Jul 13 13:36 lost+found [root@my-vm ~]# df -h Filesystem Size Used Avail Use% Mounted on /dev/sda1 8.0G 2.4G 5.7G 30% / devtmpfs 902M 0 902M 0% /dev tmpfs 920M 0 920M 0% /dev/shm tmpfs 920M 17M 904M 2% /run tmpfs 920M 0 920M 0% /sys/fs/cgroup tmpfs 184M 0 184M 0% /run/user/0 /dev/sdb 9.8G 37M 9.2G 1% /vol_b To make the volume mount persist, you can add an entry to /etc/fstab similar to this: /dev/sdb /vol_b ext4 defaults,noatime 0 0 You would need to change as needed for a different device id, mount point, and file system type. We do recommend using the noatime option as shown in the example. Once you are done with your volume or want to use it with another VM, if you are not shutting down the VM, you\u2019ll need to unmount it. umount /vol_b To detach it from the VM, you\u2019ll do: openstack server remove volume VM-Name-Or-UUID Volume-Name-Or-UUID Doing an openstack volume list now should show the volume as available: +--------------------------------------+------------------+------------+------+------------------------------+ | ID | Display Name | Status | Size | Attached to | +--------------------------------------+------------------+------------+------+------------------------------+ | af59d4fa-ced2-4049-a062-7b2a15807b7f | my-10GVolume | available | 10 | | +--------------------------------------+------------------+------------+------+------------------------------+ If you want to completely destroy a volume, you can do: openstack volume delete Volume-Name-Or-UUID","title":"Using Storage Under the CLI"},{"location":"ui/cli/troubleshooting/","text":"CLI Troubleshooting \u00b6 I get an error requesting scope when authenticating and doing openstack commands \u00b6 If you source your openrc in a session that\u2019s had another openrc sourced from another cloud (inluding Jetstream1), you\u2019ll get an error like this: Error authenticating with application credential: Application credentials cannot request a scope. (HTTP 401) (Request-ID: Please make sure to source the new Jetstream2 openrc in a fresh terminal session. If you\u2019re feeling adventurous, you can do something like this to remove all previous OpenStack environment variables: while read -r varname; do unset \"$varname\"; done < <(env | grep ^OS_ | cut -d '=' -f1) Please do that at your own risk. I get an error saying my request requires authentication after sourcing my application credential openrc. \u00b6 If your application credential secret in the openrc contains some punctuation/special characters, you might see an error like this: The request you have made requires authentication. (HTTP 401) (Request-ID: For example, if you had an ampersand in your credential password, it may have gotten escaped to \\&amp\\; instead of just the ampersand character. The same could happen for less than or greater than signs and potentially other special characters. Double check the openrc to verify that that has not happened. View the console log \u00b6 Sometimes you may need to look at the console log for troubleshooting purposes or even just to see if the boot completed normally. You can do this with openstack console log show my-server-name-or-UUID","title":"CLI Troubleshooting"},{"location":"ui/cli/troubleshooting/#cli-troubleshooting","text":"","title":"CLI Troubleshooting"},{"location":"ui/cli/troubleshooting/#i-get-an-error-requesting-scope-when-authenticating-and-doing-openstack-commands","text":"If you source your openrc in a session that\u2019s had another openrc sourced from another cloud (inluding Jetstream1), you\u2019ll get an error like this: Error authenticating with application credential: Application credentials cannot request a scope. (HTTP 401) (Request-ID: Please make sure to source the new Jetstream2 openrc in a fresh terminal session. If you\u2019re feeling adventurous, you can do something like this to remove all previous OpenStack environment variables: while read -r varname; do unset \"$varname\"; done < <(env | grep ^OS_ | cut -d '=' -f1) Please do that at your own risk.","title":"I get an error requesting scope when authenticating and doing openstack commands"},{"location":"ui/cli/troubleshooting/#i-get-an-error-saying-my-request-requires-authentication-after-sourcing-my-application-credential-openrc","text":"If your application credential secret in the openrc contains some punctuation/special characters, you might see an error like this: The request you have made requires authentication. (HTTP 401) (Request-ID: For example, if you had an ampersand in your credential password, it may have gotten escaped to \\&amp\\; instead of just the ampersand character. The same could happen for less than or greater than signs and potentially other special characters. Double check the openrc to verify that that has not happened.","title":"I get an error saying my request requires authentication after sourcing my application credential openrc."},{"location":"ui/cli/troubleshooting/#view-the-console-log","text":"Sometimes you may need to look at the console log for troubleshooting purposes or even just to see if the boot completed normally. You can do this with openstack console log show my-server-name-or-UUID","title":"View the console log"},{"location":"ui/exo/create_instance/","text":"Creating an Instance with Exosphere \u00b6 Once you have logged in and selected an allocation, select \u201cCreate\u201d and then \u201cInstance\u201d. Choose an Instance Source \u00b6 Next, choose an instance source. If you are a new user, select your desired operating system \u201cBy Type\u201d. Action Screenshot Choose by Type Select your preferred operating system, or if you don\u2019t have a preference, pick the newest Ubuntu version. Exosphere selects the latest official image for your chosen type automatically. Choose by Image Alternatively, if you want to specify a particular image to create an instance from, select the \u201cBy Image\u201d tab. Here, you can browse the entire Jetstream2 image catalog. Configure Instance \u00b6 Next, you can select options your new instance. If you\u2019re unsure what to choose for any of these, you can leave it at the default. When you\u2019re done choosing options, click the \u201cCreate\u201d button at the bottom. Action Screenshot Choose a Name If you will use the instance for anything important, resist the urge to accept the randomly-generated name. Give it a descriptive name to distinguish it from other instances later. Choose a Flavor Exosphere selects the smallest flavor by default. This is good for exploring and development work, because it conserves your Jetstream2 allocation. Select a larger-size flavor if the smallest flavor proves too small for your explorations, or if you are ready to scale a workload up to many CPU cores / many GB of RAM. Note that larger flavors will consume your allocation (SUs) more quickly. Select a GPU flavor if you require a GPU (and your Jetstream2 allocation provides GPU access). You can always resize your instance to a different flavor later. Choose a Root Disk Size If the default size for the selected flavor is too small for your needs, you can specify a larger custom disk size. This will create a volume-backed instance . Choose a Quantity You can create multiple instances simultaneously, up to the maximum quantity your quota limits support. When you create multiple instances at a time, and \u201cX of Y\u201d will be appended to the name of each. Otherwise, they will all will receive the same configuration. Decide Whether to Enable Web Desktop Enable Web Desktop if you need to use graphical software on your instance, or if you prefer working in a graphical environment instead of a text-based shell (terminal). Note that the graphical desktop environment consumes slightly more resources (CPU and RAM) than a shell, so consider using at least the m3.small flavor for a more responsive experience. Advanced Options \u00b6 Most people can skip these advanced options, and just click the \u201cCreate\u201d button at the bottom. Advanced options are intended for power users and less-common use cases. Action Screenshot Install Operating System Updates By default, new instances install operating system security and bug-fix updates when they first power on. This takes a couple of minutes. So, only skip these if you really want the fastest possible setup and you do not care about the security of your instance. This option does not disable unattended upgrades , which may still run in the background shortly after your instance powers on. Deploy Guacamole for Easy Remote Access By default, Exosphere instances provide easy remote access via a Guacamole -based terminal (Web Shell), and optionally a graphical environment (Web Desktop). If you don\u2019t want these for some reason, and you are comfortable using native SSH to connect to your instance, then you can disable setup of the Guacamole components. Choose a Network By default, Exosphere will use the network topology that OpenStack automatically creates for your allocation (and ask OpenStack to create it if needed). If you want your instance connected to a different OpenStack network, you can choose that here. If you change this without knowing what you\u2019re doing, your instance may end up with broken network connectivity. Assign a Public IP Address By default, Exosphere assigns a public (floating) IP address to your instances. Right now, a public IP is required for the Guacamole-based interactions (Web Shell and Desktop) to work. It is also required for you to make a direct native SSH connection to your instance from your workstation (or from anywhere else outside Jetstream2). Still, you may be creating a cluster of instances, and you may not want all of the cluster nodes to have public IP addresses. In this and similar cases, you can disable the assignment of a public IP address here. If you disable the public IP address for an instance, the only way to connect to that instance will be via native SSH, from one of your other instances that does have a public IP address . SSH Public Key If you want to make a native SSH connection to your instance using public key-based authentication , you can select your public SSH key here, and upload a new one if needed. Note that you do not need to use key-based authentication to connect to an instance via native SSH. Regardless of your choice here, Exosphere will also set a strong passphrase for the exouser user. You can view this passphrase (and use it to SSH in) after instance setup is complete. Boot Script Here you can see how the sausage is made! The term \u201cboot script\u201d is a slight over-simplification: this text area contains cloud-init configuration represented in YAML format, which Exosphere passes to the new instance when it is first powered on. It is generally best to leave this text area alone, create the instance, then log into it and make further configuration changes as needed (e.g. installing software and downloading data). Most of what you see here is important for Exosphere\u2019s interactions and other features to work with the new instance. So, if you make changes without knowing what you\u2019re doing, the instance setup may not complete. This will leave the instance in a partially working, partially broken state from Exosphere\u2019s perspective. In this case, Jetstream2 support staff would likely advise you to delete it and create another instance with this field left un-modified. Still, if you are not afraid of editing YAML, you can modify this configuration before clicking the \u201cCreate\u201d button. Note that Exosphere templates out a few important variables which are enclosed in single curly braces ( { and } ) in this configuration data. Finally, click the \u201cCreate\u201d button at the bottom, and Exosphere will set up your instance.","title":"Creating an Instance"},{"location":"ui/exo/create_instance/#creating-an-instance-with-exosphere","text":"Once you have logged in and selected an allocation, select \u201cCreate\u201d and then \u201cInstance\u201d.","title":"Creating an Instance with Exosphere"},{"location":"ui/exo/create_instance/#choose-an-instance-source","text":"Next, choose an instance source. If you are a new user, select your desired operating system \u201cBy Type\u201d. Action Screenshot Choose by Type Select your preferred operating system, or if you don\u2019t have a preference, pick the newest Ubuntu version. Exosphere selects the latest official image for your chosen type automatically. Choose by Image Alternatively, if you want to specify a particular image to create an instance from, select the \u201cBy Image\u201d tab. Here, you can browse the entire Jetstream2 image catalog.","title":"Choose an Instance Source"},{"location":"ui/exo/create_instance/#configure-instance","text":"Next, you can select options your new instance. If you\u2019re unsure what to choose for any of these, you can leave it at the default. When you\u2019re done choosing options, click the \u201cCreate\u201d button at the bottom. Action Screenshot Choose a Name If you will use the instance for anything important, resist the urge to accept the randomly-generated name. Give it a descriptive name to distinguish it from other instances later. Choose a Flavor Exosphere selects the smallest flavor by default. This is good for exploring and development work, because it conserves your Jetstream2 allocation. Select a larger-size flavor if the smallest flavor proves too small for your explorations, or if you are ready to scale a workload up to many CPU cores / many GB of RAM. Note that larger flavors will consume your allocation (SUs) more quickly. Select a GPU flavor if you require a GPU (and your Jetstream2 allocation provides GPU access). You can always resize your instance to a different flavor later. Choose a Root Disk Size If the default size for the selected flavor is too small for your needs, you can specify a larger custom disk size. This will create a volume-backed instance . Choose a Quantity You can create multiple instances simultaneously, up to the maximum quantity your quota limits support. When you create multiple instances at a time, and \u201cX of Y\u201d will be appended to the name of each. Otherwise, they will all will receive the same configuration. Decide Whether to Enable Web Desktop Enable Web Desktop if you need to use graphical software on your instance, or if you prefer working in a graphical environment instead of a text-based shell (terminal). Note that the graphical desktop environment consumes slightly more resources (CPU and RAM) than a shell, so consider using at least the m3.small flavor for a more responsive experience.","title":"Configure Instance"},{"location":"ui/exo/create_instance/#advanced-options","text":"Most people can skip these advanced options, and just click the \u201cCreate\u201d button at the bottom. Advanced options are intended for power users and less-common use cases. Action Screenshot Install Operating System Updates By default, new instances install operating system security and bug-fix updates when they first power on. This takes a couple of minutes. So, only skip these if you really want the fastest possible setup and you do not care about the security of your instance. This option does not disable unattended upgrades , which may still run in the background shortly after your instance powers on. Deploy Guacamole for Easy Remote Access By default, Exosphere instances provide easy remote access via a Guacamole -based terminal (Web Shell), and optionally a graphical environment (Web Desktop). If you don\u2019t want these for some reason, and you are comfortable using native SSH to connect to your instance, then you can disable setup of the Guacamole components. Choose a Network By default, Exosphere will use the network topology that OpenStack automatically creates for your allocation (and ask OpenStack to create it if needed). If you want your instance connected to a different OpenStack network, you can choose that here. If you change this without knowing what you\u2019re doing, your instance may end up with broken network connectivity. Assign a Public IP Address By default, Exosphere assigns a public (floating) IP address to your instances. Right now, a public IP is required for the Guacamole-based interactions (Web Shell and Desktop) to work. It is also required for you to make a direct native SSH connection to your instance from your workstation (or from anywhere else outside Jetstream2). Still, you may be creating a cluster of instances, and you may not want all of the cluster nodes to have public IP addresses. In this and similar cases, you can disable the assignment of a public IP address here. If you disable the public IP address for an instance, the only way to connect to that instance will be via native SSH, from one of your other instances that does have a public IP address . SSH Public Key If you want to make a native SSH connection to your instance using public key-based authentication , you can select your public SSH key here, and upload a new one if needed. Note that you do not need to use key-based authentication to connect to an instance via native SSH. Regardless of your choice here, Exosphere will also set a strong passphrase for the exouser user. You can view this passphrase (and use it to SSH in) after instance setup is complete. Boot Script Here you can see how the sausage is made! The term \u201cboot script\u201d is a slight over-simplification: this text area contains cloud-init configuration represented in YAML format, which Exosphere passes to the new instance when it is first powered on. It is generally best to leave this text area alone, create the instance, then log into it and make further configuration changes as needed (e.g. installing software and downloading data). Most of what you see here is important for Exosphere\u2019s interactions and other features to work with the new instance. So, if you make changes without knowing what you\u2019re doing, the instance setup may not complete. This will leave the instance in a partially working, partially broken state from Exosphere\u2019s perspective. In this case, Jetstream2 support staff would likely advise you to delete it and create another instance with this field left un-modified. Still, if you are not afraid of editing YAML, you can modify this configuration before clicking the \u201cCreate\u201d button. Note that Exosphere templates out a few important variables which are enclosed in single curly braces ( { and } ) in this configuration data. Finally, click the \u201cCreate\u201d button at the bottom, and Exosphere will set up your instance.","title":"Advanced Options"},{"location":"ui/exo/exo-filetransfer/","text":"Transferring files with web shell and web desktop \u00b6 Files can be transferred to the remote computer by dragging and dropping the files into your browser window, or through using the file browser located in the Guacamole menu. The Guacamole menu is a sidebar which is hidden until explicitly shown. On a desktop or other device which has a hardware keyboard, you can show this menu by pressing Ctrl+Alt+Shift and clicking on the screen. I f you are using a mobile or touchscreen device that lacks a keyboard, you can also show the menu by swiping right from the left edge of the screen. To hide the menu, you press Ctrl+Alt+Shift again or swipe left across the screen. You can select where to transfer files by clicking the \u201cDevices\u201d option in the Guacamole menu. That will give you a file manager type menu where you can select the directory to transfer files into. Note: You will only be able to read or write files in directories that your user account would have access to. This means unless you\u2019ve created directories where you can write, by default you\u2019ll only be able to write into your home directory, /tmp, and mounted volumes. Double-clicking on any directory will change the current location of the file browser to that directory, updating the list of files shown as well as the \u201cbreadcrumbs\u201d at the top of the file browser. Clicking on any of the directory names listed in the breadcrumbs will bring you back to that directory, and clicking on the drive icon on the far left will bring you all the way back to the root level. Downloads are initiated by double-clicking on any file shown, while uploads are initiated by clicking the \u201cUpload Files\u201d button. Clicking \u201cUpload Files\u201d will open a file browsing dialog where you can choose one or more files from your local computer, ultimately uploading the selected files to the directory currently displayed within the file browser. You can also drag and drop files into the dialog box from your computer\u2019s file windows. The state of all file uploads can be observed within the notification dialog that appears once an upload begins, and can be cleared once completed by clicking the \u201cClear\u201d button. Downloads are tracked through your browser\u2019s own download notification system. Note: To use web desktop you need to enable web desktop while creating the VM","title":"File Transfer"},{"location":"ui/exo/exo-filetransfer/#transferring-files-with-web-shell-and-web-desktop","text":"Files can be transferred to the remote computer by dragging and dropping the files into your browser window, or through using the file browser located in the Guacamole menu. The Guacamole menu is a sidebar which is hidden until explicitly shown. On a desktop or other device which has a hardware keyboard, you can show this menu by pressing Ctrl+Alt+Shift and clicking on the screen. I f you are using a mobile or touchscreen device that lacks a keyboard, you can also show the menu by swiping right from the left edge of the screen. To hide the menu, you press Ctrl+Alt+Shift again or swipe left across the screen. You can select where to transfer files by clicking the \u201cDevices\u201d option in the Guacamole menu. That will give you a file manager type menu where you can select the directory to transfer files into. Note: You will only be able to read or write files in directories that your user account would have access to. This means unless you\u2019ve created directories where you can write, by default you\u2019ll only be able to write into your home directory, /tmp, and mounted volumes. Double-clicking on any directory will change the current location of the file browser to that directory, updating the list of files shown as well as the \u201cbreadcrumbs\u201d at the top of the file browser. Clicking on any of the directory names listed in the breadcrumbs will bring you back to that directory, and clicking on the drive icon on the far left will bring you all the way back to the root level. Downloads are initiated by double-clicking on any file shown, while uploads are initiated by clicking the \u201cUpload Files\u201d button. Clicking \u201cUpload Files\u201d will open a file browsing dialog where you can choose one or more files from your local computer, ultimately uploading the selected files to the directory currently displayed within the file browser. You can also drag and drop files into the dialog box from your computer\u2019s file windows. The state of all file uploads can be observed within the notification dialog that appears once an upload begins, and can be cleared once completed by clicking the \u201cClear\u201d button. Downloads are tracked through your browser\u2019s own download notification system. Note: To use web desktop you need to enable web desktop while creating the VM","title":"Transferring files with web shell and web desktop"},{"location":"ui/exo/exo/","text":"Exosphere \u00b6 Exosphere strives to be the most user-friendly interface for research clouds. If you are new to Jetstream2 and unsure which interface to use, Exosphere is a great place to start. Whether you are exploring new software tools, running compute-intensive jobs, teaching a course/workshop, or building a science gateway, Exosphere can likely help you reach your goals. Use Exosphere in your web browser at https://jetstream2.exosphere.app/ . Right now, Exosphere supports creating and managing instances. Instances are virtual computers (a.k.a. servers) that run your code, containers, software, and services. When you create an instance, you get helpful features like: A one-click web shell (terminal) in your browser Optionally, a one-click desktop environment for running graphical software A browser-based file upload/download tool Resource usage graphs that show how hard your instance is working Easy passphrase-based SSH access, if you want it You can also use volumes to store large data sets, and manage persistent IP addresses for servers and science gateways. More powerful features, like data science workbenches and workflow sharing, are in experimental status now. With Exosphere, there is no requirement to learn about advanced cloud technologies like virtual networks or SSH keypairs. If your use of Jetstream2 becomes more sophisticated, and you need to reach for more complex tools like the OpenStack CLI or APIs, Exosphere does not get in your way. How Exosphere compares with the Horizon dashboard, OpenStack CLI, and APIs \u00b6 Exosphere supports users who wish to mix their use of Exosphere with other OpenStack interfaces like Horizon dashboard, the OpenStack command-line interface, and the APIs. Generally (and with a few limitations), resources that you create in one interface will show up in other interfaces. They are merely different ways to manage the same infrastructure. The other OpenStack interfaces support more features of OpenStack that Exosphere doesn\u2019t (yet), like Heat for cluster orchestration and Swift for object storage. So, they may better support some advanced cloud use cases than Exosphere, but they are generally less accessible to newer users. The Horizon dashboard, the OpenStack CLI, and the APIs were all built for use by IT engineers, not by researchers and data scientists. For example, in any of these tools you must create a network, subnet, router, security group, and SSH keypair before you can create an instance and connect to it (using an SSH client program and your private SSH key). If your use cases grow sophisticated enough, you may need this lower-level control, but using the Horizon dashboard is sort of like driving a car with a manual transmission. Using the CLI feels somewhat like using Horizon, but now you\u2019re shifting gears by typing shell commands instead of clicking buttons. Using the OpenStack APIs directly is like building your own transmission for the car. Instances created via these other tools do not get a one-click shell, desktop, data upload/download tool, or any of the other interactions that Exosphere sets up for you. If you want these with the other OpenStack interfaces, you must set them up yourself with varying degrees of difficulty. How Exosphere compares with Atmosphere2 \u00b6 This section will be populated once Atmosphere2 is ready enough to explore and compare. Getting Help \u00b6 In addition to the XSEDE ticketing system, there is an #exosphere-user-support channel in the Jetstream Slack workspace. To request access, please open a ticket from the XSEDE user portal. This support option includes no promise of immediate, real-time assistance, but the Exosphere core developers monitor it and help when they can. Sometimes it\u2019s easier to chat with them than wait for ticket notifications. Contributing \u00b6 If you\u2019d like to help build Exosphere or request a new feature, visit the project on GitLab . The Exosphere maintainers strive to provide a welcoming experience for new contributors. At a broader level than Jetstream, the Exosphere project has a chat room on Matrix/Element which is used to coordinate development work, but community members are also welcome to join. Further, the Exosphere team discusses project progress and priorities on a weekly video call on Mondays at 16:00 UTC. You can join at https://meet.jit.si/exosphere or dial in at +1.512.647.1431, PIN: 3037 7824 88#. (Find the agenda and notes from previous meetings here .)","title":"Overview"},{"location":"ui/exo/exo/#exosphere","text":"Exosphere strives to be the most user-friendly interface for research clouds. If you are new to Jetstream2 and unsure which interface to use, Exosphere is a great place to start. Whether you are exploring new software tools, running compute-intensive jobs, teaching a course/workshop, or building a science gateway, Exosphere can likely help you reach your goals. Use Exosphere in your web browser at https://jetstream2.exosphere.app/ . Right now, Exosphere supports creating and managing instances. Instances are virtual computers (a.k.a. servers) that run your code, containers, software, and services. When you create an instance, you get helpful features like: A one-click web shell (terminal) in your browser Optionally, a one-click desktop environment for running graphical software A browser-based file upload/download tool Resource usage graphs that show how hard your instance is working Easy passphrase-based SSH access, if you want it You can also use volumes to store large data sets, and manage persistent IP addresses for servers and science gateways. More powerful features, like data science workbenches and workflow sharing, are in experimental status now. With Exosphere, there is no requirement to learn about advanced cloud technologies like virtual networks or SSH keypairs. If your use of Jetstream2 becomes more sophisticated, and you need to reach for more complex tools like the OpenStack CLI or APIs, Exosphere does not get in your way.","title":"Exosphere"},{"location":"ui/exo/exo/#how-exosphere-compares-with-the-horizon-dashboard-openstack-cli-and-apis","text":"Exosphere supports users who wish to mix their use of Exosphere with other OpenStack interfaces like Horizon dashboard, the OpenStack command-line interface, and the APIs. Generally (and with a few limitations), resources that you create in one interface will show up in other interfaces. They are merely different ways to manage the same infrastructure. The other OpenStack interfaces support more features of OpenStack that Exosphere doesn\u2019t (yet), like Heat for cluster orchestration and Swift for object storage. So, they may better support some advanced cloud use cases than Exosphere, but they are generally less accessible to newer users. The Horizon dashboard, the OpenStack CLI, and the APIs were all built for use by IT engineers, not by researchers and data scientists. For example, in any of these tools you must create a network, subnet, router, security group, and SSH keypair before you can create an instance and connect to it (using an SSH client program and your private SSH key). If your use cases grow sophisticated enough, you may need this lower-level control, but using the Horizon dashboard is sort of like driving a car with a manual transmission. Using the CLI feels somewhat like using Horizon, but now you\u2019re shifting gears by typing shell commands instead of clicking buttons. Using the OpenStack APIs directly is like building your own transmission for the car. Instances created via these other tools do not get a one-click shell, desktop, data upload/download tool, or any of the other interactions that Exosphere sets up for you. If you want these with the other OpenStack interfaces, you must set them up yourself with varying degrees of difficulty.","title":"How Exosphere compares with the Horizon dashboard, OpenStack CLI, and APIs"},{"location":"ui/exo/exo/#how-exosphere-compares-with-atmosphere2","text":"This section will be populated once Atmosphere2 is ready enough to explore and compare.","title":"How Exosphere compares with Atmosphere2"},{"location":"ui/exo/exo/#getting-help","text":"In addition to the XSEDE ticketing system, there is an #exosphere-user-support channel in the Jetstream Slack workspace. To request access, please open a ticket from the XSEDE user portal. This support option includes no promise of immediate, real-time assistance, but the Exosphere core developers monitor it and help when they can. Sometimes it\u2019s easier to chat with them than wait for ticket notifications.","title":"Getting Help"},{"location":"ui/exo/exo/#contributing","text":"If you\u2019d like to help build Exosphere or request a new feature, visit the project on GitLab . The Exosphere maintainers strive to provide a welcoming experience for new contributors. At a broader level than Jetstream, the Exosphere project has a chat room on Matrix/Element which is used to coordinate development work, but community members are also welcome to join. Further, the Exosphere team discusses project progress and priorities on a weekly video call on Mondays at 16:00 UTC. You can join at https://meet.jit.si/exosphere or dial in at +1.512.647.1431, PIN: 3037 7824 88#. (Find the agenda and notes from previous meetings here .)","title":"Contributing"},{"location":"ui/exo/login/","text":"Exosphere: Logging In \u00b6 Using XSEDE Account (single sign-on) \u00b6 The default login method uses your XSEDE account credentials. Note that this may require multi-factor (Duo) authentication. (If you need help setting up or changing your multi-factor authentication method, please open an XSEDE ticket, as Jetstream2 staff cannot fix this for you directly.) Choosing Allocations and Regions \u00b6 After you log in with your XSEDE credentials, Exosphere will prompt you to select from the allocations that you are a member of. Any un-selected allocations will not be added the Exosphere interface, so select all that you may want to use. If you are granted access to a Jetstream2 regional cloud, you will be logged into those allocations in both the main (Indiana University) region and any other regions. Logging into non-Jetstream Clouds in Exosphere \u00b6 The Exosphere interface for Jetstream2 also allows you to manage resources on other OpenStack-based research clouds alongside Jetstream2. In order for this to work, these third-party clouds must expose their OpenStack APIs publicly, and you must have OpenStack credentials (or an OpenRC file) to provide. To add other clouds, choose \u201cAdd allocation\u201d, select \u201cOther login methods\u201d, and pick the \u201cOpenStack\u201d login method. If you encounter difficulty adding non-Jetstream2 clouds to Exosphere, Jetstream2 staff will have limited ability to troubleshoot and help, so this capability is not guaranteed to work.","title":"Logging In"},{"location":"ui/exo/login/#exosphere-logging-in","text":"","title":"Exosphere: Logging In"},{"location":"ui/exo/login/#using-xsede-account-single-sign-on","text":"The default login method uses your XSEDE account credentials. Note that this may require multi-factor (Duo) authentication. (If you need help setting up or changing your multi-factor authentication method, please open an XSEDE ticket, as Jetstream2 staff cannot fix this for you directly.)","title":"Using XSEDE Account (single sign-on)"},{"location":"ui/exo/login/#choosing-allocations-and-regions","text":"After you log in with your XSEDE credentials, Exosphere will prompt you to select from the allocations that you are a member of. Any un-selected allocations will not be added the Exosphere interface, so select all that you may want to use. If you are granted access to a Jetstream2 regional cloud, you will be logged into those allocations in both the main (Indiana University) region and any other regions.","title":"Choosing Allocations and Regions"},{"location":"ui/exo/login/#logging-into-non-jetstream-clouds-in-exosphere","text":"The Exosphere interface for Jetstream2 also allows you to manage resources on other OpenStack-based research clouds alongside Jetstream2. In order for this to work, these third-party clouds must expose their OpenStack APIs publicly, and you must have OpenStack credentials (or an OpenRC file) to provide. To add other clouds, choose \u201cAdd allocation\u201d, select \u201cOther login methods\u201d, and pick the \u201cOpenStack\u201d login method. If you encounter difficulty adding non-Jetstream2 clouds to Exosphere, Jetstream2 staff will have limited ability to troubleshoot and help, so this capability is not guaranteed to work.","title":"Logging into non-Jetstream Clouds in Exosphere"},{"location":"ui/exo/manage/","text":"Instance Management Actions \u00b6 In Exosphere, to see available management actions for an instance and choose one if desired, click the \u201cActions\u201d drop-down menu on the instance details page.","title":"Instance Management"},{"location":"ui/exo/manage/#instance-management-actions","text":"In Exosphere, to see available management actions for an instance and choose one if desired, click the \u201cActions\u201d drop-down menu on the instance details page.","title":"Instance Management Actions"},{"location":"ui/exo/push-button-cluster/","text":"Push-button Clusters using Exosphere \u00b6 This guide assumes that you are comfortable creating instances using Exosphere. If you are new to Exosphere, see Exosphere: Overview . Exosphere makes it simple to create a virtual Slurm cluster, complete with automatic, elastic scaling of compute nodes. This helps you get started with parallel computing, and to scale up your cluster as your needs grow. To learn more about virtual clusters, as well as other ways to create them, see Advanced Capabilities: Virtual Clusters on Jetstream2 . Enable Experimental Features in Settings \u00b6 Open the Settings page (top menu). Ensure that the \u201cExperimental Features\u201d option is enabled. Create a Cluster Head Node Instance \u00b6 Select \u201cCreate\u201d and then \u201cInstance\u201d. For instance source type select Rocky Linux 8 . Choose a unique name (preferably a short name with no spaces or special characters) for your instance. Ensure there is no existing instance with the same name by looking at the Exosphere instance list page and pressing the \u201cClear filters\u201d button. This will be the base name for all resources created for the cluster, including networks, SSH keys, compute nodes, etc. For flavor choose \u201cm1.small\u201d. Expand \u201cAdvanced Options\u201d. Select an SSH key (highly recommended). You should see an option \u201cCreate your own Slurm cluster with this instance as the head node\u201d. Select \u201cYes\u201d for this option. Click the \u201cCreate\u201d button underneath the Boot Script text input area. Wait until the instance is ready - this could take up to half-an-hour. Run a Job on the Cluster \u00b6 SSH into the new instance as exouser . (Alternatively use the web shell.) Switch to the rocky user: sudo su - rocky Go to the cluster repository directory: cd ~/CRI_Jetstream_Cluster/ Submit a test batch job: sbatch slurm_test.job View the live SLURM logs: sudo tail -f /var/log/slurm/slurm_elastic.log /var/log/slurm/slurmctld.log You should see messages indicating a new compute node being created as an OpenStack instance. Confirm the new compute node instance exists by refreshing Exosphere\u2019s instance list page. Its name should begin with the same name as your head node instance, and end with \u201c-compute-0\u201d. Once the job is finished confirm that the compute node is gone by refreshing Exosphere\u2019s instance list page. Check the output of the test batch job by finding a new file ending with .out in the directory where you ran sbatch and viewing its contents. The file should contain two copies of the hostname of the compute node. For example: ls -alt *.out cat nodes_1.out You should see something like this: $ ls -alt *.out -rw-rw-r--. 1 exouser exouser 72 Jan 28 19:10 nodes_1.out $ cat nodes_1.out yourclustername-compute-0.novalocal yourclustername-compute-0.novalocal You now have your very own working Slurm cluster. Congratulations! Clean-up steps \u00b6 Once you\u2019re done with your cluster, and you want to get rid of the head node instance as well as all the OpenStack resources created for the cluster, run the following commands on the head node instance (in an SSH session or web shell): sudo su - rocky cd ~/CRI_Jetstream_Cluster ./cluster_destroy_local.sh -d Your SSH or web shell session on the head node will be terminated, and you will be disconnected. Confirm that the head node instance is gone by refreshing Exosphere\u2019s instance list page.","title":"Push-button clusters"},{"location":"ui/exo/push-button-cluster/#push-button-clusters-using-exosphere","text":"This guide assumes that you are comfortable creating instances using Exosphere. If you are new to Exosphere, see Exosphere: Overview . Exosphere makes it simple to create a virtual Slurm cluster, complete with automatic, elastic scaling of compute nodes. This helps you get started with parallel computing, and to scale up your cluster as your needs grow. To learn more about virtual clusters, as well as other ways to create them, see Advanced Capabilities: Virtual Clusters on Jetstream2 .","title":"Push-button Clusters using Exosphere"},{"location":"ui/exo/push-button-cluster/#enable-experimental-features-in-settings","text":"Open the Settings page (top menu). Ensure that the \u201cExperimental Features\u201d option is enabled.","title":"Enable Experimental Features in Settings"},{"location":"ui/exo/push-button-cluster/#create-a-cluster-head-node-instance","text":"Select \u201cCreate\u201d and then \u201cInstance\u201d. For instance source type select Rocky Linux 8 . Choose a unique name (preferably a short name with no spaces or special characters) for your instance. Ensure there is no existing instance with the same name by looking at the Exosphere instance list page and pressing the \u201cClear filters\u201d button. This will be the base name for all resources created for the cluster, including networks, SSH keys, compute nodes, etc. For flavor choose \u201cm1.small\u201d. Expand \u201cAdvanced Options\u201d. Select an SSH key (highly recommended). You should see an option \u201cCreate your own Slurm cluster with this instance as the head node\u201d. Select \u201cYes\u201d for this option. Click the \u201cCreate\u201d button underneath the Boot Script text input area. Wait until the instance is ready - this could take up to half-an-hour.","title":"Create a Cluster Head Node Instance"},{"location":"ui/exo/push-button-cluster/#run-a-job-on-the-cluster","text":"SSH into the new instance as exouser . (Alternatively use the web shell.) Switch to the rocky user: sudo su - rocky Go to the cluster repository directory: cd ~/CRI_Jetstream_Cluster/ Submit a test batch job: sbatch slurm_test.job View the live SLURM logs: sudo tail -f /var/log/slurm/slurm_elastic.log /var/log/slurm/slurmctld.log You should see messages indicating a new compute node being created as an OpenStack instance. Confirm the new compute node instance exists by refreshing Exosphere\u2019s instance list page. Its name should begin with the same name as your head node instance, and end with \u201c-compute-0\u201d. Once the job is finished confirm that the compute node is gone by refreshing Exosphere\u2019s instance list page. Check the output of the test batch job by finding a new file ending with .out in the directory where you ran sbatch and viewing its contents. The file should contain two copies of the hostname of the compute node. For example: ls -alt *.out cat nodes_1.out You should see something like this: $ ls -alt *.out -rw-rw-r--. 1 exouser exouser 72 Jan 28 19:10 nodes_1.out $ cat nodes_1.out yourclustername-compute-0.novalocal yourclustername-compute-0.novalocal You now have your very own working Slurm cluster. Congratulations!","title":"Run a Job on the Cluster"},{"location":"ui/exo/push-button-cluster/#clean-up-steps","text":"Once you\u2019re done with your cluster, and you want to get rid of the head node instance as well as all the OpenStack resources created for the cluster, run the following commands on the head node instance (in an SSH session or web shell): sudo su - rocky cd ~/CRI_Jetstream_Cluster ./cluster_destroy_local.sh -d Your SSH or web shell session on the head node will be terminated, and you will be disconnected. Confirm that the head node instance is gone by refreshing Exosphere\u2019s instance list page.","title":"Clean-up steps"},{"location":"ui/exo/storage/","text":"Using Storage Under Exosphere \u00b6 Volumes \u00b6 In Exosphere, you can create volumes and attach them to instances. Exosphere will display the volume mount point in the user interfaces. Please note that attached volumes are mounted the first time they are accessed inside the instance (e.g. with a cd command). File Shares \u00b6 Support for shared filesystems (using OpenStack Manila) is coming soon.","title":"Storage Under Exosphere"},{"location":"ui/exo/storage/#using-storage-under-exosphere","text":"","title":"Using Storage Under Exosphere"},{"location":"ui/exo/storage/#volumes","text":"In Exosphere, you can create volumes and attach them to instances. Exosphere will display the volume mount point in the user interfaces. Please note that attached volumes are mounted the first time they are accessed inside the instance (e.g. with a cd command).","title":"Volumes"},{"location":"ui/exo/storage/#file-shares","text":"Support for shared filesystems (using OpenStack Manila) is coming soon.","title":"File Shares"},{"location":"ui/exo/troubleshooting/","text":"Exosphere Troubleshooting \u00b6 I changed the exouser password on my instance and now the web shell and web desktop in Exosphere don\u2019t work. \u00b6 The way Jetstream2 is currently architected, all users on an allocation have access to all resources on the allocation. By default, Exosphere hides some resources created by other users, but this is only a convenience and it cannot assure separation of access. It is possible to make it less straightforward for another user on the same allocation to access your running instance. You can do this by changing the password for the default exouser account. Changing the password does not prevent any access, but makes it more difficult. Currently, changing the exouser account password will break Web Shell, Web Desktop, and some other Exosphere-powered instance interactions. This may change in the future. We HIGHLY suggest utilizing ssh keys for your instances to ensure you have access. That\u2019s covered here: https://docs.jetstream-cloud.org/ui/exo/create_instance/ under the advanced options. You can also manually add your key to an already running instance. We generally recommend using only ssh keys for external access and not changing the exouser password.","title":"Exosphere Troubleshooting"},{"location":"ui/exo/troubleshooting/#exosphere-troubleshooting","text":"","title":"Exosphere Troubleshooting"},{"location":"ui/exo/troubleshooting/#i-changed-the-exouser-password-on-my-instance-and-now-the-web-shell-and-web-desktop-in-exosphere-dont-work","text":"The way Jetstream2 is currently architected, all users on an allocation have access to all resources on the allocation. By default, Exosphere hides some resources created by other users, but this is only a convenience and it cannot assure separation of access. It is possible to make it less straightforward for another user on the same allocation to access your running instance. You can do this by changing the password for the default exouser account. Changing the password does not prevent any access, but makes it more difficult. Currently, changing the exouser account password will break Web Shell, Web Desktop, and some other Exosphere-powered instance interactions. This may change in the future. We HIGHLY suggest utilizing ssh keys for your instances to ensure you have access. That\u2019s covered here: https://docs.jetstream-cloud.org/ui/exo/create_instance/ under the advanced options. You can also manually add your key to an already running instance. We generally recommend using only ssh keys for external access and not changing the exouser password.","title":"I changed the exouser password on my instance and now the web shell and web desktop in Exosphere don't work."},{"location":"ui/horizon/intro/","text":"Horizon \u00b6 Horizon is a web-based GUI for interacting with the Openstack API. It is likely the most complete GUI for working with Openstack but isn\u2019t the fastest or most user-friendly. Almost all functionality that is available from the CLI is available in Horizon. Exosphere and Cacao both provide subsets of Horizon functionality with more focus on ease of use. That said, there are times you may need the more complete features of Horizon instead of the other GUI interfaces. This documentation will cover the basics of launching, using, and managing instances, using various storage options in Horizon, using container orchestration engines, and other aspects of using Openstack. Logging into horizon \u00b6 Action Screenshot Navigate to https://js2.jetstream-cloud.org Make sure it says \"XSEDE Globus Auth\" in the Authenticate Using box. The first time you log in you'll be directed to a Globus page to permit authorization. If you have linked institutional, Google, Orcid, or other credentials, you'll be able to use those to authenticate. We know XSEDE credentials work correctly so we will show that in our example. The next page should be the login screen for your credentials. We're showing the XSEDE login screen as our example. If you're using two-factor auth with your credentials as XSEDE does, you'll likely get a Duo or Authenticator screen here. You should be at the Horizon Dashboard home now. If you are on multiple XSEDE allocations, you'll want to verify you're using the correct one and change to the correct one if you are not. You do that by clicking at the top left next to the Jetstream2 logo where it has \"XSEDE * TG-XXXXXXXXX * IU\". That will show allocations under \"Projects\". Getting started with Horizon \u00b6 Creating and managing security groups Creating networks Launching an instance Managing instances Using volumes Using Manila shares Troubleshooting in Horizon","title":"Overview"},{"location":"ui/horizon/intro/#horizon","text":"Horizon is a web-based GUI for interacting with the Openstack API. It is likely the most complete GUI for working with Openstack but isn\u2019t the fastest or most user-friendly. Almost all functionality that is available from the CLI is available in Horizon. Exosphere and Cacao both provide subsets of Horizon functionality with more focus on ease of use. That said, there are times you may need the more complete features of Horizon instead of the other GUI interfaces. This documentation will cover the basics of launching, using, and managing instances, using various storage options in Horizon, using container orchestration engines, and other aspects of using Openstack.","title":"Horizon"},{"location":"ui/horizon/intro/#logging-into-horizon","text":"Action Screenshot Navigate to https://js2.jetstream-cloud.org Make sure it says \"XSEDE Globus Auth\" in the Authenticate Using box. The first time you log in you'll be directed to a Globus page to permit authorization. If you have linked institutional, Google, Orcid, or other credentials, you'll be able to use those to authenticate. We know XSEDE credentials work correctly so we will show that in our example. The next page should be the login screen for your credentials. We're showing the XSEDE login screen as our example. If you're using two-factor auth with your credentials as XSEDE does, you'll likely get a Duo or Authenticator screen here. You should be at the Horizon Dashboard home now. If you are on multiple XSEDE allocations, you'll want to verify you're using the correct one and change to the correct one if you are not. You do that by clicking at the top left next to the Jetstream2 logo where it has \"XSEDE * TG-XXXXXXXXX * IU\". That will show allocations under \"Projects\".","title":"Logging into horizon"},{"location":"ui/horizon/intro/#getting-started-with-horizon","text":"Creating and managing security groups Creating networks Launching an instance Managing instances Using volumes Using Manila shares Troubleshooting in Horizon","title":"Getting started with Horizon"},{"location":"ui/horizon/launch/","text":"","title":"Launching Instances"},{"location":"ui/horizon/manage/","text":"Instance Management Actions \u00b6 Coming soon!","title":"Instance Management"},{"location":"ui/horizon/manage/#instance-management-actions","text":"Coming soon!","title":"Instance Management Actions"},{"location":"ui/horizon/manila/","text":"To use Manila via Horizon \u00b6 1. Create the share on Horizon \u00b6 i. Click on: Project \u2192 Share \u2192 Shares \u2192 Create Share ii. Create a share with the following settings: share name - a name of your choosing share protocol - CephFS size - the size of your manila share share type - cephnfsnativetype 2. Edit the share rule \u00b6 i. Once your share is available you can select Edit Share and Manage Rules and Add Rule : access type - cephx access level - read-write access to - an arbitrary unique name In the example above the accessTo name is manilashare . The name assigned must be globally unique, if you use a name that is already in use you will see and error state. ii. If you now go back to the share page (Project/Share/Shares) and click on the share you created you should see your share\u2019s metadata. Important things to note here are : Path - ips:ports followed by volume path (/volume/_no-group/\u2026) Access Key Using Manila Share on a VM \u00b6 This is the same whether you\u2019re using Horizon or the CLI. Please refer to Configuring a VM to use Manila Shares","title":"Manila Shares in Horizon"},{"location":"ui/horizon/manila/#to-use-manila-via-horizon","text":"","title":"To use Manila via Horizon"},{"location":"ui/horizon/manila/#1-create-the-share-on-horizon","text":"i. Click on: Project \u2192 Share \u2192 Shares \u2192 Create Share ii. Create a share with the following settings: share name - a name of your choosing share protocol - CephFS size - the size of your manila share share type - cephnfsnativetype","title":"1. Create the share on Horizon"},{"location":"ui/horizon/manila/#2-edit-the-share-rule","text":"i. Once your share is available you can select Edit Share and Manage Rules and Add Rule : access type - cephx access level - read-write access to - an arbitrary unique name In the example above the accessTo name is manilashare . The name assigned must be globally unique, if you use a name that is already in use you will see and error state. ii. If you now go back to the share page (Project/Share/Shares) and click on the share you created you should see your share\u2019s metadata. Important things to note here are : Path - ips:ports followed by volume path (/volume/_no-group/\u2026) Access Key","title":"2. Edit the share rule"},{"location":"ui/horizon/manila/#using-manila-share-on-a-vm","text":"This is the same whether you\u2019re using Horizon or the CLI. Please refer to Configuring a VM to use Manila Shares","title":"Using Manila Share on a VM"},{"location":"ui/horizon/network/","text":"Working with Networks in Horizon \u00b6","title":"Networks in Horizon"},{"location":"ui/horizon/network/#working-with-networks-in-horizon","text":"","title":"Working with Networks in Horizon"},{"location":"ui/horizon/security_group/","text":"Security Group Management in Horizon \u00b6 Security groups can be thought of like firewalls. They ultimately control inbound and outbound traffic to your virtual machines. Under the CLI and Horizon, access defaults to all outbound allowed and NO inbound allowed. To allow access to your VM for things like SSH, you will need to create a security group and add rules to it. You can reuse a security group many times, so a best practice is to create groups by related services. For insance, you might create a basic group for ssh and icmp (which is what we will show as an example) and then a separate security group for http and https access if you\u2019re running a web service on your instance. Creating Security Groups and Rules \u00b6 This will walk you through creating a basic security group in Horizon and adding a couple of simple access rules. Action Screenshot Login to the Horizon dashboard and make sure you've selected the correct allocation. Select the \"Network\" tab on the sidebar and click \"Security Groups\". Once you're on the security group page, you'll need to click the \"Create Security Group\" button (noted with a red arrow on the screenshot) In the popup box that comes up, you'll give your new security group a name (we suggest something like *my-username-ssh-and-icmp*) and optional description. We recommend giving a meaningful name and noting in the description what your intended purpose is. When the creation is successful, it will bring you back to the security group page and note the success in the corner with a green status message. You'll see your new group name at the top where it says **Manage Security Group Rules: your-rule-name**. You'll then want to click \"Add Rule\" (noted with a red arrow on the screenshot) This will bring up a new dialog box where you can select the parameters for your security group rule. If you click the \"Rule\" dropdown at the top, you'll see a list of common rule types as well as the option for custom rules. For this example, we'll select \"SSH\" to allow inbound port 22/SSH access. We'll fill in the other details needed. We do recommend putting in a description with what the rule does so it's easy to see at a glance. We'll also select CIDR as the remote type and then set *0.0.0.0/0* as the CIDR. This allows all traffic to the SSH port. You can make that be a single IP or a specific CIDR block. In general, limiting access to specific CIDR blocks or IPs is best. When the creation is successful, it will bring you back to the security group page and note the success in the corner with a green status message. You'll see your new rule now on the page. You'll need to click the \"Create Security Group\" button and second time and we'll create a second rule for \"All ICMP\". This will allow things like incoming ping to check the status of your virtual machine. You'll select *Ingress*, *CIDR*, and set the CIDR to *0.0.0.0/0* to allow all hosts to ping your virtual machine. As before, when the creation is successful, it will bring you back to the security group page and note the success in the corner with a green status message. You'll see your new rule now on the page. You can then add additional rules or additional security groups. This will allow the most basic of access to your VMs. We do recommend limiting access as much as possible for best security practices.","title":"Security Group Management"},{"location":"ui/horizon/security_group/#security-group-management-in-horizon","text":"Security groups can be thought of like firewalls. They ultimately control inbound and outbound traffic to your virtual machines. Under the CLI and Horizon, access defaults to all outbound allowed and NO inbound allowed. To allow access to your VM for things like SSH, you will need to create a security group and add rules to it. You can reuse a security group many times, so a best practice is to create groups by related services. For insance, you might create a basic group for ssh and icmp (which is what we will show as an example) and then a separate security group for http and https access if you\u2019re running a web service on your instance.","title":"Security Group Management in Horizon"},{"location":"ui/horizon/security_group/#creating-security-groups-and-rules","text":"This will walk you through creating a basic security group in Horizon and adding a couple of simple access rules. Action Screenshot Login to the Horizon dashboard and make sure you've selected the correct allocation. Select the \"Network\" tab on the sidebar and click \"Security Groups\". Once you're on the security group page, you'll need to click the \"Create Security Group\" button (noted with a red arrow on the screenshot) In the popup box that comes up, you'll give your new security group a name (we suggest something like *my-username-ssh-and-icmp*) and optional description. We recommend giving a meaningful name and noting in the description what your intended purpose is. When the creation is successful, it will bring you back to the security group page and note the success in the corner with a green status message. You'll see your new group name at the top where it says **Manage Security Group Rules: your-rule-name**. You'll then want to click \"Add Rule\" (noted with a red arrow on the screenshot) This will bring up a new dialog box where you can select the parameters for your security group rule. If you click the \"Rule\" dropdown at the top, you'll see a list of common rule types as well as the option for custom rules. For this example, we'll select \"SSH\" to allow inbound port 22/SSH access. We'll fill in the other details needed. We do recommend putting in a description with what the rule does so it's easy to see at a glance. We'll also select CIDR as the remote type and then set *0.0.0.0/0* as the CIDR. This allows all traffic to the SSH port. You can make that be a single IP or a specific CIDR block. In general, limiting access to specific CIDR blocks or IPs is best. When the creation is successful, it will bring you back to the security group page and note the success in the corner with a green status message. You'll see your new rule now on the page. You'll need to click the \"Create Security Group\" button and second time and we'll create a second rule for \"All ICMP\". This will allow things like incoming ping to check the status of your virtual machine. You'll select *Ingress*, *CIDR*, and set the CIDR to *0.0.0.0/0* to allow all hosts to ping your virtual machine. As before, when the creation is successful, it will bring you back to the security group page and note the success in the corner with a green status message. You'll see your new rule now on the page. You can then add additional rules or additional security groups. This will allow the most basic of access to your VMs. We do recommend limiting access as much as possible for best security practices.","title":"Creating Security Groups and Rules"},{"location":"ui/horizon/storage/","text":"Using Storage Under Horizon \u00b6 Coming soon!","title":"Volumes Under Horizon"},{"location":"ui/horizon/storage/#using-storage-under-horizon","text":"Coming soon!","title":"Using Storage Under Horizon"},{"location":"ui/horizon/troubleshooting/","text":"Horizon Troubleshooting \u00b6 Coming soon!","title":"Horizon Troubleshooting"},{"location":"ui/horizon/troubleshooting/#horizon-troubleshooting","text":"Coming soon!","title":"Horizon Troubleshooting"}]}